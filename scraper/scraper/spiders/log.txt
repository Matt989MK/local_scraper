INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: 296d303f903195fa
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
DEBUG: Attempting to acquire lock 2898865726912 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2898865726912 acquired on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Attempting to release lock 2898865726912 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2898865726912 released on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Crawled (200) <GET http://smithgill.com/robots.txt> (referer: None)
DEBUG: Rule at line 20 without any user agent to enforce it on.
DEBUG: Rule at line 36 without any user agent to enforce it on.
DEBUG: Rule at line 228 without any user agent to enforce it on.
DEBUG: Rule at line 229 without any user agent to enforce it on.
DEBUG: Rule at line 230 without any user agent to enforce it on.
DEBUG: Rule at line 238 without any user agent to enforce it on.
DEBUG: Rule at line 239 without any user agent to enforce it on.
DEBUG: Rule at line 240 without any user agent to enforce it on.
DEBUG: Rule at line 241 without any user agent to enforce it on.
DEBUG: Rule at line 242 without any user agent to enforce it on.
DEBUG: Rule at line 243 without any user agent to enforce it on.
DEBUG: Rule at line 244 without any user agent to enforce it on.
DEBUG: Rule at line 245 without any user agent to enforce it on.
DEBUG: Rule at line 246 without any user agent to enforce it on.
DEBUG: Rule at line 247 without any user agent to enforce it on.
DEBUG: Rule at line 248 without any user agent to enforce it on.
DEBUG: Rule at line 249 without any user agent to enforce it on.
DEBUG: Rule at line 250 without any user agent to enforce it on.
DEBUG: Rule at line 251 without any user agent to enforce it on.
DEBUG: Rule at line 285 without any user agent to enforce it on.
DEBUG: Rule at line 291 without any user agent to enforce it on.
DEBUG: Rule at line 297 without any user agent to enforce it on.
DEBUG: Rule at line 303 without any user agent to enforce it on.
DEBUG: Rule at line 309 without any user agent to enforce it on.
DEBUG: Rule at line 315 without any user agent to enforce it on.
DEBUG: Rule at line 321 without any user agent to enforce it on.
DEBUG: Rule at line 327 without any user agent to enforce it on.
DEBUG: Rule at line 333 without any user agent to enforce it on.
DEBUG: Rule at line 339 without any user agent to enforce it on.
DEBUG: Rule at line 353 without any user agent to enforce it on.
DEBUG: Rule at line 359 without any user agent to enforce it on.
DEBUG: Rule at line 365 without any user agent to enforce it on.
DEBUG: Rule at line 371 without any user agent to enforce it on.
DEBUG: Rule at line 377 without any user agent to enforce it on.
DEBUG: Rule at line 383 without any user agent to enforce it on.
DEBUG: Rule at line 389 without any user agent to enforce it on.
DEBUG: Rule at line 395 without any user agent to enforce it on.
DEBUG: Rule at line 401 without any user agent to enforce it on.
DEBUG: Rule at line 407 without any user agent to enforce it on.
DEBUG: Rule at line 413 without any user agent to enforce it on.
DEBUG: Rule at line 419 without any user agent to enforce it on.
DEBUG: Rule at line 440 without any user agent to enforce it on.
DEBUG: Rule at line 441 without any user agent to enforce it on.
DEBUG: Rule at line 442 without any user agent to enforce it on.
DEBUG: Rule at line 443 without any user agent to enforce it on.
DEBUG: Rule at line 449 without any user agent to enforce it on.
DEBUG: Rule at line 453 without any user agent to enforce it on.
DEBUG: Rule at line 454 without any user agent to enforce it on.
DEBUG: Rule at line 472 without any user agent to enforce it on.
DEBUG: Rule at line 481 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET http://smithgill.com/> (referer: None)
DEBUG: driver not found in PATH, trying Selenium Manager
DEBUG: Executing: C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\common\windows\selenium-manager.exe --browser chrome
DEBUG: Using driver at: C:\Users\moffi\.cache\selenium\chromedriver\win32\108.0.5359.71\chromedriver.exe
DEBUG: Started executable: `chromedriver` in a child process with pid: 22340
DEBUG: POST http://localhost:51635/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "pageLoadStrategy": "normal", "goog:chromeOptions": {"extensions": [], "args": []}}}}
DEBUG: Starting new HTTP connection (1): localhost:51635
DEBUG: http://localhost:51635 "POST /session HTTP/1.1" 200 791
DEBUG: Remote response: status=200 | data={"value":{"capabilities":{"acceptInsecureCerts":false,"browserName":"chrome","browserVersion":"108.0.5359.73","chrome":{"chromedriverVersion":"108.0.5359.71 (1e0e3868ee06e91ad636a874420e3ca3ae3756ac-refs/branch-heads/5359@{#1016})","userDataDir":"C:\\Users\\moffi\\AppData\\Local\\Temp\\scoped_dir22340_1552261349"},"goog:chromeOptions":{"debuggerAddress":"localhost:51638"},"networkConnectionEnabled":false,"pageLoadStrategy":"normal","platformName":"windows","proxy":{},"setWindowRect":true,"strictFileInteractability":false,"timeouts":{"implicit":0,"pageLoad":300000,"script":30000},"unhandledPromptBehavior":"dismiss and notify","webauthn:extension:credBlob":true,"webauthn:extension:largeBlob":true,"webauthn:virtualAuthenticators":true},"sessionId":"bf11cd543f059a6fa717ab4c1e834c5b"}} | headers=HTTPHeaderDict({'Content-Length': '791', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:51635/session/bf11cd543f059a6fa717ab4c1e834c5b/url {"url": "http://smithgill.com/"}
DEBUG: http://localhost:51635 "POST /session/bf11cd543f059a6fa717ab4c1e834c5b/url HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:51635/session/bf11cd543f059a6fa717ab4c1e834c5b/timeouts {"implicit": 10000}
DEBUG: http://localhost:51635 "POST /session/bf11cd543f059a6fa717ab4c1e834c5b/timeouts HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: Filtered duplicate request: <GET http://smithgill.com/contact> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG: Scraped from <200 http://smithgill.com/>
{'facebook': 'https://www.facebook.com/smithgillarch'}
DEBUG: DELETE http://localhost:51635/session/bf11cd543f059a6fa717ab4c1e834c5b {}
DEBUG: http://localhost:51635 "DELETE /session/bf11cd543f059a6fa717ab4c1e834c5b HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: Redirecting (301) to <GET http://smithgill.com/contact/> from <GET http://smithgill.com/contact>
DEBUG: Crawled (200) <GET http://smithgill.com/contact/> (referer: http://smithgill.com/)
INFO: Closing spider (finished)
INFO: Stored json feed (1 items) in: website_spider_output.json
INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1252,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 18931,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/301': 1,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 18.209871,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 5, 23, 55, 13, 986742),
 'httpcompression/response_bytes': 85365,
 'httpcompression/response_count': 3,
 'item_scraped_count': 1,
 'log_count/DEBUG': 84,
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 3,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 12, 5, 23, 54, 55, 776871)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: f74f7239a1d268d1
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
DEBUG: Attempting to acquire lock 2215736630752 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2215736630752 acquired on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Attempting to release lock 2215736630752 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2215736630752 released on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Crawled (200) <GET http://smithgill.com/robots.txt> (referer: None)
DEBUG: Rule at line 20 without any user agent to enforce it on.
DEBUG: Rule at line 36 without any user agent to enforce it on.
DEBUG: Rule at line 228 without any user agent to enforce it on.
DEBUG: Rule at line 229 without any user agent to enforce it on.
DEBUG: Rule at line 230 without any user agent to enforce it on.
DEBUG: Rule at line 238 without any user agent to enforce it on.
DEBUG: Rule at line 239 without any user agent to enforce it on.
DEBUG: Rule at line 240 without any user agent to enforce it on.
DEBUG: Rule at line 241 without any user agent to enforce it on.
DEBUG: Rule at line 242 without any user agent to enforce it on.
DEBUG: Rule at line 243 without any user agent to enforce it on.
DEBUG: Rule at line 244 without any user agent to enforce it on.
DEBUG: Rule at line 245 without any user agent to enforce it on.
DEBUG: Rule at line 246 without any user agent to enforce it on.
DEBUG: Rule at line 247 without any user agent to enforce it on.
DEBUG: Rule at line 248 without any user agent to enforce it on.
DEBUG: Rule at line 249 without any user agent to enforce it on.
DEBUG: Rule at line 250 without any user agent to enforce it on.
DEBUG: Rule at line 251 without any user agent to enforce it on.
DEBUG: Rule at line 285 without any user agent to enforce it on.
DEBUG: Rule at line 291 without any user agent to enforce it on.
DEBUG: Rule at line 297 without any user agent to enforce it on.
DEBUG: Rule at line 303 without any user agent to enforce it on.
DEBUG: Rule at line 309 without any user agent to enforce it on.
DEBUG: Rule at line 315 without any user agent to enforce it on.
DEBUG: Rule at line 321 without any user agent to enforce it on.
DEBUG: Rule at line 327 without any user agent to enforce it on.
DEBUG: Rule at line 333 without any user agent to enforce it on.
DEBUG: Rule at line 339 without any user agent to enforce it on.
DEBUG: Rule at line 353 without any user agent to enforce it on.
DEBUG: Rule at line 359 without any user agent to enforce it on.
DEBUG: Rule at line 365 without any user agent to enforce it on.
DEBUG: Rule at line 371 without any user agent to enforce it on.
DEBUG: Rule at line 377 without any user agent to enforce it on.
DEBUG: Rule at line 383 without any user agent to enforce it on.
DEBUG: Rule at line 389 without any user agent to enforce it on.
DEBUG: Rule at line 395 without any user agent to enforce it on.
DEBUG: Rule at line 401 without any user agent to enforce it on.
DEBUG: Rule at line 407 without any user agent to enforce it on.
DEBUG: Rule at line 413 without any user agent to enforce it on.
DEBUG: Rule at line 419 without any user agent to enforce it on.
DEBUG: Rule at line 440 without any user agent to enforce it on.
DEBUG: Rule at line 441 without any user agent to enforce it on.
DEBUG: Rule at line 442 without any user agent to enforce it on.
DEBUG: Rule at line 443 without any user agent to enforce it on.
DEBUG: Rule at line 449 without any user agent to enforce it on.
DEBUG: Rule at line 453 without any user agent to enforce it on.
DEBUG: Rule at line 454 without any user agent to enforce it on.
DEBUG: Rule at line 472 without any user agent to enforce it on.
DEBUG: Rule at line 481 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET http://smithgill.com/> (referer: None)
DEBUG: driver not found in PATH, trying Selenium Manager
DEBUG: Executing: C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\common\windows\selenium-manager.exe --browser chrome
DEBUG: Using driver at: C:\Users\moffi\.cache\selenium\chromedriver\win32\108.0.5359.71\chromedriver.exe
DEBUG: Started executable: `chromedriver` in a child process with pid: 15088
DEBUG: POST http://localhost:49199/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "pageLoadStrategy": "normal", "goog:chromeOptions": {"extensions": [], "args": []}}}}
DEBUG: Starting new HTTP connection (1): localhost:49199
DEBUG: http://localhost:49199 "POST /session HTTP/1.1" 200 791
DEBUG: Remote response: status=200 | data={"value":{"capabilities":{"acceptInsecureCerts":false,"browserName":"chrome","browserVersion":"108.0.5359.73","chrome":{"chromedriverVersion":"108.0.5359.71 (1e0e3868ee06e91ad636a874420e3ca3ae3756ac-refs/branch-heads/5359@{#1016})","userDataDir":"C:\\Users\\moffi\\AppData\\Local\\Temp\\scoped_dir15088_1423117287"},"goog:chromeOptions":{"debuggerAddress":"localhost:49202"},"networkConnectionEnabled":false,"pageLoadStrategy":"normal","platformName":"windows","proxy":{},"setWindowRect":true,"strictFileInteractability":false,"timeouts":{"implicit":0,"pageLoad":300000,"script":30000},"unhandledPromptBehavior":"dismiss and notify","webauthn:extension:credBlob":true,"webauthn:extension:largeBlob":true,"webauthn:virtualAuthenticators":true},"sessionId":"8104acd0b3740afb0111ae884d988a25"}} | headers=HTTPHeaderDict({'Content-Length': '791', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:49199/session/8104acd0b3740afb0111ae884d988a25/url {"url": "http://smithgill.com/"}
DEBUG: http://localhost:49199 "POST /session/8104acd0b3740afb0111ae884d988a25/url HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:49199/session/8104acd0b3740afb0111ae884d988a25/timeouts {"implicit": 10000}
DEBUG: http://localhost:49199 "POST /session/8104acd0b3740afb0111ae884d988a25/timeouts HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: Filtered duplicate request: <GET http://smithgill.com/contact> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG: Scraped from <200 http://smithgill.com/>
{'facebook': 'https://www.facebook.com/smithgillarch'}
DEBUG: DELETE http://localhost:49199/session/8104acd0b3740afb0111ae884d988a25 {}
DEBUG: http://localhost:49199 "DELETE /session/8104acd0b3740afb0111ae884d988a25 HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: Redirecting (301) to <GET http://smithgill.com/contact/> from <GET http://smithgill.com/contact>
DEBUG: Crawled (200) <GET http://smithgill.com/contact/> (referer: http://smithgill.com/)
INFO: Closing spider (finished)
INFO: Stored json feed (1 items) in: website_spider_output.json
INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1252,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 18931,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/301': 1,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 18.327564,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 5, 23, 59, 32, 128719),
 'httpcompression/response_bytes': 85365,
 'httpcompression/response_count': 3,
 'item_scraped_count': 1,
 'log_count/DEBUG': 84,
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 3,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 12, 5, 23, 59, 13, 801155)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: 37f0ff2012db75e8
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
DEBUG: Attempting to acquire lock 1502844935504 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 1502844935504 acquired on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Attempting to release lock 1502844935504 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 1502844935504 released on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Crawled (200) <GET http://smithgill.com/robots.txt> (referer: None)
DEBUG: Rule at line 20 without any user agent to enforce it on.
DEBUG: Rule at line 36 without any user agent to enforce it on.
DEBUG: Rule at line 228 without any user agent to enforce it on.
DEBUG: Rule at line 229 without any user agent to enforce it on.
DEBUG: Rule at line 230 without any user agent to enforce it on.
DEBUG: Rule at line 238 without any user agent to enforce it on.
DEBUG: Rule at line 239 without any user agent to enforce it on.
DEBUG: Rule at line 240 without any user agent to enforce it on.
DEBUG: Rule at line 241 without any user agent to enforce it on.
DEBUG: Rule at line 242 without any user agent to enforce it on.
DEBUG: Rule at line 243 without any user agent to enforce it on.
DEBUG: Rule at line 244 without any user agent to enforce it on.
DEBUG: Rule at line 245 without any user agent to enforce it on.
DEBUG: Rule at line 246 without any user agent to enforce it on.
DEBUG: Rule at line 247 without any user agent to enforce it on.
DEBUG: Rule at line 248 without any user agent to enforce it on.
DEBUG: Rule at line 249 without any user agent to enforce it on.
DEBUG: Rule at line 250 without any user agent to enforce it on.
DEBUG: Rule at line 251 without any user agent to enforce it on.
DEBUG: Rule at line 285 without any user agent to enforce it on.
DEBUG: Rule at line 291 without any user agent to enforce it on.
DEBUG: Rule at line 297 without any user agent to enforce it on.
DEBUG: Rule at line 303 without any user agent to enforce it on.
DEBUG: Rule at line 309 without any user agent to enforce it on.
DEBUG: Rule at line 315 without any user agent to enforce it on.
DEBUG: Rule at line 321 without any user agent to enforce it on.
DEBUG: Rule at line 327 without any user agent to enforce it on.
DEBUG: Rule at line 333 without any user agent to enforce it on.
DEBUG: Rule at line 339 without any user agent to enforce it on.
DEBUG: Rule at line 353 without any user agent to enforce it on.
DEBUG: Rule at line 359 without any user agent to enforce it on.
DEBUG: Rule at line 365 without any user agent to enforce it on.
DEBUG: Rule at line 371 without any user agent to enforce it on.
DEBUG: Rule at line 377 without any user agent to enforce it on.
DEBUG: Rule at line 383 without any user agent to enforce it on.
DEBUG: Rule at line 389 without any user agent to enforce it on.
DEBUG: Rule at line 395 without any user agent to enforce it on.
DEBUG: Rule at line 401 without any user agent to enforce it on.
DEBUG: Rule at line 407 without any user agent to enforce it on.
DEBUG: Rule at line 413 without any user agent to enforce it on.
DEBUG: Rule at line 419 without any user agent to enforce it on.
DEBUG: Rule at line 440 without any user agent to enforce it on.
DEBUG: Rule at line 441 without any user agent to enforce it on.
DEBUG: Rule at line 442 without any user agent to enforce it on.
DEBUG: Rule at line 443 without any user agent to enforce it on.
DEBUG: Rule at line 449 without any user agent to enforce it on.
DEBUG: Rule at line 453 without any user agent to enforce it on.
DEBUG: Rule at line 454 without any user agent to enforce it on.
DEBUG: Rule at line 472 without any user agent to enforce it on.
DEBUG: Rule at line 481 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET http://smithgill.com/> (referer: None)
DEBUG: driver not found in PATH, trying Selenium Manager
DEBUG: Executing: C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\common\windows\selenium-manager.exe --browser chrome
DEBUG: Using driver at: C:\Users\moffi\.cache\selenium\chromedriver\win32\108.0.5359.71\chromedriver.exe
DEBUG: Started executable: `chromedriver` in a child process with pid: 27404
DEBUG: POST http://localhost:49266/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "pageLoadStrategy": "normal", "goog:chromeOptions": {"extensions": [], "args": []}}}}
DEBUG: Starting new HTTP connection (1): localhost:49266
DEBUG: http://localhost:49266 "POST /session HTTP/1.1" 200 791
DEBUG: Remote response: status=200 | data={"value":{"capabilities":{"acceptInsecureCerts":false,"browserName":"chrome","browserVersion":"108.0.5359.73","chrome":{"chromedriverVersion":"108.0.5359.71 (1e0e3868ee06e91ad636a874420e3ca3ae3756ac-refs/branch-heads/5359@{#1016})","userDataDir":"C:\\Users\\moffi\\AppData\\Local\\Temp\\scoped_dir27404_1278179160"},"goog:chromeOptions":{"debuggerAddress":"localhost:49269"},"networkConnectionEnabled":false,"pageLoadStrategy":"normal","platformName":"windows","proxy":{},"setWindowRect":true,"strictFileInteractability":false,"timeouts":{"implicit":0,"pageLoad":300000,"script":30000},"unhandledPromptBehavior":"dismiss and notify","webauthn:extension:credBlob":true,"webauthn:extension:largeBlob":true,"webauthn:virtualAuthenticators":true},"sessionId":"1285595ee63e95d3abc3724ba2fadb70"}} | headers=HTTPHeaderDict({'Content-Length': '791', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:49266/session/1285595ee63e95d3abc3724ba2fadb70/url {"url": "http://smithgill.com/"}
DEBUG: http://localhost:49266 "POST /session/1285595ee63e95d3abc3724ba2fadb70/url HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:49266/session/1285595ee63e95d3abc3724ba2fadb70/timeouts {"implicit": 10000}
DEBUG: http://localhost:49266 "POST /session/1285595ee63e95d3abc3724ba2fadb70/timeouts HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: Filtered duplicate request: <GET http://smithgill.com/contact> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG: Scraped from <200 http://smithgill.com/>
{'facebook': 'https://www.facebook.com/smithgillarch'}
DEBUG: DELETE http://localhost:49266/session/1285595ee63e95d3abc3724ba2fadb70 {}
DEBUG: http://localhost:49266 "DELETE /session/1285595ee63e95d3abc3724ba2fadb70 HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: Redirecting (301) to <GET http://smithgill.com/contact/> from <GET http://smithgill.com/contact>
DEBUG: Crawled (200) <GET http://smithgill.com/contact/> (referer: http://smithgill.com/)
INFO: Closing spider (finished)
INFO: Stored json feed (1 items) in: website_spider_output.json
INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1252,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 18931,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/301': 1,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 18.828954,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 6, 0, 0, 38, 296836),
 'httpcompression/response_bytes': 85365,
 'httpcompression/response_count': 3,
 'item_scraped_count': 1,
 'log_count/DEBUG': 84,
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 3,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 12, 6, 0, 0, 19, 467882)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: c048dd801886f7e2
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
DEBUG: Attempting to acquire lock 2216693160288 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2216693160288 acquired on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Attempting to release lock 2216693160288 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2216693160288 released on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Crawled (200) <GET http://smithgill.com/robots.txt> (referer: None)
DEBUG: Rule at line 20 without any user agent to enforce it on.
DEBUG: Rule at line 36 without any user agent to enforce it on.
DEBUG: Rule at line 228 without any user agent to enforce it on.
DEBUG: Rule at line 229 without any user agent to enforce it on.
DEBUG: Rule at line 230 without any user agent to enforce it on.
DEBUG: Rule at line 238 without any user agent to enforce it on.
DEBUG: Rule at line 239 without any user agent to enforce it on.
DEBUG: Rule at line 240 without any user agent to enforce it on.
DEBUG: Rule at line 241 without any user agent to enforce it on.
DEBUG: Rule at line 242 without any user agent to enforce it on.
DEBUG: Rule at line 243 without any user agent to enforce it on.
DEBUG: Rule at line 244 without any user agent to enforce it on.
DEBUG: Rule at line 245 without any user agent to enforce it on.
DEBUG: Rule at line 246 without any user agent to enforce it on.
DEBUG: Rule at line 247 without any user agent to enforce it on.
DEBUG: Rule at line 248 without any user agent to enforce it on.
DEBUG: Rule at line 249 without any user agent to enforce it on.
DEBUG: Rule at line 250 without any user agent to enforce it on.
DEBUG: Rule at line 251 without any user agent to enforce it on.
DEBUG: Rule at line 285 without any user agent to enforce it on.
DEBUG: Rule at line 291 without any user agent to enforce it on.
DEBUG: Rule at line 297 without any user agent to enforce it on.
DEBUG: Rule at line 303 without any user agent to enforce it on.
DEBUG: Rule at line 309 without any user agent to enforce it on.
DEBUG: Rule at line 315 without any user agent to enforce it on.
DEBUG: Rule at line 321 without any user agent to enforce it on.
DEBUG: Rule at line 327 without any user agent to enforce it on.
DEBUG: Rule at line 333 without any user agent to enforce it on.
DEBUG: Rule at line 339 without any user agent to enforce it on.
DEBUG: Rule at line 353 without any user agent to enforce it on.
DEBUG: Rule at line 359 without any user agent to enforce it on.
DEBUG: Rule at line 365 without any user agent to enforce it on.
DEBUG: Rule at line 371 without any user agent to enforce it on.
DEBUG: Rule at line 377 without any user agent to enforce it on.
DEBUG: Rule at line 383 without any user agent to enforce it on.
DEBUG: Rule at line 389 without any user agent to enforce it on.
DEBUG: Rule at line 395 without any user agent to enforce it on.
DEBUG: Rule at line 401 without any user agent to enforce it on.
DEBUG: Rule at line 407 without any user agent to enforce it on.
DEBUG: Rule at line 413 without any user agent to enforce it on.
DEBUG: Rule at line 419 without any user agent to enforce it on.
DEBUG: Rule at line 440 without any user agent to enforce it on.
DEBUG: Rule at line 441 without any user agent to enforce it on.
DEBUG: Rule at line 442 without any user agent to enforce it on.
DEBUG: Rule at line 443 without any user agent to enforce it on.
DEBUG: Rule at line 449 without any user agent to enforce it on.
DEBUG: Rule at line 453 without any user agent to enforce it on.
DEBUG: Rule at line 454 without any user agent to enforce it on.
DEBUG: Rule at line 472 without any user agent to enforce it on.
DEBUG: Rule at line 481 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET http://smithgill.com/> (referer: None)
DEBUG: driver not found in PATH, trying Selenium Manager
DEBUG: Executing: C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\common\windows\selenium-manager.exe --browser chrome
DEBUG: Using driver at: C:\Users\moffi\.cache\selenium\chromedriver\win32\108.0.5359.71\chromedriver.exe
DEBUG: Started executable: `chromedriver` in a child process with pid: 20180
DEBUG: POST http://localhost:49334/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "pageLoadStrategy": "normal", "goog:chromeOptions": {"extensions": [], "args": []}}}}
DEBUG: Starting new HTTP connection (1): localhost:49334
DEBUG: http://localhost:49334 "POST /session HTTP/1.1" 200 790
DEBUG: Remote response: status=200 | data={"value":{"capabilities":{"acceptInsecureCerts":false,"browserName":"chrome","browserVersion":"108.0.5359.73","chrome":{"chromedriverVersion":"108.0.5359.71 (1e0e3868ee06e91ad636a874420e3ca3ae3756ac-refs/branch-heads/5359@{#1016})","userDataDir":"C:\\Users\\moffi\\AppData\\Local\\Temp\\scoped_dir20180_515118047"},"goog:chromeOptions":{"debuggerAddress":"localhost:49337"},"networkConnectionEnabled":false,"pageLoadStrategy":"normal","platformName":"windows","proxy":{},"setWindowRect":true,"strictFileInteractability":false,"timeouts":{"implicit":0,"pageLoad":300000,"script":30000},"unhandledPromptBehavior":"dismiss and notify","webauthn:extension:credBlob":true,"webauthn:extension:largeBlob":true,"webauthn:virtualAuthenticators":true},"sessionId":"239f2ac481164180e2bf7d45a31c2cfc"}} | headers=HTTPHeaderDict({'Content-Length': '790', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:49334/session/239f2ac481164180e2bf7d45a31c2cfc/url {"url": "http://smithgill.com/"}
DEBUG: http://localhost:49334 "POST /session/239f2ac481164180e2bf7d45a31c2cfc/url HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:49334/session/239f2ac481164180e2bf7d45a31c2cfc/timeouts {"implicit": 10000}
DEBUG: http://localhost:49334 "POST /session/239f2ac481164180e2bf7d45a31c2cfc/timeouts HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: Filtered duplicate request: <GET http://smithgill.com/contact> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG: Scraped from <200 http://smithgill.com/>
{'facebook': 'https://www.facebook.com/smithgillarch'}
DEBUG: DELETE http://localhost:49334/session/239f2ac481164180e2bf7d45a31c2cfc {}
DEBUG: http://localhost:49334 "DELETE /session/239f2ac481164180e2bf7d45a31c2cfc HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: Redirecting (301) to <GET http://smithgill.com/contact/> from <GET http://smithgill.com/contact>
DEBUG: Crawled (200) <GET http://smithgill.com/contact/> (referer: http://smithgill.com/)
INFO: Closing spider (finished)
INFO: Stored json feed (1 items) in: website_spider_output.json
INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1252,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 18931,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/301': 1,
 'dupefilter/filtered': 2,
 'elapsed_time_seconds': 20.906456,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 6, 0, 1, 7, 243798),
 'httpcompression/response_bytes': 85365,
 'httpcompression/response_count': 3,
 'item_scraped_count': 1,
 'log_count/DEBUG': 84,
 'log_count/INFO': 11,
 'request_depth_max': 2,
 'response_received_count': 3,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 12, 6, 0, 0, 46, 337342)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: 0491f4165f4044de
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
DEBUG: Attempting to acquire lock 1736992283936 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 1736992283936 acquired on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Attempting to release lock 1736992283936 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 1736992283936 released on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Crawled (200) <GET http://smithgill.com/robots.txt> (referer: None)
DEBUG: Rule at line 20 without any user agent to enforce it on.
DEBUG: Rule at line 36 without any user agent to enforce it on.
DEBUG: Rule at line 228 without any user agent to enforce it on.
DEBUG: Rule at line 229 without any user agent to enforce it on.
DEBUG: Rule at line 230 without any user agent to enforce it on.
DEBUG: Rule at line 238 without any user agent to enforce it on.
DEBUG: Rule at line 239 without any user agent to enforce it on.
DEBUG: Rule at line 240 without any user agent to enforce it on.
DEBUG: Rule at line 241 without any user agent to enforce it on.
DEBUG: Rule at line 242 without any user agent to enforce it on.
DEBUG: Rule at line 243 without any user agent to enforce it on.
DEBUG: Rule at line 244 without any user agent to enforce it on.
DEBUG: Rule at line 245 without any user agent to enforce it on.
DEBUG: Rule at line 246 without any user agent to enforce it on.
DEBUG: Rule at line 247 without any user agent to enforce it on.
DEBUG: Rule at line 248 without any user agent to enforce it on.
DEBUG: Rule at line 249 without any user agent to enforce it on.
DEBUG: Rule at line 250 without any user agent to enforce it on.
DEBUG: Rule at line 251 without any user agent to enforce it on.
DEBUG: Rule at line 285 without any user agent to enforce it on.
DEBUG: Rule at line 291 without any user agent to enforce it on.
DEBUG: Rule at line 297 without any user agent to enforce it on.
DEBUG: Rule at line 303 without any user agent to enforce it on.
DEBUG: Rule at line 309 without any user agent to enforce it on.
DEBUG: Rule at line 315 without any user agent to enforce it on.
DEBUG: Rule at line 321 without any user agent to enforce it on.
DEBUG: Rule at line 327 without any user agent to enforce it on.
DEBUG: Rule at line 333 without any user agent to enforce it on.
DEBUG: Rule at line 339 without any user agent to enforce it on.
DEBUG: Rule at line 353 without any user agent to enforce it on.
DEBUG: Rule at line 359 without any user agent to enforce it on.
DEBUG: Rule at line 365 without any user agent to enforce it on.
DEBUG: Rule at line 371 without any user agent to enforce it on.
DEBUG: Rule at line 377 without any user agent to enforce it on.
DEBUG: Rule at line 383 without any user agent to enforce it on.
DEBUG: Rule at line 389 without any user agent to enforce it on.
DEBUG: Rule at line 395 without any user agent to enforce it on.
DEBUG: Rule at line 401 without any user agent to enforce it on.
DEBUG: Rule at line 407 without any user agent to enforce it on.
DEBUG: Rule at line 413 without any user agent to enforce it on.
DEBUG: Rule at line 419 without any user agent to enforce it on.
DEBUG: Rule at line 440 without any user agent to enforce it on.
DEBUG: Rule at line 441 without any user agent to enforce it on.
DEBUG: Rule at line 442 without any user agent to enforce it on.
DEBUG: Rule at line 443 without any user agent to enforce it on.
DEBUG: Rule at line 449 without any user agent to enforce it on.
DEBUG: Rule at line 453 without any user agent to enforce it on.
DEBUG: Rule at line 454 without any user agent to enforce it on.
DEBUG: Rule at line 472 without any user agent to enforce it on.
DEBUG: Rule at line 481 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET http://smithgill.com/> (referer: None)
DEBUG: driver not found in PATH, trying Selenium Manager
DEBUG: Executing: C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\common\windows\selenium-manager.exe --browser chrome
DEBUG: Using driver at: C:\Users\moffi\.cache\selenium\chromedriver\win32\108.0.5359.71\chromedriver.exe
DEBUG: Started executable: `chromedriver` in a child process with pid: 26932
DEBUG: POST http://localhost:49405/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "pageLoadStrategy": "normal", "goog:chromeOptions": {"extensions": [], "args": []}}}}
DEBUG: Starting new HTTP connection (1): localhost:49405
DEBUG: http://localhost:49405 "POST /session HTTP/1.1" 200 790
DEBUG: Remote response: status=200 | data={"value":{"capabilities":{"acceptInsecureCerts":false,"browserName":"chrome","browserVersion":"108.0.5359.73","chrome":{"chromedriverVersion":"108.0.5359.71 (1e0e3868ee06e91ad636a874420e3ca3ae3756ac-refs/branch-heads/5359@{#1016})","userDataDir":"C:\\Users\\moffi\\AppData\\Local\\Temp\\scoped_dir26932_346921101"},"goog:chromeOptions":{"debuggerAddress":"localhost:49409"},"networkConnectionEnabled":false,"pageLoadStrategy":"normal","platformName":"windows","proxy":{},"setWindowRect":true,"strictFileInteractability":false,"timeouts":{"implicit":0,"pageLoad":300000,"script":30000},"unhandledPromptBehavior":"dismiss and notify","webauthn:extension:credBlob":true,"webauthn:extension:largeBlob":true,"webauthn:virtualAuthenticators":true},"sessionId":"8d92a46e2b752e1f3cea941f212e23d1"}} | headers=HTTPHeaderDict({'Content-Length': '790', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:49405/session/8d92a46e2b752e1f3cea941f212e23d1/url {"url": "http://smithgill.com/"}
DEBUG: http://localhost:49405 "POST /session/8d92a46e2b752e1f3cea941f212e23d1/url HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:49405/session/8d92a46e2b752e1f3cea941f212e23d1/timeouts {"implicit": 10000}
DEBUG: http://localhost:49405 "POST /session/8d92a46e2b752e1f3cea941f212e23d1/timeouts HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: Filtered duplicate request: <GET http://smithgill.com/contact> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG: Scraped from <200 http://smithgill.com/>
{'facebook': 'https://www.facebook.com/smithgillarch'}
DEBUG: DELETE http://localhost:49405/session/8d92a46e2b752e1f3cea941f212e23d1 {}
DEBUG: http://localhost:49405 "DELETE /session/8d92a46e2b752e1f3cea941f212e23d1 HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: Redirecting (301) to <GET http://smithgill.com/contact/> from <GET http://smithgill.com/contact>
DEBUG: Crawled (200) <GET http://smithgill.com/contact/> (referer: http://smithgill.com/)
INFO: Closing spider (finished)
INFO: Stored json feed (1 items) in: website_spider_output.json
INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1252,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 18931,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/301': 1,
 'dupefilter/filtered': 2,
 'elapsed_time_seconds': 19.6648,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 6, 0, 2, 22, 805851),
 'httpcompression/response_bytes': 85365,
 'httpcompression/response_count': 3,
 'item_scraped_count': 1,
 'log_count/DEBUG': 84,
 'log_count/INFO': 11,
 'request_depth_max': 2,
 'response_received_count': 3,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 12, 6, 0, 2, 3, 141051)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: ef8661ccd5b45378
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
DEBUG: Attempting to acquire lock 1375770288496 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 1375770288496 acquired on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Attempting to release lock 1375770288496 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 1375770288496 released on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Crawled (200) <GET http://smithgill.com/robots.txt> (referer: None)
DEBUG: Rule at line 20 without any user agent to enforce it on.
DEBUG: Rule at line 36 without any user agent to enforce it on.
DEBUG: Rule at line 228 without any user agent to enforce it on.
DEBUG: Rule at line 229 without any user agent to enforce it on.
DEBUG: Rule at line 230 without any user agent to enforce it on.
DEBUG: Rule at line 238 without any user agent to enforce it on.
DEBUG: Rule at line 239 without any user agent to enforce it on.
DEBUG: Rule at line 240 without any user agent to enforce it on.
DEBUG: Rule at line 241 without any user agent to enforce it on.
DEBUG: Rule at line 242 without any user agent to enforce it on.
DEBUG: Rule at line 243 without any user agent to enforce it on.
DEBUG: Rule at line 244 without any user agent to enforce it on.
DEBUG: Rule at line 245 without any user agent to enforce it on.
DEBUG: Rule at line 246 without any user agent to enforce it on.
DEBUG: Rule at line 247 without any user agent to enforce it on.
DEBUG: Rule at line 248 without any user agent to enforce it on.
DEBUG: Rule at line 249 without any user agent to enforce it on.
DEBUG: Rule at line 250 without any user agent to enforce it on.
DEBUG: Rule at line 251 without any user agent to enforce it on.
DEBUG: Rule at line 285 without any user agent to enforce it on.
DEBUG: Rule at line 291 without any user agent to enforce it on.
DEBUG: Rule at line 297 without any user agent to enforce it on.
DEBUG: Rule at line 303 without any user agent to enforce it on.
DEBUG: Rule at line 309 without any user agent to enforce it on.
DEBUG: Rule at line 315 without any user agent to enforce it on.
DEBUG: Rule at line 321 without any user agent to enforce it on.
DEBUG: Rule at line 327 without any user agent to enforce it on.
DEBUG: Rule at line 333 without any user agent to enforce it on.
DEBUG: Rule at line 339 without any user agent to enforce it on.
DEBUG: Rule at line 353 without any user agent to enforce it on.
DEBUG: Rule at line 359 without any user agent to enforce it on.
DEBUG: Rule at line 365 without any user agent to enforce it on.
DEBUG: Rule at line 371 without any user agent to enforce it on.
DEBUG: Rule at line 377 without any user agent to enforce it on.
DEBUG: Rule at line 383 without any user agent to enforce it on.
DEBUG: Rule at line 389 without any user agent to enforce it on.
DEBUG: Rule at line 395 without any user agent to enforce it on.
DEBUG: Rule at line 401 without any user agent to enforce it on.
DEBUG: Rule at line 407 without any user agent to enforce it on.
DEBUG: Rule at line 413 without any user agent to enforce it on.
DEBUG: Rule at line 419 without any user agent to enforce it on.
DEBUG: Rule at line 440 without any user agent to enforce it on.
DEBUG: Rule at line 441 without any user agent to enforce it on.
DEBUG: Rule at line 442 without any user agent to enforce it on.
DEBUG: Rule at line 443 without any user agent to enforce it on.
DEBUG: Rule at line 449 without any user agent to enforce it on.
DEBUG: Rule at line 453 without any user agent to enforce it on.
DEBUG: Rule at line 454 without any user agent to enforce it on.
DEBUG: Rule at line 472 without any user agent to enforce it on.
DEBUG: Rule at line 481 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET http://smithgill.com/> (referer: None)
DEBUG: driver not found in PATH, trying Selenium Manager
DEBUG: Executing: C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\common\windows\selenium-manager.exe --browser chrome
DEBUG: Using driver at: C:\Users\moffi\.cache\selenium\chromedriver\win32\108.0.5359.71\chromedriver.exe
DEBUG: Started executable: `chromedriver` in a child process with pid: 20428
DEBUG: POST http://localhost:52074/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "pageLoadStrategy": "normal", "goog:chromeOptions": {"extensions": [], "args": []}}}}
DEBUG: Starting new HTTP connection (1): localhost:52074
DEBUG: http://localhost:52074 "POST /session HTTP/1.1" 200 790
DEBUG: Remote response: status=200 | data={"value":{"capabilities":{"acceptInsecureCerts":false,"browserName":"chrome","browserVersion":"108.0.5359.73","chrome":{"chromedriverVersion":"108.0.5359.71 (1e0e3868ee06e91ad636a874420e3ca3ae3756ac-refs/branch-heads/5359@{#1016})","userDataDir":"C:\\Users\\moffi\\AppData\\Local\\Temp\\scoped_dir20428_444834778"},"goog:chromeOptions":{"debuggerAddress":"localhost:52077"},"networkConnectionEnabled":false,"pageLoadStrategy":"normal","platformName":"windows","proxy":{},"setWindowRect":true,"strictFileInteractability":false,"timeouts":{"implicit":0,"pageLoad":300000,"script":30000},"unhandledPromptBehavior":"dismiss and notify","webauthn:extension:credBlob":true,"webauthn:extension:largeBlob":true,"webauthn:virtualAuthenticators":true},"sessionId":"458b276eb34697e0d41d1d0345d04fc1"}} | headers=HTTPHeaderDict({'Content-Length': '790', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:52074/session/458b276eb34697e0d41d1d0345d04fc1/url {"url": "http://smithgill.com/"}
DEBUG: http://localhost:52074 "POST /session/458b276eb34697e0d41d1d0345d04fc1/url HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:52074/session/458b276eb34697e0d41d1d0345d04fc1/timeouts {"implicit": 10000}
DEBUG: http://localhost:52074 "POST /session/458b276eb34697e0d41d1d0345d04fc1/timeouts HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: Filtered duplicate request: <GET http://smithgill.com/contact> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG: Scraped from <200 http://smithgill.com/>
{'facebook': 'https://www.facebook.com/smithgillarch'}
DEBUG: DELETE http://localhost:52074/session/458b276eb34697e0d41d1d0345d04fc1 {}
DEBUG: http://localhost:52074 "DELETE /session/458b276eb34697e0d41d1d0345d04fc1 HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: Redirecting (301) to <GET http://smithgill.com/contact/> from <GET http://smithgill.com/contact>
DEBUG: Crawled (200) <GET http://smithgill.com/contact/> (referer: http://smithgill.com/)
INFO: Closing spider (finished)
INFO: Stored json feed (1 items) in: website_spider_output.json
INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1252,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 18931,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/301': 1,
 'dupefilter/filtered': 2,
 'elapsed_time_seconds': 17.379686,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 6, 9, 28, 56, 34259),
 'httpcompression/response_bytes': 85365,
 'httpcompression/response_count': 3,
 'item_scraped_count': 1,
 'log_count/DEBUG': 84,
 'log_count/INFO': 11,
 'request_depth_max': 2,
 'response_received_count': 3,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 12, 6, 9, 28, 38, 654573)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: 283762aecb5a140d
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
DEBUG: Attempting to acquire lock 2717806405008 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2717806405008 acquired on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Attempting to release lock 2717806405008 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2717806405008 released on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Crawled (200) <GET http://smithgill.com/robots.txt> (referer: None)
DEBUG: Rule at line 20 without any user agent to enforce it on.
DEBUG: Rule at line 36 without any user agent to enforce it on.
DEBUG: Rule at line 228 without any user agent to enforce it on.
DEBUG: Rule at line 229 without any user agent to enforce it on.
DEBUG: Rule at line 230 without any user agent to enforce it on.
DEBUG: Rule at line 238 without any user agent to enforce it on.
DEBUG: Rule at line 239 without any user agent to enforce it on.
DEBUG: Rule at line 240 without any user agent to enforce it on.
DEBUG: Rule at line 241 without any user agent to enforce it on.
DEBUG: Rule at line 242 without any user agent to enforce it on.
DEBUG: Rule at line 243 without any user agent to enforce it on.
DEBUG: Rule at line 244 without any user agent to enforce it on.
DEBUG: Rule at line 245 without any user agent to enforce it on.
DEBUG: Rule at line 246 without any user agent to enforce it on.
DEBUG: Rule at line 247 without any user agent to enforce it on.
DEBUG: Rule at line 248 without any user agent to enforce it on.
DEBUG: Rule at line 249 without any user agent to enforce it on.
DEBUG: Rule at line 250 without any user agent to enforce it on.
DEBUG: Rule at line 251 without any user agent to enforce it on.
DEBUG: Rule at line 285 without any user agent to enforce it on.
DEBUG: Rule at line 291 without any user agent to enforce it on.
DEBUG: Rule at line 297 without any user agent to enforce it on.
DEBUG: Rule at line 303 without any user agent to enforce it on.
DEBUG: Rule at line 309 without any user agent to enforce it on.
DEBUG: Rule at line 315 without any user agent to enforce it on.
DEBUG: Rule at line 321 without any user agent to enforce it on.
DEBUG: Rule at line 327 without any user agent to enforce it on.
DEBUG: Rule at line 333 without any user agent to enforce it on.
DEBUG: Rule at line 339 without any user agent to enforce it on.
DEBUG: Rule at line 353 without any user agent to enforce it on.
DEBUG: Rule at line 359 without any user agent to enforce it on.
DEBUG: Rule at line 365 without any user agent to enforce it on.
DEBUG: Rule at line 371 without any user agent to enforce it on.
DEBUG: Rule at line 377 without any user agent to enforce it on.
DEBUG: Rule at line 383 without any user agent to enforce it on.
DEBUG: Rule at line 389 without any user agent to enforce it on.
DEBUG: Rule at line 395 without any user agent to enforce it on.
DEBUG: Rule at line 401 without any user agent to enforce it on.
DEBUG: Rule at line 407 without any user agent to enforce it on.
DEBUG: Rule at line 413 without any user agent to enforce it on.
DEBUG: Rule at line 419 without any user agent to enforce it on.
DEBUG: Rule at line 440 without any user agent to enforce it on.
DEBUG: Rule at line 441 without any user agent to enforce it on.
DEBUG: Rule at line 442 without any user agent to enforce it on.
DEBUG: Rule at line 443 without any user agent to enforce it on.
DEBUG: Rule at line 449 without any user agent to enforce it on.
DEBUG: Rule at line 453 without any user agent to enforce it on.
DEBUG: Rule at line 454 without any user agent to enforce it on.
DEBUG: Rule at line 472 without any user agent to enforce it on.
DEBUG: Rule at line 481 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET http://smithgill.com/> (referer: None)
DEBUG: driver not found in PATH, trying Selenium Manager
DEBUG: Executing: C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\common\windows\selenium-manager.exe --browser chrome
DEBUG: Using driver at: C:\Users\moffi\.cache\selenium\chromedriver\win32\108.0.5359.71\chromedriver.exe
DEBUG: Started executable: `chromedriver` in a child process with pid: 14128
DEBUG: POST http://localhost:52149/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "pageLoadStrategy": "normal", "goog:chromeOptions": {"extensions": [], "args": []}}}}
DEBUG: Starting new HTTP connection (1): localhost:52149
DEBUG: http://localhost:52149 "POST /session HTTP/1.1" 200 790
DEBUG: Remote response: status=200 | data={"value":{"capabilities":{"acceptInsecureCerts":false,"browserName":"chrome","browserVersion":"108.0.5359.73","chrome":{"chromedriverVersion":"108.0.5359.71 (1e0e3868ee06e91ad636a874420e3ca3ae3756ac-refs/branch-heads/5359@{#1016})","userDataDir":"C:\\Users\\moffi\\AppData\\Local\\Temp\\scoped_dir14128_447787574"},"goog:chromeOptions":{"debuggerAddress":"localhost:52152"},"networkConnectionEnabled":false,"pageLoadStrategy":"normal","platformName":"windows","proxy":{},"setWindowRect":true,"strictFileInteractability":false,"timeouts":{"implicit":0,"pageLoad":300000,"script":30000},"unhandledPromptBehavior":"dismiss and notify","webauthn:extension:credBlob":true,"webauthn:extension:largeBlob":true,"webauthn:virtualAuthenticators":true},"sessionId":"8c68d953356ec0a27582a6fd6f727023"}} | headers=HTTPHeaderDict({'Content-Length': '790', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:52149/session/8c68d953356ec0a27582a6fd6f727023/url {"url": "http://smithgill.com/"}
DEBUG: http://localhost:52149 "POST /session/8c68d953356ec0a27582a6fd6f727023/url HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:52149/session/8c68d953356ec0a27582a6fd6f727023/timeouts {"implicit": 10000}
DEBUG: http://localhost:52149 "POST /session/8c68d953356ec0a27582a6fd6f727023/timeouts HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: Filtered duplicate request: <GET http://smithgill.com/contact> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG: Scraped from <200 http://smithgill.com/>
{'facebook': 'https://www.facebook.com/smithgillarch'}
DEBUG: DELETE http://localhost:52149/session/8c68d953356ec0a27582a6fd6f727023 {}
DEBUG: http://localhost:52149 "DELETE /session/8c68d953356ec0a27582a6fd6f727023 HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: Redirecting (301) to <GET http://smithgill.com/contact/> from <GET http://smithgill.com/contact>
DEBUG: Crawled (200) <GET http://smithgill.com/contact/> (referer: http://smithgill.com/)
INFO: Closing spider (finished)
INFO: Stored json feed (1 items) in: website_spider_output.json
INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1252,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 18931,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/301': 1,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 16.656587,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 6, 9, 30, 7, 270818),
 'httpcompression/response_bytes': 85365,
 'httpcompression/response_count': 3,
 'item_scraped_count': 1,
 'log_count/DEBUG': 84,
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 3,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 12, 6, 9, 29, 50, 614231)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: affc52e95524307e
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
DEBUG: Attempting to acquire lock 2350901470368 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2350901470368 acquired on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Attempting to release lock 2350901470368 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2350901470368 released on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Crawled (200) <GET http://smithgill.com/robots.txt> (referer: None)
DEBUG: Rule at line 20 without any user agent to enforce it on.
DEBUG: Rule at line 36 without any user agent to enforce it on.
DEBUG: Rule at line 228 without any user agent to enforce it on.
DEBUG: Rule at line 229 without any user agent to enforce it on.
DEBUG: Rule at line 230 without any user agent to enforce it on.
DEBUG: Rule at line 238 without any user agent to enforce it on.
DEBUG: Rule at line 239 without any user agent to enforce it on.
DEBUG: Rule at line 240 without any user agent to enforce it on.
DEBUG: Rule at line 241 without any user agent to enforce it on.
DEBUG: Rule at line 242 without any user agent to enforce it on.
DEBUG: Rule at line 243 without any user agent to enforce it on.
DEBUG: Rule at line 244 without any user agent to enforce it on.
DEBUG: Rule at line 245 without any user agent to enforce it on.
DEBUG: Rule at line 246 without any user agent to enforce it on.
DEBUG: Rule at line 247 without any user agent to enforce it on.
DEBUG: Rule at line 248 without any user agent to enforce it on.
DEBUG: Rule at line 249 without any user agent to enforce it on.
DEBUG: Rule at line 250 without any user agent to enforce it on.
DEBUG: Rule at line 251 without any user agent to enforce it on.
DEBUG: Rule at line 285 without any user agent to enforce it on.
DEBUG: Rule at line 291 without any user agent to enforce it on.
DEBUG: Rule at line 297 without any user agent to enforce it on.
DEBUG: Rule at line 303 without any user agent to enforce it on.
DEBUG: Rule at line 309 without any user agent to enforce it on.
DEBUG: Rule at line 315 without any user agent to enforce it on.
DEBUG: Rule at line 321 without any user agent to enforce it on.
DEBUG: Rule at line 327 without any user agent to enforce it on.
DEBUG: Rule at line 333 without any user agent to enforce it on.
DEBUG: Rule at line 339 without any user agent to enforce it on.
DEBUG: Rule at line 353 without any user agent to enforce it on.
DEBUG: Rule at line 359 without any user agent to enforce it on.
DEBUG: Rule at line 365 without any user agent to enforce it on.
DEBUG: Rule at line 371 without any user agent to enforce it on.
DEBUG: Rule at line 377 without any user agent to enforce it on.
DEBUG: Rule at line 383 without any user agent to enforce it on.
DEBUG: Rule at line 389 without any user agent to enforce it on.
DEBUG: Rule at line 395 without any user agent to enforce it on.
DEBUG: Rule at line 401 without any user agent to enforce it on.
DEBUG: Rule at line 407 without any user agent to enforce it on.
DEBUG: Rule at line 413 without any user agent to enforce it on.
DEBUG: Rule at line 419 without any user agent to enforce it on.
DEBUG: Rule at line 440 without any user agent to enforce it on.
DEBUG: Rule at line 441 without any user agent to enforce it on.
DEBUG: Rule at line 442 without any user agent to enforce it on.
DEBUG: Rule at line 443 without any user agent to enforce it on.
DEBUG: Rule at line 449 without any user agent to enforce it on.
DEBUG: Rule at line 453 without any user agent to enforce it on.
DEBUG: Rule at line 454 without any user agent to enforce it on.
DEBUG: Rule at line 472 without any user agent to enforce it on.
DEBUG: Rule at line 481 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET http://smithgill.com/> (referer: None)
DEBUG: driver not found in PATH, trying Selenium Manager
DEBUG: Executing: C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\common\windows\selenium-manager.exe --browser chrome
DEBUG: Using driver at: C:\Users\moffi\.cache\selenium\chromedriver\win32\108.0.5359.71\chromedriver.exe
DEBUG: Started executable: `chromedriver` in a child process with pid: 25104
DEBUG: POST http://localhost:52217/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "pageLoadStrategy": "normal", "goog:chromeOptions": {"extensions": [], "args": []}}}}
DEBUG: Starting new HTTP connection (1): localhost:52217
DEBUG: http://localhost:52217 "POST /session HTTP/1.1" 200 791
DEBUG: Remote response: status=200 | data={"value":{"capabilities":{"acceptInsecureCerts":false,"browserName":"chrome","browserVersion":"108.0.5359.73","chrome":{"chromedriverVersion":"108.0.5359.71 (1e0e3868ee06e91ad636a874420e3ca3ae3756ac-refs/branch-heads/5359@{#1016})","userDataDir":"C:\\Users\\moffi\\AppData\\Local\\Temp\\scoped_dir25104_1910651868"},"goog:chromeOptions":{"debuggerAddress":"localhost:52220"},"networkConnectionEnabled":false,"pageLoadStrategy":"normal","platformName":"windows","proxy":{},"setWindowRect":true,"strictFileInteractability":false,"timeouts":{"implicit":0,"pageLoad":300000,"script":30000},"unhandledPromptBehavior":"dismiss and notify","webauthn:extension:credBlob":true,"webauthn:extension:largeBlob":true,"webauthn:virtualAuthenticators":true},"sessionId":"fcaa33da4510af84b4fe84ed93dd7630"}} | headers=HTTPHeaderDict({'Content-Length': '791', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:52217/session/fcaa33da4510af84b4fe84ed93dd7630/url {"url": "http://smithgill.com/"}
DEBUG: http://localhost:52217 "POST /session/fcaa33da4510af84b4fe84ed93dd7630/url HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:52217/session/fcaa33da4510af84b4fe84ed93dd7630/timeouts {"implicit": 10000}
DEBUG: http://localhost:52217 "POST /session/fcaa33da4510af84b4fe84ed93dd7630/timeouts HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: Scraped from <200 http://smithgill.com/>
{'facebook': 'https://www.facebook.com/smithgillarch'}
DEBUG: DELETE http://localhost:52217/session/fcaa33da4510af84b4fe84ed93dd7630 {}
DEBUG: http://localhost:52217 "DELETE /session/fcaa33da4510af84b4fe84ed93dd7630 HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
INFO: Closing spider (finished)
INFO: Stored json feed (1 items) in: website_spider_output.json
INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 501,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 12533,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 15.835927,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 6, 9, 30, 43, 405514),
 'httpcompression/response_bytes': 59820,
 'httpcompression/response_count': 2,
 'item_scraped_count': 1,
 'log_count/DEBUG': 81,
 'log_count/INFO': 11,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2022, 12, 6, 9, 30, 27, 569587)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: 5b20fedc61d40384
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
DEBUG: Attempting to acquire lock 2433226155408 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2433226155408 acquired on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Attempting to release lock 2433226155408 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2433226155408 released on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Crawled (200) <GET http://smithgill.com/robots.txt> (referer: None)
DEBUG: Rule at line 20 without any user agent to enforce it on.
DEBUG: Rule at line 36 without any user agent to enforce it on.
DEBUG: Rule at line 228 without any user agent to enforce it on.
DEBUG: Rule at line 229 without any user agent to enforce it on.
DEBUG: Rule at line 230 without any user agent to enforce it on.
DEBUG: Rule at line 238 without any user agent to enforce it on.
DEBUG: Rule at line 239 without any user agent to enforce it on.
DEBUG: Rule at line 240 without any user agent to enforce it on.
DEBUG: Rule at line 241 without any user agent to enforce it on.
DEBUG: Rule at line 242 without any user agent to enforce it on.
DEBUG: Rule at line 243 without any user agent to enforce it on.
DEBUG: Rule at line 244 without any user agent to enforce it on.
DEBUG: Rule at line 245 without any user agent to enforce it on.
DEBUG: Rule at line 246 without any user agent to enforce it on.
DEBUG: Rule at line 247 without any user agent to enforce it on.
DEBUG: Rule at line 248 without any user agent to enforce it on.
DEBUG: Rule at line 249 without any user agent to enforce it on.
DEBUG: Rule at line 250 without any user agent to enforce it on.
DEBUG: Rule at line 251 without any user agent to enforce it on.
DEBUG: Rule at line 285 without any user agent to enforce it on.
DEBUG: Rule at line 291 without any user agent to enforce it on.
DEBUG: Rule at line 297 without any user agent to enforce it on.
DEBUG: Rule at line 303 without any user agent to enforce it on.
DEBUG: Rule at line 309 without any user agent to enforce it on.
DEBUG: Rule at line 315 without any user agent to enforce it on.
DEBUG: Rule at line 321 without any user agent to enforce it on.
DEBUG: Rule at line 327 without any user agent to enforce it on.
DEBUG: Rule at line 333 without any user agent to enforce it on.
DEBUG: Rule at line 339 without any user agent to enforce it on.
DEBUG: Rule at line 353 without any user agent to enforce it on.
DEBUG: Rule at line 359 without any user agent to enforce it on.
DEBUG: Rule at line 365 without any user agent to enforce it on.
DEBUG: Rule at line 371 without any user agent to enforce it on.
DEBUG: Rule at line 377 without any user agent to enforce it on.
DEBUG: Rule at line 383 without any user agent to enforce it on.
DEBUG: Rule at line 389 without any user agent to enforce it on.
DEBUG: Rule at line 395 without any user agent to enforce it on.
DEBUG: Rule at line 401 without any user agent to enforce it on.
DEBUG: Rule at line 407 without any user agent to enforce it on.
DEBUG: Rule at line 413 without any user agent to enforce it on.
DEBUG: Rule at line 419 without any user agent to enforce it on.
DEBUG: Rule at line 440 without any user agent to enforce it on.
DEBUG: Rule at line 441 without any user agent to enforce it on.
DEBUG: Rule at line 442 without any user agent to enforce it on.
DEBUG: Rule at line 443 without any user agent to enforce it on.
DEBUG: Rule at line 449 without any user agent to enforce it on.
DEBUG: Rule at line 453 without any user agent to enforce it on.
DEBUG: Rule at line 454 without any user agent to enforce it on.
DEBUG: Rule at line 472 without any user agent to enforce it on.
DEBUG: Rule at line 481 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET http://smithgill.com/> (referer: None)
DEBUG: driver not found in PATH, trying Selenium Manager
DEBUG: Executing: C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\common\windows\selenium-manager.exe --browser chrome
DEBUG: Using driver at: C:\Users\moffi\.cache\selenium\chromedriver\win32\108.0.5359.71\chromedriver.exe
DEBUG: Started executable: `chromedriver` in a child process with pid: 3816
DEBUG: POST http://localhost:52282/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "pageLoadStrategy": "normal", "goog:chromeOptions": {"extensions": [], "args": []}}}}
DEBUG: Starting new HTTP connection (1): localhost:52282
DEBUG: http://localhost:52282 "POST /session HTTP/1.1" 200 790
DEBUG: Remote response: status=200 | data={"value":{"capabilities":{"acceptInsecureCerts":false,"browserName":"chrome","browserVersion":"108.0.5359.73","chrome":{"chromedriverVersion":"108.0.5359.71 (1e0e3868ee06e91ad636a874420e3ca3ae3756ac-refs/branch-heads/5359@{#1016})","userDataDir":"C:\\Users\\moffi\\AppData\\Local\\Temp\\scoped_dir3816_1587810755"},"goog:chromeOptions":{"debuggerAddress":"localhost:52285"},"networkConnectionEnabled":false,"pageLoadStrategy":"normal","platformName":"windows","proxy":{},"setWindowRect":true,"strictFileInteractability":false,"timeouts":{"implicit":0,"pageLoad":300000,"script":30000},"unhandledPromptBehavior":"dismiss and notify","webauthn:extension:credBlob":true,"webauthn:extension:largeBlob":true,"webauthn:virtualAuthenticators":true},"sessionId":"6174ffa3da4c5914c9d832214c1526ff"}} | headers=HTTPHeaderDict({'Content-Length': '790', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:52282/session/6174ffa3da4c5914c9d832214c1526ff/url {"url": "http://smithgill.com/"}
DEBUG: http://localhost:52282 "POST /session/6174ffa3da4c5914c9d832214c1526ff/url HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:52282/session/6174ffa3da4c5914c9d832214c1526ff/timeouts {"implicit": 10000}
DEBUG: http://localhost:52282 "POST /session/6174ffa3da4c5914c9d832214c1526ff/timeouts HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: Filtered duplicate request: <GET http://smithgill.com/contact> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG: Scraped from <200 http://smithgill.com/>
{'facebook': 'https://www.facebook.com/smithgillarch'}
DEBUG: DELETE http://localhost:52282/session/6174ffa3da4c5914c9d832214c1526ff {}
DEBUG: http://localhost:52282 "DELETE /session/6174ffa3da4c5914c9d832214c1526ff HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: Redirecting (301) to <GET http://smithgill.com/contact/> from <GET http://smithgill.com/contact>
DEBUG: Crawled (200) <GET http://smithgill.com/contact/> (referer: http://smithgill.com/)
INFO: Closing spider (finished)
INFO: Stored json feed (1 items) in: website_spider_output.json
INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1252,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 18931,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/301': 1,
 'dupefilter/filtered': 2,
 'elapsed_time_seconds': 17.05021,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 6, 9, 31, 29, 714641),
 'httpcompression/response_bytes': 85365,
 'httpcompression/response_count': 3,
 'item_scraped_count': 1,
 'log_count/DEBUG': 84,
 'log_count/INFO': 11,
 'request_depth_max': 2,
 'response_received_count': 3,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 12, 6, 9, 31, 12, 664431)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: 0d813e74b4642481
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
DEBUG: Attempting to acquire lock 1687325826768 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 1687325826768 acquired on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Attempting to release lock 1687325826768 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 1687325826768 released on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Crawled (200) <GET http://smithgill.com/robots.txt> (referer: None)
DEBUG: Rule at line 20 without any user agent to enforce it on.
DEBUG: Rule at line 36 without any user agent to enforce it on.
DEBUG: Rule at line 228 without any user agent to enforce it on.
DEBUG: Rule at line 229 without any user agent to enforce it on.
DEBUG: Rule at line 230 without any user agent to enforce it on.
DEBUG: Rule at line 238 without any user agent to enforce it on.
DEBUG: Rule at line 239 without any user agent to enforce it on.
DEBUG: Rule at line 240 without any user agent to enforce it on.
DEBUG: Rule at line 241 without any user agent to enforce it on.
DEBUG: Rule at line 242 without any user agent to enforce it on.
DEBUG: Rule at line 243 without any user agent to enforce it on.
DEBUG: Rule at line 244 without any user agent to enforce it on.
DEBUG: Rule at line 245 without any user agent to enforce it on.
DEBUG: Rule at line 246 without any user agent to enforce it on.
DEBUG: Rule at line 247 without any user agent to enforce it on.
DEBUG: Rule at line 248 without any user agent to enforce it on.
DEBUG: Rule at line 249 without any user agent to enforce it on.
DEBUG: Rule at line 250 without any user agent to enforce it on.
DEBUG: Rule at line 251 without any user agent to enforce it on.
DEBUG: Rule at line 285 without any user agent to enforce it on.
DEBUG: Rule at line 291 without any user agent to enforce it on.
DEBUG: Rule at line 297 without any user agent to enforce it on.
DEBUG: Rule at line 303 without any user agent to enforce it on.
DEBUG: Rule at line 309 without any user agent to enforce it on.
DEBUG: Rule at line 315 without any user agent to enforce it on.
DEBUG: Rule at line 321 without any user agent to enforce it on.
DEBUG: Rule at line 327 without any user agent to enforce it on.
DEBUG: Rule at line 333 without any user agent to enforce it on.
DEBUG: Rule at line 339 without any user agent to enforce it on.
DEBUG: Rule at line 353 without any user agent to enforce it on.
DEBUG: Rule at line 359 without any user agent to enforce it on.
DEBUG: Rule at line 365 without any user agent to enforce it on.
DEBUG: Rule at line 371 without any user agent to enforce it on.
DEBUG: Rule at line 377 without any user agent to enforce it on.
DEBUG: Rule at line 383 without any user agent to enforce it on.
DEBUG: Rule at line 389 without any user agent to enforce it on.
DEBUG: Rule at line 395 without any user agent to enforce it on.
DEBUG: Rule at line 401 without any user agent to enforce it on.
DEBUG: Rule at line 407 without any user agent to enforce it on.
DEBUG: Rule at line 413 without any user agent to enforce it on.
DEBUG: Rule at line 419 without any user agent to enforce it on.
DEBUG: Rule at line 440 without any user agent to enforce it on.
DEBUG: Rule at line 441 without any user agent to enforce it on.
DEBUG: Rule at line 442 without any user agent to enforce it on.
DEBUG: Rule at line 443 without any user agent to enforce it on.
DEBUG: Rule at line 449 without any user agent to enforce it on.
DEBUG: Rule at line 453 without any user agent to enforce it on.
DEBUG: Rule at line 454 without any user agent to enforce it on.
DEBUG: Rule at line 472 without any user agent to enforce it on.
DEBUG: Rule at line 481 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET http://smithgill.com/> (referer: None)
DEBUG: driver not found in PATH, trying Selenium Manager
DEBUG: Executing: C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\common\windows\selenium-manager.exe --browser chrome
DEBUG: Using driver at: C:\Users\moffi\.cache\selenium\chromedriver\win32\108.0.5359.71\chromedriver.exe
DEBUG: Started executable: `chromedriver` in a child process with pid: 9700
DEBUG: POST http://localhost:50911/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "pageLoadStrategy": "normal", "goog:chromeOptions": {"extensions": [], "args": []}}}}
DEBUG: Starting new HTTP connection (1): localhost:50911
DEBUG: http://localhost:50911 "POST /session HTTP/1.1" 200 789
DEBUG: Remote response: status=200 | data={"value":{"capabilities":{"acceptInsecureCerts":false,"browserName":"chrome","browserVersion":"108.0.5359.98","chrome":{"chromedriverVersion":"108.0.5359.71 (1e0e3868ee06e91ad636a874420e3ca3ae3756ac-refs/branch-heads/5359@{#1016})","userDataDir":"C:\\Users\\moffi\\AppData\\Local\\Temp\\scoped_dir9700_903949772"},"goog:chromeOptions":{"debuggerAddress":"localhost:50915"},"networkConnectionEnabled":false,"pageLoadStrategy":"normal","platformName":"windows","proxy":{},"setWindowRect":true,"strictFileInteractability":false,"timeouts":{"implicit":0,"pageLoad":300000,"script":30000},"unhandledPromptBehavior":"dismiss and notify","webauthn:extension:credBlob":true,"webauthn:extension:largeBlob":true,"webauthn:virtualAuthenticators":true},"sessionId":"87c684a523448f90c5150c44ef55740e"}} | headers=HTTPHeaderDict({'Content-Length': '789', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:50911/session/87c684a523448f90c5150c44ef55740e/url {"url": "http://smithgill.com/"}
DEBUG: http://localhost:50911 "POST /session/87c684a523448f90c5150c44ef55740e/url HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:50911/session/87c684a523448f90c5150c44ef55740e/timeouts {"implicit": 10000}
DEBUG: http://localhost:50911 "POST /session/87c684a523448f90c5150c44ef55740e/timeouts HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: Filtered duplicate request: <GET http://smithgill.com/contact> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG: Scraped from <200 http://smithgill.com/>
{'facebook': 'https://www.facebook.com/smithgillarch'}
DEBUG: DELETE http://localhost:50911/session/87c684a523448f90c5150c44ef55740e {}
DEBUG: http://localhost:50911 "DELETE /session/87c684a523448f90c5150c44ef55740e HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: Redirecting (301) to <GET http://smithgill.com/contact/> from <GET http://smithgill.com/contact>
DEBUG: Crawled (200) <GET http://smithgill.com/contact/> (referer: http://smithgill.com/)
INFO: Closing spider (finished)
INFO: Stored json feed (1 items) in: website_spider_output.json
INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1252,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 18931,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/301': 1,
 'dupefilter/filtered': 2,
 'elapsed_time_seconds': 18.03273,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 8, 19, 32, 29, 194945),
 'httpcompression/response_bytes': 85365,
 'httpcompression/response_count': 3,
 'item_scraped_count': 1,
 'log_count/DEBUG': 84,
 'log_count/INFO': 11,
 'request_depth_max': 2,
 'response_received_count': 3,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 12, 8, 19, 32, 11, 162215)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: c9746dcc08e7cdfa
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
DEBUG: Attempting to acquire lock 2261708677392 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2261708677392 acquired on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Attempting to release lock 2261708677392 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2261708677392 released on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Crawled (200) <GET http://smithgill.com/robots.txt> (referer: None)
DEBUG: Rule at line 20 without any user agent to enforce it on.
DEBUG: Rule at line 36 without any user agent to enforce it on.
DEBUG: Rule at line 228 without any user agent to enforce it on.
DEBUG: Rule at line 229 without any user agent to enforce it on.
DEBUG: Rule at line 230 without any user agent to enforce it on.
DEBUG: Rule at line 238 without any user agent to enforce it on.
DEBUG: Rule at line 239 without any user agent to enforce it on.
DEBUG: Rule at line 240 without any user agent to enforce it on.
DEBUG: Rule at line 241 without any user agent to enforce it on.
DEBUG: Rule at line 242 without any user agent to enforce it on.
DEBUG: Rule at line 243 without any user agent to enforce it on.
DEBUG: Rule at line 244 without any user agent to enforce it on.
DEBUG: Rule at line 245 without any user agent to enforce it on.
DEBUG: Rule at line 246 without any user agent to enforce it on.
DEBUG: Rule at line 247 without any user agent to enforce it on.
DEBUG: Rule at line 248 without any user agent to enforce it on.
DEBUG: Rule at line 249 without any user agent to enforce it on.
DEBUG: Rule at line 250 without any user agent to enforce it on.
DEBUG: Rule at line 251 without any user agent to enforce it on.
DEBUG: Rule at line 285 without any user agent to enforce it on.
DEBUG: Rule at line 291 without any user agent to enforce it on.
DEBUG: Rule at line 297 without any user agent to enforce it on.
DEBUG: Rule at line 303 without any user agent to enforce it on.
DEBUG: Rule at line 309 without any user agent to enforce it on.
DEBUG: Rule at line 315 without any user agent to enforce it on.
DEBUG: Rule at line 321 without any user agent to enforce it on.
DEBUG: Rule at line 327 without any user agent to enforce it on.
DEBUG: Rule at line 333 without any user agent to enforce it on.
DEBUG: Rule at line 339 without any user agent to enforce it on.
DEBUG: Rule at line 353 without any user agent to enforce it on.
DEBUG: Rule at line 359 without any user agent to enforce it on.
DEBUG: Rule at line 365 without any user agent to enforce it on.
DEBUG: Rule at line 371 without any user agent to enforce it on.
DEBUG: Rule at line 377 without any user agent to enforce it on.
DEBUG: Rule at line 383 without any user agent to enforce it on.
DEBUG: Rule at line 389 without any user agent to enforce it on.
DEBUG: Rule at line 395 without any user agent to enforce it on.
DEBUG: Rule at line 401 without any user agent to enforce it on.
DEBUG: Rule at line 407 without any user agent to enforce it on.
DEBUG: Rule at line 413 without any user agent to enforce it on.
DEBUG: Rule at line 419 without any user agent to enforce it on.
DEBUG: Rule at line 440 without any user agent to enforce it on.
DEBUG: Rule at line 441 without any user agent to enforce it on.
DEBUG: Rule at line 442 without any user agent to enforce it on.
DEBUG: Rule at line 443 without any user agent to enforce it on.
DEBUG: Rule at line 449 without any user agent to enforce it on.
DEBUG: Rule at line 453 without any user agent to enforce it on.
DEBUG: Rule at line 454 without any user agent to enforce it on.
DEBUG: Rule at line 472 without any user agent to enforce it on.
DEBUG: Rule at line 481 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET http://smithgill.com/> (referer: None)
DEBUG: driver not found in PATH, trying Selenium Manager
DEBUG: Executing: C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\common\windows\selenium-manager.exe --browser chrome
DEBUG: Using driver at: C:\Users\moffi\.cache\selenium\chromedriver\win32\108.0.5359.71\chromedriver.exe
DEBUG: Started executable: `chromedriver` in a child process with pid: 36268
DEBUG: POST http://localhost:50980/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "pageLoadStrategy": "normal", "goog:chromeOptions": {"extensions": [], "args": []}}}}
DEBUG: Starting new HTTP connection (1): localhost:50980
DEBUG: http://localhost:50980 "POST /session HTTP/1.1" 200 790
DEBUG: Remote response: status=200 | data={"value":{"capabilities":{"acceptInsecureCerts":false,"browserName":"chrome","browserVersion":"108.0.5359.98","chrome":{"chromedriverVersion":"108.0.5359.71 (1e0e3868ee06e91ad636a874420e3ca3ae3756ac-refs/branch-heads/5359@{#1016})","userDataDir":"C:\\Users\\moffi\\AppData\\Local\\Temp\\scoped_dir36268_752997432"},"goog:chromeOptions":{"debuggerAddress":"localhost:50983"},"networkConnectionEnabled":false,"pageLoadStrategy":"normal","platformName":"windows","proxy":{},"setWindowRect":true,"strictFileInteractability":false,"timeouts":{"implicit":0,"pageLoad":300000,"script":30000},"unhandledPromptBehavior":"dismiss and notify","webauthn:extension:credBlob":true,"webauthn:extension:largeBlob":true,"webauthn:virtualAuthenticators":true},"sessionId":"5a5aba8db20ba9fb97469b9cf619cd04"}} | headers=HTTPHeaderDict({'Content-Length': '790', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:50980/session/5a5aba8db20ba9fb97469b9cf619cd04/url {"url": "http://smithgill.com/"}
DEBUG: http://localhost:50980 "POST /session/5a5aba8db20ba9fb97469b9cf619cd04/url HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:50980/session/5a5aba8db20ba9fb97469b9cf619cd04/timeouts {"implicit": 10000}
DEBUG: http://localhost:50980 "POST /session/5a5aba8db20ba9fb97469b9cf619cd04/timeouts HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: Filtered duplicate request: <GET http://smithgill.com/contact> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG: Scraped from <200 http://smithgill.com/>
{'facebook': 'https://www.facebook.com/smithgillarch'}
DEBUG: DELETE http://localhost:50980/session/5a5aba8db20ba9fb97469b9cf619cd04 {}
DEBUG: http://localhost:50980 "DELETE /session/5a5aba8db20ba9fb97469b9cf619cd04 HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: Redirecting (301) to <GET http://smithgill.com/contact/> from <GET http://smithgill.com/contact>
DEBUG: Crawled (200) <GET http://smithgill.com/contact/> (referer: http://smithgill.com/)
INFO: Closing spider (finished)
INFO: Stored json feed (1 items) in: website_spider_output.json
INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1252,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 18931,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/301': 1,
 'dupefilter/filtered': 2,
 'elapsed_time_seconds': 18.006735,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 8, 19, 33, 43, 228139),
 'httpcompression/response_bytes': 85365,
 'httpcompression/response_count': 3,
 'item_scraped_count': 1,
 'log_count/DEBUG': 84,
 'log_count/INFO': 11,
 'request_depth_max': 2,
 'response_received_count': 3,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 12, 8, 19, 33, 25, 221404)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: b135c9e51a268f2c
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
DEBUG: Attempting to acquire lock 2109861568976 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2109861568976 acquired on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Attempting to release lock 2109861568976 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2109861568976 released on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Crawled (200) <GET http://smithgill.com/robots.txt> (referer: None)
DEBUG: Rule at line 20 without any user agent to enforce it on.
DEBUG: Rule at line 36 without any user agent to enforce it on.
DEBUG: Rule at line 228 without any user agent to enforce it on.
DEBUG: Rule at line 229 without any user agent to enforce it on.
DEBUG: Rule at line 230 without any user agent to enforce it on.
DEBUG: Rule at line 238 without any user agent to enforce it on.
DEBUG: Rule at line 239 without any user agent to enforce it on.
DEBUG: Rule at line 240 without any user agent to enforce it on.
DEBUG: Rule at line 241 without any user agent to enforce it on.
DEBUG: Rule at line 242 without any user agent to enforce it on.
DEBUG: Rule at line 243 without any user agent to enforce it on.
DEBUG: Rule at line 244 without any user agent to enforce it on.
DEBUG: Rule at line 245 without any user agent to enforce it on.
DEBUG: Rule at line 246 without any user agent to enforce it on.
DEBUG: Rule at line 247 without any user agent to enforce it on.
DEBUG: Rule at line 248 without any user agent to enforce it on.
DEBUG: Rule at line 249 without any user agent to enforce it on.
DEBUG: Rule at line 250 without any user agent to enforce it on.
DEBUG: Rule at line 251 without any user agent to enforce it on.
DEBUG: Rule at line 285 without any user agent to enforce it on.
DEBUG: Rule at line 291 without any user agent to enforce it on.
DEBUG: Rule at line 297 without any user agent to enforce it on.
DEBUG: Rule at line 303 without any user agent to enforce it on.
DEBUG: Rule at line 309 without any user agent to enforce it on.
DEBUG: Rule at line 315 without any user agent to enforce it on.
DEBUG: Rule at line 321 without any user agent to enforce it on.
DEBUG: Rule at line 327 without any user agent to enforce it on.
DEBUG: Rule at line 333 without any user agent to enforce it on.
DEBUG: Rule at line 339 without any user agent to enforce it on.
DEBUG: Rule at line 353 without any user agent to enforce it on.
DEBUG: Rule at line 359 without any user agent to enforce it on.
DEBUG: Rule at line 365 without any user agent to enforce it on.
DEBUG: Rule at line 371 without any user agent to enforce it on.
DEBUG: Rule at line 377 without any user agent to enforce it on.
DEBUG: Rule at line 383 without any user agent to enforce it on.
DEBUG: Rule at line 389 without any user agent to enforce it on.
DEBUG: Rule at line 395 without any user agent to enforce it on.
DEBUG: Rule at line 401 without any user agent to enforce it on.
DEBUG: Rule at line 407 without any user agent to enforce it on.
DEBUG: Rule at line 413 without any user agent to enforce it on.
DEBUG: Rule at line 419 without any user agent to enforce it on.
DEBUG: Rule at line 440 without any user agent to enforce it on.
DEBUG: Rule at line 441 without any user agent to enforce it on.
DEBUG: Rule at line 442 without any user agent to enforce it on.
DEBUG: Rule at line 443 without any user agent to enforce it on.
DEBUG: Rule at line 449 without any user agent to enforce it on.
DEBUG: Rule at line 453 without any user agent to enforce it on.
DEBUG: Rule at line 454 without any user agent to enforce it on.
DEBUG: Rule at line 472 without any user agent to enforce it on.
DEBUG: Rule at line 481 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET http://smithgill.com/> (referer: None)
DEBUG: driver not found in PATH, trying Selenium Manager
DEBUG: Executing: C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\common\windows\selenium-manager.exe --browser chrome
DEBUG: Using driver at: C:\Users\moffi\.cache\selenium\chromedriver\win32\108.0.5359.71\chromedriver.exe
DEBUG: Started executable: `chromedriver` in a child process with pid: 33712
DEBUG: POST http://localhost:60095/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "pageLoadStrategy": "normal", "goog:chromeOptions": {"extensions": [], "args": []}}}}
DEBUG: Starting new HTTP connection (1): localhost:60095
DEBUG: http://localhost:60095 "POST /session HTTP/1.1" 200 790
DEBUG: Remote response: status=200 | data={"value":{"capabilities":{"acceptInsecureCerts":false,"browserName":"chrome","browserVersion":"108.0.5359.98","chrome":{"chromedriverVersion":"108.0.5359.71 (1e0e3868ee06e91ad636a874420e3ca3ae3756ac-refs/branch-heads/5359@{#1016})","userDataDir":"C:\\Users\\moffi\\AppData\\Local\\Temp\\scoped_dir33712_835472672"},"goog:chromeOptions":{"debuggerAddress":"localhost:60098"},"networkConnectionEnabled":false,"pageLoadStrategy":"normal","platformName":"windows","proxy":{},"setWindowRect":true,"strictFileInteractability":false,"timeouts":{"implicit":0,"pageLoad":300000,"script":30000},"unhandledPromptBehavior":"dismiss and notify","webauthn:extension:credBlob":true,"webauthn:extension:largeBlob":true,"webauthn:virtualAuthenticators":true},"sessionId":"d508a53b3d90f437cd7d922c52eaaa68"}} | headers=HTTPHeaderDict({'Content-Length': '790', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:60095/session/d508a53b3d90f437cd7d922c52eaaa68/url {"url": "http://smithgill.com/"}
DEBUG: http://localhost:60095 "POST /session/d508a53b3d90f437cd7d922c52eaaa68/url HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:60095/session/d508a53b3d90f437cd7d922c52eaaa68/timeouts {"implicit": 10000}
DEBUG: http://localhost:60095 "POST /session/d508a53b3d90f437cd7d922c52eaaa68/timeouts HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: Filtered duplicate request: <GET http://smithgill.com/contact> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG: Scraped from <200 http://smithgill.com/>
{'facebook': 'https://www.facebook.com/smithgillarch'}
DEBUG: DELETE http://localhost:60095/session/d508a53b3d90f437cd7d922c52eaaa68 {}
DEBUG: http://localhost:60095 "DELETE /session/d508a53b3d90f437cd7d922c52eaaa68 HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: Redirecting (301) to <GET http://smithgill.com/contact/> from <GET http://smithgill.com/contact>
DEBUG: Crawled (200) <GET http://smithgill.com/contact/> (referer: http://smithgill.com/)
INFO: Closing spider (finished)
INFO: Stored json feed (1 items) in: website_spider_output.json
INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1252,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 18931,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/301': 1,
 'dupefilter/filtered': 2,
 'elapsed_time_seconds': 18.189901,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 8, 19, 41, 45, 25030),
 'httpcompression/response_bytes': 85365,
 'httpcompression/response_count': 3,
 'item_scraped_count': 1,
 'log_count/DEBUG': 84,
 'log_count/INFO': 11,
 'request_depth_max': 2,
 'response_received_count': 3,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 12, 8, 19, 41, 26, 835129)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: 9c2b773f8c27b9bf
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
DEBUG: Attempting to acquire lock 1833405621312 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 1833405621312 acquired on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Attempting to release lock 1833405621312 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 1833405621312 released on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Crawled (200) <GET http://smithgill.com/robots.txt> (referer: None)
DEBUG: Rule at line 20 without any user agent to enforce it on.
DEBUG: Rule at line 36 without any user agent to enforce it on.
DEBUG: Rule at line 228 without any user agent to enforce it on.
DEBUG: Rule at line 229 without any user agent to enforce it on.
DEBUG: Rule at line 230 without any user agent to enforce it on.
DEBUG: Rule at line 238 without any user agent to enforce it on.
DEBUG: Rule at line 239 without any user agent to enforce it on.
DEBUG: Rule at line 240 without any user agent to enforce it on.
DEBUG: Rule at line 241 without any user agent to enforce it on.
DEBUG: Rule at line 242 without any user agent to enforce it on.
DEBUG: Rule at line 243 without any user agent to enforce it on.
DEBUG: Rule at line 244 without any user agent to enforce it on.
DEBUG: Rule at line 245 without any user agent to enforce it on.
DEBUG: Rule at line 246 without any user agent to enforce it on.
DEBUG: Rule at line 247 without any user agent to enforce it on.
DEBUG: Rule at line 248 without any user agent to enforce it on.
DEBUG: Rule at line 249 without any user agent to enforce it on.
DEBUG: Rule at line 250 without any user agent to enforce it on.
DEBUG: Rule at line 251 without any user agent to enforce it on.
DEBUG: Rule at line 285 without any user agent to enforce it on.
DEBUG: Rule at line 291 without any user agent to enforce it on.
DEBUG: Rule at line 297 without any user agent to enforce it on.
DEBUG: Rule at line 303 without any user agent to enforce it on.
DEBUG: Rule at line 309 without any user agent to enforce it on.
DEBUG: Rule at line 315 without any user agent to enforce it on.
DEBUG: Rule at line 321 without any user agent to enforce it on.
DEBUG: Rule at line 327 without any user agent to enforce it on.
DEBUG: Rule at line 333 without any user agent to enforce it on.
DEBUG: Rule at line 339 without any user agent to enforce it on.
DEBUG: Rule at line 353 without any user agent to enforce it on.
DEBUG: Rule at line 359 without any user agent to enforce it on.
DEBUG: Rule at line 365 without any user agent to enforce it on.
DEBUG: Rule at line 371 without any user agent to enforce it on.
DEBUG: Rule at line 377 without any user agent to enforce it on.
DEBUG: Rule at line 383 without any user agent to enforce it on.
DEBUG: Rule at line 389 without any user agent to enforce it on.
DEBUG: Rule at line 395 without any user agent to enforce it on.
DEBUG: Rule at line 401 without any user agent to enforce it on.
DEBUG: Rule at line 407 without any user agent to enforce it on.
DEBUG: Rule at line 413 without any user agent to enforce it on.
DEBUG: Rule at line 419 without any user agent to enforce it on.
DEBUG: Rule at line 440 without any user agent to enforce it on.
DEBUG: Rule at line 441 without any user agent to enforce it on.
DEBUG: Rule at line 442 without any user agent to enforce it on.
DEBUG: Rule at line 443 without any user agent to enforce it on.
DEBUG: Rule at line 449 without any user agent to enforce it on.
DEBUG: Rule at line 453 without any user agent to enforce it on.
DEBUG: Rule at line 454 without any user agent to enforce it on.
DEBUG: Rule at line 472 without any user agent to enforce it on.
DEBUG: Rule at line 481 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET http://smithgill.com/> (referer: None)
DEBUG: driver not found in PATH, trying Selenium Manager
DEBUG: Executing: C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\common\windows\selenium-manager.exe --browser chrome
DEBUG: Using driver at: C:\Users\moffi\.cache\selenium\chromedriver\win32\108.0.5359.71\chromedriver.exe
DEBUG: Started executable: `chromedriver` in a child process with pid: 21548
DEBUG: POST http://localhost:60178/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "pageLoadStrategy": "normal", "goog:chromeOptions": {"extensions": [], "args": []}}}}
DEBUG: Starting new HTTP connection (1): localhost:60178
DEBUG: http://localhost:60178 "POST /session HTTP/1.1" 200 791
DEBUG: Remote response: status=200 | data={"value":{"capabilities":{"acceptInsecureCerts":false,"browserName":"chrome","browserVersion":"108.0.5359.98","chrome":{"chromedriverVersion":"108.0.5359.71 (1e0e3868ee06e91ad636a874420e3ca3ae3756ac-refs/branch-heads/5359@{#1016})","userDataDir":"C:\\Users\\moffi\\AppData\\Local\\Temp\\scoped_dir21548_1158249242"},"goog:chromeOptions":{"debuggerAddress":"localhost:60181"},"networkConnectionEnabled":false,"pageLoadStrategy":"normal","platformName":"windows","proxy":{},"setWindowRect":true,"strictFileInteractability":false,"timeouts":{"implicit":0,"pageLoad":300000,"script":30000},"unhandledPromptBehavior":"dismiss and notify","webauthn:extension:credBlob":true,"webauthn:extension:largeBlob":true,"webauthn:virtualAuthenticators":true},"sessionId":"0821091a9ee2810323285944c797f066"}} | headers=HTTPHeaderDict({'Content-Length': '791', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:60178/session/0821091a9ee2810323285944c797f066/url {"url": "http://smithgill.com/"}
DEBUG: http://localhost:60178 "POST /session/0821091a9ee2810323285944c797f066/url HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:60178/session/0821091a9ee2810323285944c797f066/timeouts {"implicit": 10000}
DEBUG: http://localhost:60178 "POST /session/0821091a9ee2810323285944c797f066/timeouts HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: Filtered duplicate request: <GET http://smithgill.com/contact> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG: Scraped from <200 http://smithgill.com/>
{'facebook': 'https://www.facebook.com/smithgillarch'}
DEBUG: DELETE http://localhost:60178/session/0821091a9ee2810323285944c797f066 {}
DEBUG: http://localhost:60178 "DELETE /session/0821091a9ee2810323285944c797f066 HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: Redirecting (301) to <GET http://smithgill.com/contact/> from <GET http://smithgill.com/contact>
DEBUG: Crawled (200) <GET http://smithgill.com/contact/> (referer: http://smithgill.com/)
ERROR: Spider must return request, item, or None, got 'list' in <GET http://smithgill.com/contact/>
INFO: Closing spider (finished)
INFO: Stored json feed (1 items) in: website_spider_output.json
INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1252,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 18931,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/301': 1,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 18.137216,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 8, 19, 47, 43, 104396),
 'httpcompression/response_bytes': 85365,
 'httpcompression/response_count': 3,
 'item_scraped_count': 1,
 'log_count/DEBUG': 84,
 'log_count/ERROR': 1,
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 3,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 12, 8, 19, 47, 24, 967180)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: b105392a9c7c798f
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
DEBUG: Attempting to acquire lock 2384008486032 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2384008486032 acquired on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Attempting to release lock 2384008486032 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2384008486032 released on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Crawled (200) <GET http://smithgill.com/robots.txt> (referer: None)
DEBUG: Rule at line 20 without any user agent to enforce it on.
DEBUG: Rule at line 36 without any user agent to enforce it on.
DEBUG: Rule at line 228 without any user agent to enforce it on.
DEBUG: Rule at line 229 without any user agent to enforce it on.
DEBUG: Rule at line 230 without any user agent to enforce it on.
DEBUG: Rule at line 238 without any user agent to enforce it on.
DEBUG: Rule at line 239 without any user agent to enforce it on.
DEBUG: Rule at line 240 without any user agent to enforce it on.
DEBUG: Rule at line 241 without any user agent to enforce it on.
DEBUG: Rule at line 242 without any user agent to enforce it on.
DEBUG: Rule at line 243 without any user agent to enforce it on.
DEBUG: Rule at line 244 without any user agent to enforce it on.
DEBUG: Rule at line 245 without any user agent to enforce it on.
DEBUG: Rule at line 246 without any user agent to enforce it on.
DEBUG: Rule at line 247 without any user agent to enforce it on.
DEBUG: Rule at line 248 without any user agent to enforce it on.
DEBUG: Rule at line 249 without any user agent to enforce it on.
DEBUG: Rule at line 250 without any user agent to enforce it on.
DEBUG: Rule at line 251 without any user agent to enforce it on.
DEBUG: Rule at line 285 without any user agent to enforce it on.
DEBUG: Rule at line 291 without any user agent to enforce it on.
DEBUG: Rule at line 297 without any user agent to enforce it on.
DEBUG: Rule at line 303 without any user agent to enforce it on.
DEBUG: Rule at line 309 without any user agent to enforce it on.
DEBUG: Rule at line 315 without any user agent to enforce it on.
DEBUG: Rule at line 321 without any user agent to enforce it on.
DEBUG: Rule at line 327 without any user agent to enforce it on.
DEBUG: Rule at line 333 without any user agent to enforce it on.
DEBUG: Rule at line 339 without any user agent to enforce it on.
DEBUG: Rule at line 353 without any user agent to enforce it on.
DEBUG: Rule at line 359 without any user agent to enforce it on.
DEBUG: Rule at line 365 without any user agent to enforce it on.
DEBUG: Rule at line 371 without any user agent to enforce it on.
DEBUG: Rule at line 377 without any user agent to enforce it on.
DEBUG: Rule at line 383 without any user agent to enforce it on.
DEBUG: Rule at line 389 without any user agent to enforce it on.
DEBUG: Rule at line 395 without any user agent to enforce it on.
DEBUG: Rule at line 401 without any user agent to enforce it on.
DEBUG: Rule at line 407 without any user agent to enforce it on.
DEBUG: Rule at line 413 without any user agent to enforce it on.
DEBUG: Rule at line 419 without any user agent to enforce it on.
DEBUG: Rule at line 440 without any user agent to enforce it on.
DEBUG: Rule at line 441 without any user agent to enforce it on.
DEBUG: Rule at line 442 without any user agent to enforce it on.
DEBUG: Rule at line 443 without any user agent to enforce it on.
DEBUG: Rule at line 449 without any user agent to enforce it on.
DEBUG: Rule at line 453 without any user agent to enforce it on.
DEBUG: Rule at line 454 without any user agent to enforce it on.
DEBUG: Rule at line 472 without any user agent to enforce it on.
DEBUG: Rule at line 481 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET http://smithgill.com/> (referer: None)
DEBUG: driver not found in PATH, trying Selenium Manager
DEBUG: Executing: C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\common\windows\selenium-manager.exe --browser chrome
DEBUG: Using driver at: C:\Users\moffi\.cache\selenium\chromedriver\win32\108.0.5359.71\chromedriver.exe
DEBUG: Started executable: `chromedriver` in a child process with pid: 18596
DEBUG: POST http://localhost:60259/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "pageLoadStrategy": "normal", "goog:chromeOptions": {"extensions": [], "args": []}}}}
DEBUG: Starting new HTTP connection (1): localhost:60259
DEBUG: http://localhost:60259 "POST /session HTTP/1.1" 200 791
DEBUG: Remote response: status=200 | data={"value":{"capabilities":{"acceptInsecureCerts":false,"browserName":"chrome","browserVersion":"108.0.5359.98","chrome":{"chromedriverVersion":"108.0.5359.71 (1e0e3868ee06e91ad636a874420e3ca3ae3756ac-refs/branch-heads/5359@{#1016})","userDataDir":"C:\\Users\\moffi\\AppData\\Local\\Temp\\scoped_dir18596_1343328411"},"goog:chromeOptions":{"debuggerAddress":"localhost:60262"},"networkConnectionEnabled":false,"pageLoadStrategy":"normal","platformName":"windows","proxy":{},"setWindowRect":true,"strictFileInteractability":false,"timeouts":{"implicit":0,"pageLoad":300000,"script":30000},"unhandledPromptBehavior":"dismiss and notify","webauthn:extension:credBlob":true,"webauthn:extension:largeBlob":true,"webauthn:virtualAuthenticators":true},"sessionId":"64c27804eadde3d7f15b09c30d2c4576"}} | headers=HTTPHeaderDict({'Content-Length': '791', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:60259/session/64c27804eadde3d7f15b09c30d2c4576/url {"url": "http://smithgill.com/"}
DEBUG: http://localhost:60259 "POST /session/64c27804eadde3d7f15b09c30d2c4576/url HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:60259/session/64c27804eadde3d7f15b09c30d2c4576/timeouts {"implicit": 10000}
DEBUG: http://localhost:60259 "POST /session/64c27804eadde3d7f15b09c30d2c4576/timeouts HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
ERROR: Spider error processing <GET http://smithgill.com/> (referer: None)
Traceback (most recent call last):
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\utils\defer.py", line 240, in iter_errback
    yield next(it)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\utils\python.py", line 338, in __next__
    return next(self.data)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\utils\python.py", line 338, in __next__
    return next(self.data)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\spidermw.py", line 79, in process_sync
    for r in iterable:
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\spidermw.py", line 79, in process_sync
    for r in iterable:
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 336, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\spidermw.py", line 79, in process_sync
    for r in iterable:
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\spidermw.py", line 79, in process_sync
    for r in iterable:
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 32, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\spidermw.py", line 79, in process_sync
    for r in iterable:
  File "C:\Users\moffi\PycharmProjects\scraping_web\scraper\scraper\spiders\website_spider.py", line 79, in parse_website
    meta_contact = {'facebook': facebook_link, 'instagram': instagram_link, 'twitter': twitter_link, 'linkedin': linkedin_link,'emails': emails}
UnboundLocalError: local variable 'instagram_link' referenced before assignment
INFO: Closing spider (finished)
INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 501,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 12533,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 10.407969,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 8, 19, 51, 3, 742978),
 'httpcompression/response_bytes': 59820,
 'httpcompression/response_count': 2,
 'log_count/DEBUG': 76,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/UnboundLocalError': 1,
 'start_time': datetime.datetime(2022, 12, 8, 19, 50, 53, 335009)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: 470da2703463bbd4
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
DEBUG: Attempting to acquire lock 2951221714304 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2951221714304 acquired on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Attempting to release lock 2951221714304 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2951221714304 released on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Crawled (200) <GET http://smithgill.com/robots.txt> (referer: None)
DEBUG: Rule at line 20 without any user agent to enforce it on.
DEBUG: Rule at line 36 without any user agent to enforce it on.
DEBUG: Rule at line 228 without any user agent to enforce it on.
DEBUG: Rule at line 229 without any user agent to enforce it on.
DEBUG: Rule at line 230 without any user agent to enforce it on.
DEBUG: Rule at line 238 without any user agent to enforce it on.
DEBUG: Rule at line 239 without any user agent to enforce it on.
DEBUG: Rule at line 240 without any user agent to enforce it on.
DEBUG: Rule at line 241 without any user agent to enforce it on.
DEBUG: Rule at line 242 without any user agent to enforce it on.
DEBUG: Rule at line 243 without any user agent to enforce it on.
DEBUG: Rule at line 244 without any user agent to enforce it on.
DEBUG: Rule at line 245 without any user agent to enforce it on.
DEBUG: Rule at line 246 without any user agent to enforce it on.
DEBUG: Rule at line 247 without any user agent to enforce it on.
DEBUG: Rule at line 248 without any user agent to enforce it on.
DEBUG: Rule at line 249 without any user agent to enforce it on.
DEBUG: Rule at line 250 without any user agent to enforce it on.
DEBUG: Rule at line 251 without any user agent to enforce it on.
DEBUG: Rule at line 285 without any user agent to enforce it on.
DEBUG: Rule at line 291 without any user agent to enforce it on.
DEBUG: Rule at line 297 without any user agent to enforce it on.
DEBUG: Rule at line 303 without any user agent to enforce it on.
DEBUG: Rule at line 309 without any user agent to enforce it on.
DEBUG: Rule at line 315 without any user agent to enforce it on.
DEBUG: Rule at line 321 without any user agent to enforce it on.
DEBUG: Rule at line 327 without any user agent to enforce it on.
DEBUG: Rule at line 333 without any user agent to enforce it on.
DEBUG: Rule at line 339 without any user agent to enforce it on.
DEBUG: Rule at line 353 without any user agent to enforce it on.
DEBUG: Rule at line 359 without any user agent to enforce it on.
DEBUG: Rule at line 365 without any user agent to enforce it on.
DEBUG: Rule at line 371 without any user agent to enforce it on.
DEBUG: Rule at line 377 without any user agent to enforce it on.
DEBUG: Rule at line 383 without any user agent to enforce it on.
DEBUG: Rule at line 389 without any user agent to enforce it on.
DEBUG: Rule at line 395 without any user agent to enforce it on.
DEBUG: Rule at line 401 without any user agent to enforce it on.
DEBUG: Rule at line 407 without any user agent to enforce it on.
DEBUG: Rule at line 413 without any user agent to enforce it on.
DEBUG: Rule at line 419 without any user agent to enforce it on.
DEBUG: Rule at line 440 without any user agent to enforce it on.
DEBUG: Rule at line 441 without any user agent to enforce it on.
DEBUG: Rule at line 442 without any user agent to enforce it on.
DEBUG: Rule at line 443 without any user agent to enforce it on.
DEBUG: Rule at line 449 without any user agent to enforce it on.
DEBUG: Rule at line 453 without any user agent to enforce it on.
DEBUG: Rule at line 454 without any user agent to enforce it on.
DEBUG: Rule at line 472 without any user agent to enforce it on.
DEBUG: Rule at line 481 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET http://smithgill.com/> (referer: None)
DEBUG: driver not found in PATH, trying Selenium Manager
DEBUG: Executing: C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\common\windows\selenium-manager.exe --browser chrome
DEBUG: Using driver at: C:\Users\moffi\.cache\selenium\chromedriver\win32\108.0.5359.71\chromedriver.exe
DEBUG: Started executable: `chromedriver` in a child process with pid: 5700
DEBUG: POST http://localhost:60321/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "pageLoadStrategy": "normal", "goog:chromeOptions": {"extensions": [], "args": []}}}}
DEBUG: Starting new HTTP connection (1): localhost:60321
DEBUG: http://localhost:60321 "POST /session HTTP/1.1" 200 789
DEBUG: Remote response: status=200 | data={"value":{"capabilities":{"acceptInsecureCerts":false,"browserName":"chrome","browserVersion":"108.0.5359.98","chrome":{"chromedriverVersion":"108.0.5359.71 (1e0e3868ee06e91ad636a874420e3ca3ae3756ac-refs/branch-heads/5359@{#1016})","userDataDir":"C:\\Users\\moffi\\AppData\\Local\\Temp\\scoped_dir5700_747423759"},"goog:chromeOptions":{"debuggerAddress":"localhost:60324"},"networkConnectionEnabled":false,"pageLoadStrategy":"normal","platformName":"windows","proxy":{},"setWindowRect":true,"strictFileInteractability":false,"timeouts":{"implicit":0,"pageLoad":300000,"script":30000},"unhandledPromptBehavior":"dismiss and notify","webauthn:extension:credBlob":true,"webauthn:extension:largeBlob":true,"webauthn:virtualAuthenticators":true},"sessionId":"46f1b43febea027e9019398212a9526c"}} | headers=HTTPHeaderDict({'Content-Length': '789', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:60321/session/46f1b43febea027e9019398212a9526c/url {"url": "http://smithgill.com/"}
DEBUG: http://localhost:60321 "POST /session/46f1b43febea027e9019398212a9526c/url HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:60321/session/46f1b43febea027e9019398212a9526c/timeouts {"implicit": 10000}
DEBUG: http://localhost:60321 "POST /session/46f1b43febea027e9019398212a9526c/timeouts HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
ERROR: Spider error processing <GET http://smithgill.com/> (referer: None)
Traceback (most recent call last):
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\utils\defer.py", line 240, in iter_errback
    yield next(it)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\utils\python.py", line 338, in __next__
    return next(self.data)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\utils\python.py", line 338, in __next__
    return next(self.data)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\spidermw.py", line 79, in process_sync
    for r in iterable:
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\spidermw.py", line 79, in process_sync
    for r in iterable:
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 336, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\spidermw.py", line 79, in process_sync
    for r in iterable:
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\spidermw.py", line 79, in process_sync
    for r in iterable:
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 32, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\spidermw.py", line 79, in process_sync
    for r in iterable:
  File "C:\Users\moffi\PycharmProjects\scraping_web\scraper\scraper\spiders\website_spider.py", line 79, in parse_website
    meta_contact = {'facebook': facebook_link, 'instagram': instagram_link, 'twitter': twitter_link, 'linkedin': linkedin_link}
UnboundLocalError: local variable 'instagram_link' referenced before assignment
INFO: Closing spider (finished)
INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 501,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 12533,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 11.168823,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 8, 19, 51, 52, 710287),
 'httpcompression/response_bytes': 59820,
 'httpcompression/response_count': 2,
 'log_count/DEBUG': 76,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/UnboundLocalError': 1,
 'start_time': datetime.datetime(2022, 12, 8, 19, 51, 41, 541464)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: 4169149c0fc5ab55
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
DEBUG: Attempting to acquire lock 2090484080144 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2090484080144 acquired on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Attempting to release lock 2090484080144 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2090484080144 released on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Crawled (200) <GET http://smithgill.com/robots.txt> (referer: None)
DEBUG: Rule at line 20 without any user agent to enforce it on.
DEBUG: Rule at line 36 without any user agent to enforce it on.
DEBUG: Rule at line 228 without any user agent to enforce it on.
DEBUG: Rule at line 229 without any user agent to enforce it on.
DEBUG: Rule at line 230 without any user agent to enforce it on.
DEBUG: Rule at line 238 without any user agent to enforce it on.
DEBUG: Rule at line 239 without any user agent to enforce it on.
DEBUG: Rule at line 240 without any user agent to enforce it on.
DEBUG: Rule at line 241 without any user agent to enforce it on.
DEBUG: Rule at line 242 without any user agent to enforce it on.
DEBUG: Rule at line 243 without any user agent to enforce it on.
DEBUG: Rule at line 244 without any user agent to enforce it on.
DEBUG: Rule at line 245 without any user agent to enforce it on.
DEBUG: Rule at line 246 without any user agent to enforce it on.
DEBUG: Rule at line 247 without any user agent to enforce it on.
DEBUG: Rule at line 248 without any user agent to enforce it on.
DEBUG: Rule at line 249 without any user agent to enforce it on.
DEBUG: Rule at line 250 without any user agent to enforce it on.
DEBUG: Rule at line 251 without any user agent to enforce it on.
DEBUG: Rule at line 285 without any user agent to enforce it on.
DEBUG: Rule at line 291 without any user agent to enforce it on.
DEBUG: Rule at line 297 without any user agent to enforce it on.
DEBUG: Rule at line 303 without any user agent to enforce it on.
DEBUG: Rule at line 309 without any user agent to enforce it on.
DEBUG: Rule at line 315 without any user agent to enforce it on.
DEBUG: Rule at line 321 without any user agent to enforce it on.
DEBUG: Rule at line 327 without any user agent to enforce it on.
DEBUG: Rule at line 333 without any user agent to enforce it on.
DEBUG: Rule at line 339 without any user agent to enforce it on.
DEBUG: Rule at line 353 without any user agent to enforce it on.
DEBUG: Rule at line 359 without any user agent to enforce it on.
DEBUG: Rule at line 365 without any user agent to enforce it on.
DEBUG: Rule at line 371 without any user agent to enforce it on.
DEBUG: Rule at line 377 without any user agent to enforce it on.
DEBUG: Rule at line 383 without any user agent to enforce it on.
DEBUG: Rule at line 389 without any user agent to enforce it on.
DEBUG: Rule at line 395 without any user agent to enforce it on.
DEBUG: Rule at line 401 without any user agent to enforce it on.
DEBUG: Rule at line 407 without any user agent to enforce it on.
DEBUG: Rule at line 413 without any user agent to enforce it on.
DEBUG: Rule at line 419 without any user agent to enforce it on.
DEBUG: Rule at line 440 without any user agent to enforce it on.
DEBUG: Rule at line 441 without any user agent to enforce it on.
DEBUG: Rule at line 442 without any user agent to enforce it on.
DEBUG: Rule at line 443 without any user agent to enforce it on.
DEBUG: Rule at line 449 without any user agent to enforce it on.
DEBUG: Rule at line 453 without any user agent to enforce it on.
DEBUG: Rule at line 454 without any user agent to enforce it on.
DEBUG: Rule at line 472 without any user agent to enforce it on.
DEBUG: Rule at line 481 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET http://smithgill.com/> (referer: None)
DEBUG: driver not found in PATH, trying Selenium Manager
DEBUG: Executing: C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\common\windows\selenium-manager.exe --browser chrome
DEBUG: Using driver at: C:\Users\moffi\.cache\selenium\chromedriver\win32\108.0.5359.71\chromedriver.exe
DEBUG: Started executable: `chromedriver` in a child process with pid: 36436
DEBUG: POST http://localhost:60387/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "pageLoadStrategy": "normal", "goog:chromeOptions": {"extensions": [], "args": []}}}}
DEBUG: Starting new HTTP connection (1): localhost:60387
DEBUG: http://localhost:60387 "POST /session HTTP/1.1" 200 790
DEBUG: Remote response: status=200 | data={"value":{"capabilities":{"acceptInsecureCerts":false,"browserName":"chrome","browserVersion":"108.0.5359.98","chrome":{"chromedriverVersion":"108.0.5359.71 (1e0e3868ee06e91ad636a874420e3ca3ae3756ac-refs/branch-heads/5359@{#1016})","userDataDir":"C:\\Users\\moffi\\AppData\\Local\\Temp\\scoped_dir36436_456798415"},"goog:chromeOptions":{"debuggerAddress":"localhost:60390"},"networkConnectionEnabled":false,"pageLoadStrategy":"normal","platformName":"windows","proxy":{},"setWindowRect":true,"strictFileInteractability":false,"timeouts":{"implicit":0,"pageLoad":300000,"script":30000},"unhandledPromptBehavior":"dismiss and notify","webauthn:extension:credBlob":true,"webauthn:extension:largeBlob":true,"webauthn:virtualAuthenticators":true},"sessionId":"18ac2ec394b0b92c370712d6ab6d70c0"}} | headers=HTTPHeaderDict({'Content-Length': '790', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:60387/session/18ac2ec394b0b92c370712d6ab6d70c0/url {"url": "http://smithgill.com/"}
DEBUG: http://localhost:60387 "POST /session/18ac2ec394b0b92c370712d6ab6d70c0/url HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:60387/session/18ac2ec394b0b92c370712d6ab6d70c0/timeouts {"implicit": 10000}
DEBUG: http://localhost:60387 "POST /session/18ac2ec394b0b92c370712d6ab6d70c0/timeouts HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
ERROR: Spider error processing <GET http://smithgill.com/> (referer: None)
Traceback (most recent call last):
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\utils\defer.py", line 240, in iter_errback
    yield next(it)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\utils\python.py", line 338, in __next__
    return next(self.data)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\utils\python.py", line 338, in __next__
    return next(self.data)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\spidermw.py", line 79, in process_sync
    for r in iterable:
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\spidermw.py", line 79, in process_sync
    for r in iterable:
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 336, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\spidermw.py", line 79, in process_sync
    for r in iterable:
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\spidermw.py", line 79, in process_sync
    for r in iterable:
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 32, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\spidermw.py", line 79, in process_sync
    for r in iterable:
  File "C:\Users\moffi\PycharmProjects\scraping_web\scraper\scraper\spiders\website_spider.py", line 79, in parse_website
    meta_contact = {'facebook': facebook_link, 'instagram': instagram_link, 'twitter': twitter_link, 'linkedin': linkedin_link}
UnboundLocalError: local variable 'instagram_link' referenced before assignment
INFO: Closing spider (finished)
INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 501,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 12533,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 10.795219,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 8, 19, 54, 5, 82162),
 'httpcompression/response_bytes': 59820,
 'httpcompression/response_count': 2,
 'log_count/DEBUG': 76,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/UnboundLocalError': 1,
 'start_time': datetime.datetime(2022, 12, 8, 19, 53, 54, 286943)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: 605c37237f448985
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
DEBUG: Attempting to acquire lock 2206733542752 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2206733542752 acquired on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Attempting to release lock 2206733542752 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2206733542752 released on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Crawled (200) <GET http://smithgill.com/robots.txt> (referer: None)
DEBUG: Rule at line 20 without any user agent to enforce it on.
DEBUG: Rule at line 36 without any user agent to enforce it on.
DEBUG: Rule at line 228 without any user agent to enforce it on.
DEBUG: Rule at line 229 without any user agent to enforce it on.
DEBUG: Rule at line 230 without any user agent to enforce it on.
DEBUG: Rule at line 238 without any user agent to enforce it on.
DEBUG: Rule at line 239 without any user agent to enforce it on.
DEBUG: Rule at line 240 without any user agent to enforce it on.
DEBUG: Rule at line 241 without any user agent to enforce it on.
DEBUG: Rule at line 242 without any user agent to enforce it on.
DEBUG: Rule at line 243 without any user agent to enforce it on.
DEBUG: Rule at line 244 without any user agent to enforce it on.
DEBUG: Rule at line 245 without any user agent to enforce it on.
DEBUG: Rule at line 246 without any user agent to enforce it on.
DEBUG: Rule at line 247 without any user agent to enforce it on.
DEBUG: Rule at line 248 without any user agent to enforce it on.
DEBUG: Rule at line 249 without any user agent to enforce it on.
DEBUG: Rule at line 250 without any user agent to enforce it on.
DEBUG: Rule at line 251 without any user agent to enforce it on.
DEBUG: Rule at line 285 without any user agent to enforce it on.
DEBUG: Rule at line 291 without any user agent to enforce it on.
DEBUG: Rule at line 297 without any user agent to enforce it on.
DEBUG: Rule at line 303 without any user agent to enforce it on.
DEBUG: Rule at line 309 without any user agent to enforce it on.
DEBUG: Rule at line 315 without any user agent to enforce it on.
DEBUG: Rule at line 321 without any user agent to enforce it on.
DEBUG: Rule at line 327 without any user agent to enforce it on.
DEBUG: Rule at line 333 without any user agent to enforce it on.
DEBUG: Rule at line 339 without any user agent to enforce it on.
DEBUG: Rule at line 353 without any user agent to enforce it on.
DEBUG: Rule at line 359 without any user agent to enforce it on.
DEBUG: Rule at line 365 without any user agent to enforce it on.
DEBUG: Rule at line 371 without any user agent to enforce it on.
DEBUG: Rule at line 377 without any user agent to enforce it on.
DEBUG: Rule at line 383 without any user agent to enforce it on.
DEBUG: Rule at line 389 without any user agent to enforce it on.
DEBUG: Rule at line 395 without any user agent to enforce it on.
DEBUG: Rule at line 401 without any user agent to enforce it on.
DEBUG: Rule at line 407 without any user agent to enforce it on.
DEBUG: Rule at line 413 without any user agent to enforce it on.
DEBUG: Rule at line 419 without any user agent to enforce it on.
DEBUG: Rule at line 440 without any user agent to enforce it on.
DEBUG: Rule at line 441 without any user agent to enforce it on.
DEBUG: Rule at line 442 without any user agent to enforce it on.
DEBUG: Rule at line 443 without any user agent to enforce it on.
DEBUG: Rule at line 449 without any user agent to enforce it on.
DEBUG: Rule at line 453 without any user agent to enforce it on.
DEBUG: Rule at line 454 without any user agent to enforce it on.
DEBUG: Rule at line 472 without any user agent to enforce it on.
DEBUG: Rule at line 481 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET http://smithgill.com/> (referer: None)
DEBUG: driver not found in PATH, trying Selenium Manager
DEBUG: Executing: C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\common\windows\selenium-manager.exe --browser chrome
DEBUG: Using driver at: C:\Users\moffi\.cache\selenium\chromedriver\win32\108.0.5359.71\chromedriver.exe
DEBUG: Started executable: `chromedriver` in a child process with pid: 33420
DEBUG: POST http://localhost:60449/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "pageLoadStrategy": "normal", "goog:chromeOptions": {"extensions": [], "args": []}}}}
DEBUG: Starting new HTTP connection (1): localhost:60449
DEBUG: http://localhost:60449 "POST /session HTTP/1.1" 200 791
DEBUG: Remote response: status=200 | data={"value":{"capabilities":{"acceptInsecureCerts":false,"browserName":"chrome","browserVersion":"108.0.5359.98","chrome":{"chromedriverVersion":"108.0.5359.71 (1e0e3868ee06e91ad636a874420e3ca3ae3756ac-refs/branch-heads/5359@{#1016})","userDataDir":"C:\\Users\\moffi\\AppData\\Local\\Temp\\scoped_dir33420_1556172617"},"goog:chromeOptions":{"debuggerAddress":"localhost:60452"},"networkConnectionEnabled":false,"pageLoadStrategy":"normal","platformName":"windows","proxy":{},"setWindowRect":true,"strictFileInteractability":false,"timeouts":{"implicit":0,"pageLoad":300000,"script":30000},"unhandledPromptBehavior":"dismiss and notify","webauthn:extension:credBlob":true,"webauthn:extension:largeBlob":true,"webauthn:virtualAuthenticators":true},"sessionId":"1fe041a32f9fde7b3e387f2a40f956cd"}} | headers=HTTPHeaderDict({'Content-Length': '791', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:60449/session/1fe041a32f9fde7b3e387f2a40f956cd/url {"url": "http://smithgill.com/"}
DEBUG: http://localhost:60449 "POST /session/1fe041a32f9fde7b3e387f2a40f956cd/url HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:60449/session/1fe041a32f9fde7b3e387f2a40f956cd/timeouts {"implicit": 10000}
DEBUG: http://localhost:60449 "POST /session/1fe041a32f9fde7b3e387f2a40f956cd/timeouts HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: Filtered duplicate request: <GET http://smithgill.com/contact> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG: DELETE http://localhost:60449/session/1fe041a32f9fde7b3e387f2a40f956cd {}
DEBUG: http://localhost:60449 "DELETE /session/1fe041a32f9fde7b3e387f2a40f956cd HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: Redirecting (301) to <GET http://smithgill.com/contact/> from <GET http://smithgill.com/contact>
DEBUG: Crawled (200) <GET http://smithgill.com/contact/> (referer: http://smithgill.com/)
ERROR: Spider must return request, item, or None, got 'list' in <GET http://smithgill.com/contact/>
INFO: Closing spider (finished)
INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1252,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 18931,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/301': 1,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 17.589261,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 8, 19, 55, 32, 383342),
 'httpcompression/response_bytes': 85365,
 'httpcompression/response_count': 3,
 'log_count/DEBUG': 83,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'request_depth_max': 1,
 'response_received_count': 3,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 12, 8, 19, 55, 14, 794081)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: 78b1db63d0a90f33
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
DEBUG: Attempting to acquire lock 2975018754128 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2975018754128 acquired on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Attempting to release lock 2975018754128 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2975018754128 released on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Crawled (200) <GET http://smithgill.com/robots.txt> (referer: None)
DEBUG: Rule at line 20 without any user agent to enforce it on.
DEBUG: Rule at line 36 without any user agent to enforce it on.
DEBUG: Rule at line 228 without any user agent to enforce it on.
DEBUG: Rule at line 229 without any user agent to enforce it on.
DEBUG: Rule at line 230 without any user agent to enforce it on.
DEBUG: Rule at line 238 without any user agent to enforce it on.
DEBUG: Rule at line 239 without any user agent to enforce it on.
DEBUG: Rule at line 240 without any user agent to enforce it on.
DEBUG: Rule at line 241 without any user agent to enforce it on.
DEBUG: Rule at line 242 without any user agent to enforce it on.
DEBUG: Rule at line 243 without any user agent to enforce it on.
DEBUG: Rule at line 244 without any user agent to enforce it on.
DEBUG: Rule at line 245 without any user agent to enforce it on.
DEBUG: Rule at line 246 without any user agent to enforce it on.
DEBUG: Rule at line 247 without any user agent to enforce it on.
DEBUG: Rule at line 248 without any user agent to enforce it on.
DEBUG: Rule at line 249 without any user agent to enforce it on.
DEBUG: Rule at line 250 without any user agent to enforce it on.
DEBUG: Rule at line 251 without any user agent to enforce it on.
DEBUG: Rule at line 285 without any user agent to enforce it on.
DEBUG: Rule at line 291 without any user agent to enforce it on.
DEBUG: Rule at line 297 without any user agent to enforce it on.
DEBUG: Rule at line 303 without any user agent to enforce it on.
DEBUG: Rule at line 309 without any user agent to enforce it on.
DEBUG: Rule at line 315 without any user agent to enforce it on.
DEBUG: Rule at line 321 without any user agent to enforce it on.
DEBUG: Rule at line 327 without any user agent to enforce it on.
DEBUG: Rule at line 333 without any user agent to enforce it on.
DEBUG: Rule at line 339 without any user agent to enforce it on.
DEBUG: Rule at line 353 without any user agent to enforce it on.
DEBUG: Rule at line 359 without any user agent to enforce it on.
DEBUG: Rule at line 365 without any user agent to enforce it on.
DEBUG: Rule at line 371 without any user agent to enforce it on.
DEBUG: Rule at line 377 without any user agent to enforce it on.
DEBUG: Rule at line 383 without any user agent to enforce it on.
DEBUG: Rule at line 389 without any user agent to enforce it on.
DEBUG: Rule at line 395 without any user agent to enforce it on.
DEBUG: Rule at line 401 without any user agent to enforce it on.
DEBUG: Rule at line 407 without any user agent to enforce it on.
DEBUG: Rule at line 413 without any user agent to enforce it on.
DEBUG: Rule at line 419 without any user agent to enforce it on.
DEBUG: Rule at line 440 without any user agent to enforce it on.
DEBUG: Rule at line 441 without any user agent to enforce it on.
DEBUG: Rule at line 442 without any user agent to enforce it on.
DEBUG: Rule at line 443 without any user agent to enforce it on.
DEBUG: Rule at line 449 without any user agent to enforce it on.
DEBUG: Rule at line 453 without any user agent to enforce it on.
DEBUG: Rule at line 454 without any user agent to enforce it on.
DEBUG: Rule at line 472 without any user agent to enforce it on.
DEBUG: Rule at line 481 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET http://smithgill.com/> (referer: None)
DEBUG: driver not found in PATH, trying Selenium Manager
DEBUG: Executing: C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\common\windows\selenium-manager.exe --browser chrome
DEBUG: Using driver at: C:\Users\moffi\.cache\selenium\chromedriver\win32\108.0.5359.71\chromedriver.exe
DEBUG: Started executable: `chromedriver` in a child process with pid: 13968
DEBUG: POST http://localhost:60523/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "pageLoadStrategy": "normal", "goog:chromeOptions": {"extensions": [], "args": []}}}}
DEBUG: Starting new HTTP connection (1): localhost:60523
DEBUG: http://localhost:60523 "POST /session HTTP/1.1" 200 790
DEBUG: Remote response: status=200 | data={"value":{"capabilities":{"acceptInsecureCerts":false,"browserName":"chrome","browserVersion":"108.0.5359.98","chrome":{"chromedriverVersion":"108.0.5359.71 (1e0e3868ee06e91ad636a874420e3ca3ae3756ac-refs/branch-heads/5359@{#1016})","userDataDir":"C:\\Users\\moffi\\AppData\\Local\\Temp\\scoped_dir13968_592609336"},"goog:chromeOptions":{"debuggerAddress":"localhost:60526"},"networkConnectionEnabled":false,"pageLoadStrategy":"normal","platformName":"windows","proxy":{},"setWindowRect":true,"strictFileInteractability":false,"timeouts":{"implicit":0,"pageLoad":300000,"script":30000},"unhandledPromptBehavior":"dismiss and notify","webauthn:extension:credBlob":true,"webauthn:extension:largeBlob":true,"webauthn:virtualAuthenticators":true},"sessionId":"b28fb3b3e60a51b349ee81a6bf0d9d48"}} | headers=HTTPHeaderDict({'Content-Length': '790', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:60523/session/b28fb3b3e60a51b349ee81a6bf0d9d48/url {"url": "http://smithgill.com/"}
DEBUG: http://localhost:60523 "POST /session/b28fb3b3e60a51b349ee81a6bf0d9d48/url HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:60523/session/b28fb3b3e60a51b349ee81a6bf0d9d48/timeouts {"implicit": 10000}
DEBUG: http://localhost:60523 "POST /session/b28fb3b3e60a51b349ee81a6bf0d9d48/timeouts HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: Filtered duplicate request: <GET http://smithgill.com/contact> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG: DELETE http://localhost:60523/session/b28fb3b3e60a51b349ee81a6bf0d9d48 {}
DEBUG: http://localhost:60523 "DELETE /session/b28fb3b3e60a51b349ee81a6bf0d9d48 HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: Redirecting (301) to <GET http://smithgill.com/contact/> from <GET http://smithgill.com/contact>
DEBUG: Crawled (200) <GET http://smithgill.com/contact/> (referer: http://smithgill.com/)
ERROR: Spider error processing <GET http://smithgill.com/contact/> (referer: http://smithgill.com/)
Traceback (most recent call last):
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\utils\defer.py", line 240, in iter_errback
    yield next(it)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\utils\python.py", line 338, in __next__
    return next(self.data)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\utils\python.py", line 338, in __next__
    return next(self.data)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\spidermw.py", line 79, in process_sync
    for r in iterable:
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\spidermw.py", line 79, in process_sync
    for r in iterable:
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 336, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\spidermw.py", line 79, in process_sync
    for r in iterable:
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\spidermw.py", line 79, in process_sync
    for r in iterable:
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 32, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\spidermw.py", line 79, in process_sync
    for r in iterable:
  File "C:\Users\moffi\PycharmProjects\scraping_web\scraper\scraper\spiders\website_spider.py", line 117, in parse_contact
    print("meta test", response.meta['meta'])
KeyError: 'meta'
INFO: Closing spider (finished)
INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1252,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 18931,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/301': 1,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 18.29174,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 8, 19, 58, 12, 153012),
 'httpcompression/response_bytes': 85365,
 'httpcompression/response_count': 3,
 'log_count/DEBUG': 83,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'request_depth_max': 1,
 'response_received_count': 3,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'spider_exceptions/KeyError': 1,
 'start_time': datetime.datetime(2022, 12, 8, 19, 57, 53, 861272)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: 0aa98f3bc481fedf
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
DEBUG: Attempting to acquire lock 1734779183808 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 1734779183808 acquired on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Attempting to release lock 1734779183808 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 1734779183808 released on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Crawled (200) <GET http://smithgill.com/robots.txt> (referer: None)
DEBUG: Rule at line 20 without any user agent to enforce it on.
DEBUG: Rule at line 36 without any user agent to enforce it on.
DEBUG: Rule at line 228 without any user agent to enforce it on.
DEBUG: Rule at line 229 without any user agent to enforce it on.
DEBUG: Rule at line 230 without any user agent to enforce it on.
DEBUG: Rule at line 238 without any user agent to enforce it on.
DEBUG: Rule at line 239 without any user agent to enforce it on.
DEBUG: Rule at line 240 without any user agent to enforce it on.
DEBUG: Rule at line 241 without any user agent to enforce it on.
DEBUG: Rule at line 242 without any user agent to enforce it on.
DEBUG: Rule at line 243 without any user agent to enforce it on.
DEBUG: Rule at line 244 without any user agent to enforce it on.
DEBUG: Rule at line 245 without any user agent to enforce it on.
DEBUG: Rule at line 246 without any user agent to enforce it on.
DEBUG: Rule at line 247 without any user agent to enforce it on.
DEBUG: Rule at line 248 without any user agent to enforce it on.
DEBUG: Rule at line 249 without any user agent to enforce it on.
DEBUG: Rule at line 250 without any user agent to enforce it on.
DEBUG: Rule at line 251 without any user agent to enforce it on.
DEBUG: Rule at line 285 without any user agent to enforce it on.
DEBUG: Rule at line 291 without any user agent to enforce it on.
DEBUG: Rule at line 297 without any user agent to enforce it on.
DEBUG: Rule at line 303 without any user agent to enforce it on.
DEBUG: Rule at line 309 without any user agent to enforce it on.
DEBUG: Rule at line 315 without any user agent to enforce it on.
DEBUG: Rule at line 321 without any user agent to enforce it on.
DEBUG: Rule at line 327 without any user agent to enforce it on.
DEBUG: Rule at line 333 without any user agent to enforce it on.
DEBUG: Rule at line 339 without any user agent to enforce it on.
DEBUG: Rule at line 353 without any user agent to enforce it on.
DEBUG: Rule at line 359 without any user agent to enforce it on.
DEBUG: Rule at line 365 without any user agent to enforce it on.
DEBUG: Rule at line 371 without any user agent to enforce it on.
DEBUG: Rule at line 377 without any user agent to enforce it on.
DEBUG: Rule at line 383 without any user agent to enforce it on.
DEBUG: Rule at line 389 without any user agent to enforce it on.
DEBUG: Rule at line 395 without any user agent to enforce it on.
DEBUG: Rule at line 401 without any user agent to enforce it on.
DEBUG: Rule at line 407 without any user agent to enforce it on.
DEBUG: Rule at line 413 without any user agent to enforce it on.
DEBUG: Rule at line 419 without any user agent to enforce it on.
DEBUG: Rule at line 440 without any user agent to enforce it on.
DEBUG: Rule at line 441 without any user agent to enforce it on.
DEBUG: Rule at line 442 without any user agent to enforce it on.
DEBUG: Rule at line 443 without any user agent to enforce it on.
DEBUG: Rule at line 449 without any user agent to enforce it on.
DEBUG: Rule at line 453 without any user agent to enforce it on.
DEBUG: Rule at line 454 without any user agent to enforce it on.
DEBUG: Rule at line 472 without any user agent to enforce it on.
DEBUG: Rule at line 481 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET http://smithgill.com/> (referer: None)
DEBUG: driver not found in PATH, trying Selenium Manager
DEBUG: Executing: C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\common\windows\selenium-manager.exe --browser chrome
DEBUG: Using driver at: C:\Users\moffi\.cache\selenium\chromedriver\win32\108.0.5359.71\chromedriver.exe
DEBUG: Started executable: `chromedriver` in a child process with pid: 39452
DEBUG: POST http://localhost:60589/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "pageLoadStrategy": "normal", "goog:chromeOptions": {"extensions": [], "args": []}}}}
DEBUG: Starting new HTTP connection (1): localhost:60589
DEBUG: http://localhost:60589 "POST /session HTTP/1.1" 200 790
DEBUG: Remote response: status=200 | data={"value":{"capabilities":{"acceptInsecureCerts":false,"browserName":"chrome","browserVersion":"108.0.5359.98","chrome":{"chromedriverVersion":"108.0.5359.71 (1e0e3868ee06e91ad636a874420e3ca3ae3756ac-refs/branch-heads/5359@{#1016})","userDataDir":"C:\\Users\\moffi\\AppData\\Local\\Temp\\scoped_dir39452_528592525"},"goog:chromeOptions":{"debuggerAddress":"localhost:60592"},"networkConnectionEnabled":false,"pageLoadStrategy":"normal","platformName":"windows","proxy":{},"setWindowRect":true,"strictFileInteractability":false,"timeouts":{"implicit":0,"pageLoad":300000,"script":30000},"unhandledPromptBehavior":"dismiss and notify","webauthn:extension:credBlob":true,"webauthn:extension:largeBlob":true,"webauthn:virtualAuthenticators":true},"sessionId":"8f2f7d2856d1a43e16e785f1f6c356ad"}} | headers=HTTPHeaderDict({'Content-Length': '790', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:60589/session/8f2f7d2856d1a43e16e785f1f6c356ad/url {"url": "http://smithgill.com/"}
DEBUG: http://localhost:60589 "POST /session/8f2f7d2856d1a43e16e785f1f6c356ad/url HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:60589/session/8f2f7d2856d1a43e16e785f1f6c356ad/timeouts {"implicit": 10000}
DEBUG: http://localhost:60589 "POST /session/8f2f7d2856d1a43e16e785f1f6c356ad/timeouts HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: Filtered duplicate request: <GET http://smithgill.com/contact> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG: DELETE http://localhost:60589/session/8f2f7d2856d1a43e16e785f1f6c356ad {}
DEBUG: http://localhost:60589 "DELETE /session/8f2f7d2856d1a43e16e785f1f6c356ad HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: Redirecting (301) to <GET http://smithgill.com/contact/> from <GET http://smithgill.com/contact>
DEBUG: Crawled (200) <GET http://smithgill.com/contact/> (referer: http://smithgill.com/)
ERROR: Spider error processing <GET http://smithgill.com/contact/> (referer: http://smithgill.com/)
Traceback (most recent call last):
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\utils\defer.py", line 240, in iter_errback
    yield next(it)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\utils\python.py", line 338, in __next__
    return next(self.data)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\utils\python.py", line 338, in __next__
    return next(self.data)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\spidermw.py", line 79, in process_sync
    for r in iterable:
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\spidermw.py", line 79, in process_sync
    for r in iterable:
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 336, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\spidermw.py", line 79, in process_sync
    for r in iterable:
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\spidermw.py", line 79, in process_sync
    for r in iterable:
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 32, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\spidermw.py", line 79, in process_sync
    for r in iterable:
  File "C:\Users\moffi\PycharmProjects\scraping_web\scraper\scraper\spiders\website_spider.py", line 117, in parse_contact
    print("meta test", response.meta['meta'])
KeyError: 'meta'
INFO: Closing spider (finished)
INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1252,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 18931,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/301': 1,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 21.320392,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 8, 19, 58, 41, 372768),
 'httpcompression/response_bytes': 85365,
 'httpcompression/response_count': 3,
 'log_count/DEBUG': 83,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'request_depth_max': 1,
 'response_received_count': 3,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'spider_exceptions/KeyError': 1,
 'start_time': datetime.datetime(2022, 12, 8, 19, 58, 20, 52376)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: ee4301448b1a7310
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
DEBUG: Attempting to acquire lock 2048533634960 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2048533634960 acquired on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Attempting to release lock 2048533634960 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2048533634960 released on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Crawled (200) <GET http://smithgill.com/robots.txt> (referer: None)
DEBUG: Rule at line 20 without any user agent to enforce it on.
DEBUG: Rule at line 36 without any user agent to enforce it on.
DEBUG: Rule at line 228 without any user agent to enforce it on.
DEBUG: Rule at line 229 without any user agent to enforce it on.
DEBUG: Rule at line 230 without any user agent to enforce it on.
DEBUG: Rule at line 238 without any user agent to enforce it on.
DEBUG: Rule at line 239 without any user agent to enforce it on.
DEBUG: Rule at line 240 without any user agent to enforce it on.
DEBUG: Rule at line 241 without any user agent to enforce it on.
DEBUG: Rule at line 242 without any user agent to enforce it on.
DEBUG: Rule at line 243 without any user agent to enforce it on.
DEBUG: Rule at line 244 without any user agent to enforce it on.
DEBUG: Rule at line 245 without any user agent to enforce it on.
DEBUG: Rule at line 246 without any user agent to enforce it on.
DEBUG: Rule at line 247 without any user agent to enforce it on.
DEBUG: Rule at line 248 without any user agent to enforce it on.
DEBUG: Rule at line 249 without any user agent to enforce it on.
DEBUG: Rule at line 250 without any user agent to enforce it on.
DEBUG: Rule at line 251 without any user agent to enforce it on.
DEBUG: Rule at line 285 without any user agent to enforce it on.
DEBUG: Rule at line 291 without any user agent to enforce it on.
DEBUG: Rule at line 297 without any user agent to enforce it on.
DEBUG: Rule at line 303 without any user agent to enforce it on.
DEBUG: Rule at line 309 without any user agent to enforce it on.
DEBUG: Rule at line 315 without any user agent to enforce it on.
DEBUG: Rule at line 321 without any user agent to enforce it on.
DEBUG: Rule at line 327 without any user agent to enforce it on.
DEBUG: Rule at line 333 without any user agent to enforce it on.
DEBUG: Rule at line 339 without any user agent to enforce it on.
DEBUG: Rule at line 353 without any user agent to enforce it on.
DEBUG: Rule at line 359 without any user agent to enforce it on.
DEBUG: Rule at line 365 without any user agent to enforce it on.
DEBUG: Rule at line 371 without any user agent to enforce it on.
DEBUG: Rule at line 377 without any user agent to enforce it on.
DEBUG: Rule at line 383 without any user agent to enforce it on.
DEBUG: Rule at line 389 without any user agent to enforce it on.
DEBUG: Rule at line 395 without any user agent to enforce it on.
DEBUG: Rule at line 401 without any user agent to enforce it on.
DEBUG: Rule at line 407 without any user agent to enforce it on.
DEBUG: Rule at line 413 without any user agent to enforce it on.
DEBUG: Rule at line 419 without any user agent to enforce it on.
DEBUG: Rule at line 440 without any user agent to enforce it on.
DEBUG: Rule at line 441 without any user agent to enforce it on.
DEBUG: Rule at line 442 without any user agent to enforce it on.
DEBUG: Rule at line 443 without any user agent to enforce it on.
DEBUG: Rule at line 449 without any user agent to enforce it on.
DEBUG: Rule at line 453 without any user agent to enforce it on.
DEBUG: Rule at line 454 without any user agent to enforce it on.
DEBUG: Rule at line 472 without any user agent to enforce it on.
DEBUG: Rule at line 481 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET http://smithgill.com/> (referer: None)
DEBUG: driver not found in PATH, trying Selenium Manager
DEBUG: Executing: C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\common\windows\selenium-manager.exe --browser chrome
DEBUG: Using driver at: C:\Users\moffi\.cache\selenium\chromedriver\win32\108.0.5359.71\chromedriver.exe
DEBUG: Started executable: `chromedriver` in a child process with pid: 30816
DEBUG: POST http://localhost:60655/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "pageLoadStrategy": "normal", "goog:chromeOptions": {"extensions": [], "args": []}}}}
DEBUG: Starting new HTTP connection (1): localhost:60655
DEBUG: http://localhost:60655 "POST /session HTTP/1.1" 200 791
DEBUG: Remote response: status=200 | data={"value":{"capabilities":{"acceptInsecureCerts":false,"browserName":"chrome","browserVersion":"108.0.5359.98","chrome":{"chromedriverVersion":"108.0.5359.71 (1e0e3868ee06e91ad636a874420e3ca3ae3756ac-refs/branch-heads/5359@{#1016})","userDataDir":"C:\\Users\\moffi\\AppData\\Local\\Temp\\scoped_dir30816_1729229009"},"goog:chromeOptions":{"debuggerAddress":"localhost:60658"},"networkConnectionEnabled":false,"pageLoadStrategy":"normal","platformName":"windows","proxy":{},"setWindowRect":true,"strictFileInteractability":false,"timeouts":{"implicit":0,"pageLoad":300000,"script":30000},"unhandledPromptBehavior":"dismiss and notify","webauthn:extension:credBlob":true,"webauthn:extension:largeBlob":true,"webauthn:virtualAuthenticators":true},"sessionId":"b87ce40caeb4fab7a8188e7df25a4ea0"}} | headers=HTTPHeaderDict({'Content-Length': '791', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:60655/session/b87ce40caeb4fab7a8188e7df25a4ea0/url {"url": "http://smithgill.com/"}
DEBUG: http://localhost:60655 "POST /session/b87ce40caeb4fab7a8188e7df25a4ea0/url HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:60655/session/b87ce40caeb4fab7a8188e7df25a4ea0/timeouts {"implicit": 10000}
DEBUG: http://localhost:60655 "POST /session/b87ce40caeb4fab7a8188e7df25a4ea0/timeouts HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: Filtered duplicate request: <GET http://smithgill.com/contact> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG: DELETE http://localhost:60655/session/b87ce40caeb4fab7a8188e7df25a4ea0 {}
DEBUG: http://localhost:60655 "DELETE /session/b87ce40caeb4fab7a8188e7df25a4ea0 HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: Redirecting (301) to <GET http://smithgill.com/contact/> from <GET http://smithgill.com/contact>
DEBUG: Crawled (200) <GET http://smithgill.com/contact/> (referer: http://smithgill.com/)
INFO: Closing spider (finished)
INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1252,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 18931,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/301': 1,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 19.654422,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 8, 19, 59, 19, 591426),
 'httpcompression/response_bytes': 85365,
 'httpcompression/response_count': 3,
 'log_count/DEBUG': 83,
 'log_count/INFO': 10,
 'request_depth_max': 1,
 'response_received_count': 3,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 12, 8, 19, 58, 59, 937004)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: 8fa33a6686e9fb96
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
DEBUG: Attempting to acquire lock 2192890830304 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2192890830304 acquired on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Attempting to release lock 2192890830304 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2192890830304 released on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Crawled (200) <GET http://smithgill.com/robots.txt> (referer: None)
DEBUG: Rule at line 20 without any user agent to enforce it on.
DEBUG: Rule at line 36 without any user agent to enforce it on.
DEBUG: Rule at line 228 without any user agent to enforce it on.
DEBUG: Rule at line 229 without any user agent to enforce it on.
DEBUG: Rule at line 230 without any user agent to enforce it on.
DEBUG: Rule at line 238 without any user agent to enforce it on.
DEBUG: Rule at line 239 without any user agent to enforce it on.
DEBUG: Rule at line 240 without any user agent to enforce it on.
DEBUG: Rule at line 241 without any user agent to enforce it on.
DEBUG: Rule at line 242 without any user agent to enforce it on.
DEBUG: Rule at line 243 without any user agent to enforce it on.
DEBUG: Rule at line 244 without any user agent to enforce it on.
DEBUG: Rule at line 245 without any user agent to enforce it on.
DEBUG: Rule at line 246 without any user agent to enforce it on.
DEBUG: Rule at line 247 without any user agent to enforce it on.
DEBUG: Rule at line 248 without any user agent to enforce it on.
DEBUG: Rule at line 249 without any user agent to enforce it on.
DEBUG: Rule at line 250 without any user agent to enforce it on.
DEBUG: Rule at line 251 without any user agent to enforce it on.
DEBUG: Rule at line 285 without any user agent to enforce it on.
DEBUG: Rule at line 291 without any user agent to enforce it on.
DEBUG: Rule at line 297 without any user agent to enforce it on.
DEBUG: Rule at line 303 without any user agent to enforce it on.
DEBUG: Rule at line 309 without any user agent to enforce it on.
DEBUG: Rule at line 315 without any user agent to enforce it on.
DEBUG: Rule at line 321 without any user agent to enforce it on.
DEBUG: Rule at line 327 without any user agent to enforce it on.
DEBUG: Rule at line 333 without any user agent to enforce it on.
DEBUG: Rule at line 339 without any user agent to enforce it on.
DEBUG: Rule at line 353 without any user agent to enforce it on.
DEBUG: Rule at line 359 without any user agent to enforce it on.
DEBUG: Rule at line 365 without any user agent to enforce it on.
DEBUG: Rule at line 371 without any user agent to enforce it on.
DEBUG: Rule at line 377 without any user agent to enforce it on.
DEBUG: Rule at line 383 without any user agent to enforce it on.
DEBUG: Rule at line 389 without any user agent to enforce it on.
DEBUG: Rule at line 395 without any user agent to enforce it on.
DEBUG: Rule at line 401 without any user agent to enforce it on.
DEBUG: Rule at line 407 without any user agent to enforce it on.
DEBUG: Rule at line 413 without any user agent to enforce it on.
DEBUG: Rule at line 419 without any user agent to enforce it on.
DEBUG: Rule at line 440 without any user agent to enforce it on.
DEBUG: Rule at line 441 without any user agent to enforce it on.
DEBUG: Rule at line 442 without any user agent to enforce it on.
DEBUG: Rule at line 443 without any user agent to enforce it on.
DEBUG: Rule at line 449 without any user agent to enforce it on.
DEBUG: Rule at line 453 without any user agent to enforce it on.
DEBUG: Rule at line 454 without any user agent to enforce it on.
DEBUG: Rule at line 472 without any user agent to enforce it on.
DEBUG: Rule at line 481 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET http://smithgill.com/> (referer: None)
DEBUG: driver not found in PATH, trying Selenium Manager
DEBUG: Executing: C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\common\windows\selenium-manager.exe --browser chrome
DEBUG: Using driver at: C:\Users\moffi\.cache\selenium\chromedriver\win32\108.0.5359.71\chromedriver.exe
DEBUG: Started executable: `chromedriver` in a child process with pid: 16228
DEBUG: POST http://localhost:60721/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "pageLoadStrategy": "normal", "goog:chromeOptions": {"extensions": [], "args": []}}}}
DEBUG: Starting new HTTP connection (1): localhost:60721
DEBUG: http://localhost:60721 "POST /session HTTP/1.1" 200 790
DEBUG: Remote response: status=200 | data={"value":{"capabilities":{"acceptInsecureCerts":false,"browserName":"chrome","browserVersion":"108.0.5359.98","chrome":{"chromedriverVersion":"108.0.5359.71 (1e0e3868ee06e91ad636a874420e3ca3ae3756ac-refs/branch-heads/5359@{#1016})","userDataDir":"C:\\Users\\moffi\\AppData\\Local\\Temp\\scoped_dir16228_651677254"},"goog:chromeOptions":{"debuggerAddress":"localhost:60724"},"networkConnectionEnabled":false,"pageLoadStrategy":"normal","platformName":"windows","proxy":{},"setWindowRect":true,"strictFileInteractability":false,"timeouts":{"implicit":0,"pageLoad":300000,"script":30000},"unhandledPromptBehavior":"dismiss and notify","webauthn:extension:credBlob":true,"webauthn:extension:largeBlob":true,"webauthn:virtualAuthenticators":true},"sessionId":"f6814cce0c5b1223fd3adc3b313bfbb2"}} | headers=HTTPHeaderDict({'Content-Length': '790', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:60721/session/f6814cce0c5b1223fd3adc3b313bfbb2/url {"url": "http://smithgill.com/"}
DEBUG: http://localhost:60721 "POST /session/f6814cce0c5b1223fd3adc3b313bfbb2/url HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:60721/session/f6814cce0c5b1223fd3adc3b313bfbb2/timeouts {"implicit": 10000}
DEBUG: http://localhost:60721 "POST /session/f6814cce0c5b1223fd3adc3b313bfbb2/timeouts HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: Filtered duplicate request: <GET http://smithgill.com/contact> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG: DELETE http://localhost:60721/session/f6814cce0c5b1223fd3adc3b313bfbb2 {}
DEBUG: http://localhost:60721 "DELETE /session/f6814cce0c5b1223fd3adc3b313bfbb2 HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: Redirecting (301) to <GET http://smithgill.com/contact/> from <GET http://smithgill.com/contact>
DEBUG: Crawled (200) <GET http://smithgill.com/contact/> (referer: http://smithgill.com/)
DEBUG: Scraped from <200 http://smithgill.com/contact/>
{'emails': ['resumes@smithgill.com',
            'weiweiluo@smithgill.com',
            'info@smithgill.com',
            'press@smithgill.com'],
 'facebook': 'https://www.facebook.com/smithgillarch',
 'instagram': '',
 'linkedin': '',
 'twitter': 'https://www.twitter.com/smithgillarch'}
INFO: Closing spider (finished)
INFO: Stored json feed (1 items) in: website_spider_output.json
INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1252,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 18931,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/301': 1,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 19.020343,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 8, 20, 0, 1, 946477),
 'httpcompression/response_bytes': 85365,
 'httpcompression/response_count': 3,
 'item_scraped_count': 1,
 'log_count/DEBUG': 84,
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 3,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 12, 8, 19, 59, 42, 926134)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: e3fa88ea69b7af65
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
DEBUG: Attempting to acquire lock 3033818702144 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 3033818702144 acquired on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Attempting to release lock 3033818702144 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 3033818702144 released on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Crawled (200) <GET http://smithgill.com/robots.txt> (referer: None)
DEBUG: Rule at line 20 without any user agent to enforce it on.
DEBUG: Rule at line 36 without any user agent to enforce it on.
DEBUG: Rule at line 228 without any user agent to enforce it on.
DEBUG: Rule at line 229 without any user agent to enforce it on.
DEBUG: Rule at line 230 without any user agent to enforce it on.
DEBUG: Rule at line 238 without any user agent to enforce it on.
DEBUG: Rule at line 239 without any user agent to enforce it on.
DEBUG: Rule at line 240 without any user agent to enforce it on.
DEBUG: Rule at line 241 without any user agent to enforce it on.
DEBUG: Rule at line 242 without any user agent to enforce it on.
DEBUG: Rule at line 243 without any user agent to enforce it on.
DEBUG: Rule at line 244 without any user agent to enforce it on.
DEBUG: Rule at line 245 without any user agent to enforce it on.
DEBUG: Rule at line 246 without any user agent to enforce it on.
DEBUG: Rule at line 247 without any user agent to enforce it on.
DEBUG: Rule at line 248 without any user agent to enforce it on.
DEBUG: Rule at line 249 without any user agent to enforce it on.
DEBUG: Rule at line 250 without any user agent to enforce it on.
DEBUG: Rule at line 251 without any user agent to enforce it on.
DEBUG: Rule at line 285 without any user agent to enforce it on.
DEBUG: Rule at line 291 without any user agent to enforce it on.
DEBUG: Rule at line 297 without any user agent to enforce it on.
DEBUG: Rule at line 303 without any user agent to enforce it on.
DEBUG: Rule at line 309 without any user agent to enforce it on.
DEBUG: Rule at line 315 without any user agent to enforce it on.
DEBUG: Rule at line 321 without any user agent to enforce it on.
DEBUG: Rule at line 327 without any user agent to enforce it on.
DEBUG: Rule at line 333 without any user agent to enforce it on.
DEBUG: Rule at line 339 without any user agent to enforce it on.
DEBUG: Rule at line 353 without any user agent to enforce it on.
DEBUG: Rule at line 359 without any user agent to enforce it on.
DEBUG: Rule at line 365 without any user agent to enforce it on.
DEBUG: Rule at line 371 without any user agent to enforce it on.
DEBUG: Rule at line 377 without any user agent to enforce it on.
DEBUG: Rule at line 383 without any user agent to enforce it on.
DEBUG: Rule at line 389 without any user agent to enforce it on.
DEBUG: Rule at line 395 without any user agent to enforce it on.
DEBUG: Rule at line 401 without any user agent to enforce it on.
DEBUG: Rule at line 407 without any user agent to enforce it on.
DEBUG: Rule at line 413 without any user agent to enforce it on.
DEBUG: Rule at line 419 without any user agent to enforce it on.
DEBUG: Rule at line 440 without any user agent to enforce it on.
DEBUG: Rule at line 441 without any user agent to enforce it on.
DEBUG: Rule at line 442 without any user agent to enforce it on.
DEBUG: Rule at line 443 without any user agent to enforce it on.
DEBUG: Rule at line 449 without any user agent to enforce it on.
DEBUG: Rule at line 453 without any user agent to enforce it on.
DEBUG: Rule at line 454 without any user agent to enforce it on.
DEBUG: Rule at line 472 without any user agent to enforce it on.
DEBUG: Rule at line 481 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET http://smithgill.com/> (referer: None)
DEBUG: driver not found in PATH, trying Selenium Manager
DEBUG: Executing: C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\common\windows\selenium-manager.exe --browser chrome
DEBUG: Using driver at: C:\Users\moffi\.cache\selenium\chromedriver\win32\108.0.5359.71\chromedriver.exe
DEBUG: Started executable: `chromedriver` in a child process with pid: 39560
DEBUG: POST http://localhost:60789/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "pageLoadStrategy": "normal", "goog:chromeOptions": {"extensions": [], "args": []}}}}
DEBUG: Starting new HTTP connection (1): localhost:60789
DEBUG: http://localhost:60789 "POST /session HTTP/1.1" 200 791
DEBUG: Remote response: status=200 | data={"value":{"capabilities":{"acceptInsecureCerts":false,"browserName":"chrome","browserVersion":"108.0.5359.98","chrome":{"chromedriverVersion":"108.0.5359.71 (1e0e3868ee06e91ad636a874420e3ca3ae3756ac-refs/branch-heads/5359@{#1016})","userDataDir":"C:\\Users\\moffi\\AppData\\Local\\Temp\\scoped_dir39560_1953390729"},"goog:chromeOptions":{"debuggerAddress":"localhost:60792"},"networkConnectionEnabled":false,"pageLoadStrategy":"normal","platformName":"windows","proxy":{},"setWindowRect":true,"strictFileInteractability":false,"timeouts":{"implicit":0,"pageLoad":300000,"script":30000},"unhandledPromptBehavior":"dismiss and notify","webauthn:extension:credBlob":true,"webauthn:extension:largeBlob":true,"webauthn:virtualAuthenticators":true},"sessionId":"0884e43fee273ef7c087ed5ba511c321"}} | headers=HTTPHeaderDict({'Content-Length': '791', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:60789/session/0884e43fee273ef7c087ed5ba511c321/url {"url": "http://smithgill.com/"}
DEBUG: http://localhost:60789 "POST /session/0884e43fee273ef7c087ed5ba511c321/url HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:60789/session/0884e43fee273ef7c087ed5ba511c321/timeouts {"implicit": 10000}
DEBUG: http://localhost:60789 "POST /session/0884e43fee273ef7c087ed5ba511c321/timeouts HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: Filtered duplicate request: <GET http://smithgill.com/contact> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG: DELETE http://localhost:60789/session/0884e43fee273ef7c087ed5ba511c321 {}
DEBUG: http://localhost:60789 "DELETE /session/0884e43fee273ef7c087ed5ba511c321 HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: Redirecting (301) to <GET http://smithgill.com/contact/> from <GET http://smithgill.com/contact>
DEBUG: Crawled (200) <GET http://smithgill.com/contact/> (referer: http://smithgill.com/)
DEBUG: Scraped from <200 http://smithgill.com/contact/>
{'emails': ['info@smithgill.com',
            'resumes@smithgill.com',
            'weiweiluo@smithgill.com',
            'press@smithgill.com'],
 'facebook': 'https://www.facebook.com/smithgillarch',
 'instagram': '',
 'linkedin': '',
 'twitter': 'https://www.twitter.com/smithgillarch'}
INFO: Closing spider (finished)
INFO: Stored json feed (1 items) in: website_spider_output.json
INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1252,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 18931,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/301': 1,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 17.509849,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 8, 20, 0, 49, 572385),
 'httpcompression/response_bytes': 85365,
 'httpcompression/response_count': 3,
 'item_scraped_count': 1,
 'log_count/DEBUG': 84,
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 3,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 12, 8, 20, 0, 32, 62536)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: 3056dc336498360e
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\engine.py", line 152, in _next_request
    request = next(self.slot.start_requests)
  File "C:\Users\moffi\PycharmProjects\scraping_web\scraper\scraper\spiders\website_spider.py", line 28, in start_requests
    with open("data.csv") as csvfile:
FileNotFoundError: [Errno 2] No such file or directory: 'data.csv'
INFO: Closing spider (finished)
INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 0.004001,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 8, 20, 23, 7, 208396),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'start_time': datetime.datetime(2022, 12, 8, 20, 23, 7, 204395)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: b901e17bf20db432
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\engine.py", line 152, in _next_request
    request = next(self.slot.start_requests)
  File "C:\Users\moffi\PycharmProjects\scraping_web\scraper\scraper\spiders\website_spider.py", line 37, in start_requests
    yield scrapy.Request(url=url, callback=self.parse_website())
TypeError: WebsiteSpider.parse_website() missing 1 required positional argument: 'response'
INFO: Closing spider (finished)
INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 0.004001,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 8, 20, 23, 46, 916757),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'start_time': datetime.datetime(2022, 12, 8, 20, 23, 46, 912756)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: 44eb6e611193f36b
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\engine.py", line 152, in _next_request
    request = next(self.slot.start_requests)
  File "C:\Users\moffi\PycharmProjects\scraping_web\scraper\scraper\spiders\website_spider.py", line 37, in start_requests
    yield scrapy.Request(url=url, callback=self.parse_website())
TypeError: WebsiteSpider.parse_website() missing 1 required positional argument: 'response'
INFO: Closing spider (finished)
INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 0.004001,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 8, 20, 23, 52, 355286),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'start_time': datetime.datetime(2022, 12, 8, 20, 23, 52, 351285)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: 5ce4de5a332d6eb1
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\engine.py", line 152, in _next_request
    request = next(self.slot.start_requests)
  File "C:\Users\moffi\PycharmProjects\scraping_web\scraper\scraper\spiders\website_spider.py", line 37, in start_requests
    yield scrapy.Request(url=url, callback=self.parse_website)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\http\request\__init__.py", line 60, in __init__
    self._set_url(url)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\http\request\__init__.py", line 108, in _set_url
    raise ValueError(f'Missing scheme in request url: {self._url}')
ValueError: Missing scheme in request url: address
INFO: Closing spider (finished)
INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 0.015004,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 8, 20, 24, 1, 587172),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'start_time': datetime.datetime(2022, 12, 8, 20, 24, 1, 572168)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: 06bc2e8d0110886c
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\engine.py", line 152, in _next_request
    request = next(self.slot.start_requests)
  File "C:\Users\moffi\PycharmProjects\scraping_web\scraper\scraper\spiders\website_spider.py", line 38, in start_requests
    yield scrapy.Request(url=url, callback=self.parse_website)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\http\request\__init__.py", line 60, in __init__
    self._set_url(url)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\http\request\__init__.py", line 108, in _set_url
    raise ValueError(f'Missing scheme in request url: {self._url}')
ValueError: Missing scheme in request url: address
INFO: Closing spider (finished)
INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 0.005003,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 8, 20, 24, 27, 988975),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'start_time': datetime.datetime(2022, 12, 8, 20, 24, 27, 983972)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: c1c35e26042cf710
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\engine.py", line 152, in _next_request
    request = next(self.slot.start_requests)
  File "C:\Users\moffi\PycharmProjects\scraping_web\scraper\scraper\spiders\website_spider.py", line 38, in start_requests
    yield scrapy.Request(url=url, callback=self.parse_website)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\http\request\__init__.py", line 60, in __init__
    self._set_url(url)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\http\request\__init__.py", line 108, in _set_url
    raise ValueError(f'Missing scheme in request url: {self._url}')
ValueError: Missing scheme in request url: address
INFO: Closing spider (finished)
INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 0.004,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 8, 20, 25, 4, 207008),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'start_time': datetime.datetime(2022, 12, 8, 20, 25, 4, 203008)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: 93a76448dbbbd902
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\engine.py", line 152, in _next_request
    request = next(self.slot.start_requests)
  File "C:\Users\moffi\PycharmProjects\scraping_web\scraper\scraper\spiders\website_spider.py", line 38, in start_requests
    yield scrapy.Request(url=url, callback=self.parse_website)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\http\request\__init__.py", line 60, in __init__
    self._set_url(url)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\http\request\__init__.py", line 108, in _set_url
    raise ValueError(f'Missing scheme in request url: {self._url}')
ValueError: Missing scheme in request url: adres:%206336%20OH-605%20S,%20Westerville,%20OH%2043082,%20Stany%20Zjednoczone
INFO: Closing spider (finished)
INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 0.005,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 8, 20, 26, 23, 848242),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'start_time': datetime.datetime(2022, 12, 8, 20, 26, 23, 843242)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: a067d5f9820db64b
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\engine.py", line 152, in _next_request
    request = next(self.slot.start_requests)
  File "C:\Users\moffi\PycharmProjects\scraping_web\scraper\scraper\spiders\website_spider.py", line 38, in start_requests
    yield scrapy.Request(url=url, callback=self.parse_website)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\http\request\__init__.py", line 60, in __init__
    self._set_url(url)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\http\request\__init__.py", line 108, in _set_url
    raise ValueError(f'Missing scheme in request url: {self._url}')
ValueError: Missing scheme in request url: adres:%206336%20OH-605%20S,%20Westerville,%20OH%2043082,%20Stany%20Zjednoczone
INFO: Closing spider (finished)
INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 0.005001,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 8, 20, 27, 12, 817514),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'start_time': datetime.datetime(2022, 12, 8, 20, 27, 12, 812513)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: 21cc6c1bbc7a8b92
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
DEBUG: Crawled (403) <GET https://trapperstreeservice.com/robots.txt> (referer: None)
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Rule at line 14 without any user agent to enforce it on.
DEBUG: Rule at line 16 without any user agent to enforce it on.
DEBUG: Rule at line 35 without any user agent to enforce it on.
DEBUG: Rule at line 42 without any user agent to enforce it on.
DEBUG: Rule at line 43 without any user agent to enforce it on.
DEBUG: Rule at line 44 without any user agent to enforce it on.
DEBUG: Rule at line 45 without any user agent to enforce it on.
DEBUG: Rule at line 46 without any user agent to enforce it on.
DEBUG: Rule at line 47 without any user agent to enforce it on.
DEBUG: Rule at line 69 without any user agent to enforce it on.
DEBUG: Redirecting (301) to <GET https://www.fandftrees.com/robots.txt> from <GET http://www.fandftrees.com/robots.txt>
DEBUG: Crawled (403) <GET https://trapperstreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.treetechohio.com/robots.txt> (referer: None)
DEBUG: Attempting to acquire lock 2107676850880 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2107676850880 acquired on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Attempting to release lock 2107676850880 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2107676850880 released on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Crawled (200) <GET http://www.columbustreeservicesllc.com/robots.txt> (referer: None)
INFO: Ignoring response <403 https://trapperstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (403) <GET https://www.midohiotree.org/robots.txt> (referer: None)
DEBUG: Filtered duplicate request: <GET http://hardwicktreecare.com/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG: Crawled (200) <GET http://www.columbustreeservicesllc.com/> (referer: None)
DEBUG: driver not found in PATH, trying Selenium Manager
DEBUG: Executing: C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\common\windows\selenium-manager.exe --browser chrome
DEBUG: Using driver at: C:\Users\moffi\.cache\selenium\chromedriver\win32\108.0.5359.71\chromedriver.exe
DEBUG: Started executable: `chromedriver` in a child process with pid: 22680
DEBUG: POST http://localhost:65267/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "pageLoadStrategy": "normal", "goog:chromeOptions": {"extensions": [], "args": []}}}}
DEBUG: Starting new HTTP connection (1): localhost:65267
DEBUG: http://localhost:65267 "POST /session HTTP/1.1" 200 790
DEBUG: Remote response: status=200 | data={"value":{"capabilities":{"acceptInsecureCerts":false,"browserName":"chrome","browserVersion":"108.0.5359.98","chrome":{"chromedriverVersion":"108.0.5359.71 (1e0e3868ee06e91ad636a874420e3ca3ae3756ac-refs/branch-heads/5359@{#1016})","userDataDir":"C:\\Users\\moffi\\AppData\\Local\\Temp\\scoped_dir22680_661478169"},"goog:chromeOptions":{"debuggerAddress":"localhost:65270"},"networkConnectionEnabled":false,"pageLoadStrategy":"normal","platformName":"windows","proxy":{},"setWindowRect":true,"strictFileInteractability":false,"timeouts":{"implicit":0,"pageLoad":300000,"script":30000},"unhandledPromptBehavior":"dismiss and notify","webauthn:extension:credBlob":true,"webauthn:extension:largeBlob":true,"webauthn:virtualAuthenticators":true},"sessionId":"10a1d99cef3f71a34f18369a7748bff2"}} | headers=HTTPHeaderDict({'Content-Length': '790', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:65267/session/10a1d99cef3f71a34f18369a7748bff2/url {"url": "http://smithgill.com/"}
DEBUG: http://localhost:65267 "POST /session/10a1d99cef3f71a34f18369a7748bff2/url HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:65267/session/10a1d99cef3f71a34f18369a7748bff2/timeouts {"implicit": 10000}
DEBUG: http://localhost:65267 "POST /session/10a1d99cef3f71a34f18369a7748bff2/timeouts HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: DELETE http://localhost:65267/session/10a1d99cef3f71a34f18369a7748bff2 {}
DEBUG: http://localhost:65267 "DELETE /session/10a1d99cef3f71a34f18369a7748bff2 HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: Crawled (403) <GET https://www.midohiotree.org/> (referer: None)
DEBUG: Crawled (200) <GET http://hardwicktreecare.com/robots.txt> (referer: None)
DEBUG: Crawled (404) <GET http://ohiotreeandexcavating.net/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.fandftrees.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://starwoodtree.com/robots.txt> from <GET http://starwoodtree.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.treetechohio.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.fandftrees.com/> from <GET http://www.fandftrees.com/>
DEBUG: Redirecting (301) to <GET https://www.hardwicktreecare.com/> from <GET http://hardwicktreecare.com/>
INFO: Ignoring response <403 https://www.midohiotree.org/>: HTTP status code is not handled or not allowed
DEBUG: driver not found in PATH, trying Selenium Manager
DEBUG: Executing: C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\common\windows\selenium-manager.exe --browser chrome
DEBUG: Using driver at: C:\Users\moffi\.cache\selenium\chromedriver\win32\108.0.5359.71\chromedriver.exe
DEBUG: Started executable: `chromedriver` in a child process with pid: 39048
DEBUG: POST http://localhost:65328/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "pageLoadStrategy": "normal", "goog:chromeOptions": {"extensions": [], "args": []}}}}
DEBUG: Starting new HTTP connection (1): localhost:65328
DEBUG: http://localhost:65328 "POST /session HTTP/1.1" 200 790
DEBUG: Remote response: status=200 | data={"value":{"capabilities":{"acceptInsecureCerts":false,"browserName":"chrome","browserVersion":"108.0.5359.98","chrome":{"chromedriverVersion":"108.0.5359.71 (1e0e3868ee06e91ad636a874420e3ca3ae3756ac-refs/branch-heads/5359@{#1016})","userDataDir":"C:\\Users\\moffi\\AppData\\Local\\Temp\\scoped_dir39048_105985459"},"goog:chromeOptions":{"debuggerAddress":"localhost:65331"},"networkConnectionEnabled":false,"pageLoadStrategy":"normal","platformName":"windows","proxy":{},"setWindowRect":true,"strictFileInteractability":false,"timeouts":{"implicit":0,"pageLoad":300000,"script":30000},"unhandledPromptBehavior":"dismiss and notify","webauthn:extension:credBlob":true,"webauthn:extension:largeBlob":true,"webauthn:virtualAuthenticators":true},"sessionId":"4cd165bb2334557510d367a7080b7d3d"}} | headers=HTTPHeaderDict({'Content-Length': '790', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:65328/session/4cd165bb2334557510d367a7080b7d3d/url {"url": "http://smithgill.com/"}
DEBUG: http://localhost:65328 "POST /session/4cd165bb2334557510d367a7080b7d3d/url HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:65328/session/4cd165bb2334557510d367a7080b7d3d/timeouts {"implicit": 10000}
DEBUG: http://localhost:65328 "POST /session/4cd165bb2334557510d367a7080b7d3d/timeouts HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: Crawled (200) <GET https://www.russelltreeexperts.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://ohiotreeandexcavating.com/> from <GET http://ohiotreeandexcavating.net/>
DEBUG: DELETE http://localhost:65328/session/4cd165bb2334557510d367a7080b7d3d {}
DEBUG: http://localhost:65328 "DELETE /session/4cd165bb2334557510d367a7080b7d3d HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: Crawled (200) <GET http://herculestree.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://ohiotreeandexcavating.com/robots.txt> from <GET http://ohiotreeandexcavating.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.hardwicktreecare.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://www.treesaremybusiness.com/robots.txt> (referer: None)
DEBUG: Crawled (404) <GET http://ohiotreecare.com/robots.txt> (referer: None)
DEBUG: Rule at line 1 without any user agent to enforce it on.
DEBUG: Rule at line 2 without any user agent to enforce it on.
DEBUG: Rule at line 8 without any user agent to enforce it on.
DEBUG: Rule at line 9 without any user agent to enforce it on.
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Rule at line 11 without any user agent to enforce it on.
DEBUG: Rule at line 12 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://www.russelltreeexperts.com/> (referer: None)
DEBUG: driver not found in PATH, trying Selenium Manager
DEBUG: Executing: C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\common\windows\selenium-manager.exe --browser chrome
DEBUG: Using driver at: C:\Users\moffi\.cache\selenium\chromedriver\win32\108.0.5359.71\chromedriver.exe
DEBUG: Started executable: `chromedriver` in a child process with pid: 4228
DEBUG: POST http://localhost:65366/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "pageLoadStrategy": "normal", "goog:chromeOptions": {"extensions": [], "args": []}}}}
DEBUG: Starting new HTTP connection (1): localhost:65366
DEBUG: http://localhost:65366 "POST /session HTTP/1.1" 200 790
DEBUG: Remote response: status=200 | data={"value":{"capabilities":{"acceptInsecureCerts":false,"browserName":"chrome","browserVersion":"108.0.5359.98","chrome":{"chromedriverVersion":"108.0.5359.71 (1e0e3868ee06e91ad636a874420e3ca3ae3756ac-refs/branch-heads/5359@{#1016})","userDataDir":"C:\\Users\\moffi\\AppData\\Local\\Temp\\scoped_dir4228_1410459277"},"goog:chromeOptions":{"debuggerAddress":"localhost:65369"},"networkConnectionEnabled":false,"pageLoadStrategy":"normal","platformName":"windows","proxy":{},"setWindowRect":true,"strictFileInteractability":false,"timeouts":{"implicit":0,"pageLoad":300000,"script":30000},"unhandledPromptBehavior":"dismiss and notify","webauthn:extension:credBlob":true,"webauthn:extension:largeBlob":true,"webauthn:virtualAuthenticators":true},"sessionId":"ff332d6ae864940a17154f0d198cf441"}} | headers=HTTPHeaderDict({'Content-Length': '790', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:65366/session/ff332d6ae864940a17154f0d198cf441/url {"url": "http://smithgill.com/"}
DEBUG: http://localhost:65366 "POST /session/ff332d6ae864940a17154f0d198cf441/url HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:65366/session/ff332d6ae864940a17154f0d198cf441/timeouts {"implicit": 10000}
DEBUG: http://localhost:65366 "POST /session/ff332d6ae864940a17154f0d198cf441/timeouts HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: DELETE http://localhost:65366/session/ff332d6ae864940a17154f0d198cf441 {}
DEBUG: http://localhost:65366 "DELETE /session/ff332d6ae864940a17154f0d198cf441 HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: Redirecting (301) to <GET https://herculestree.com/> from <GET http://herculestree.com/>
DEBUG: Redirecting (301) to <GET https://www.treesaremybusiness.com/> from <GET http://www.treesaremybusiness.com/>
DEBUG: Crawled (403) <GET https://www.tackettstreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/akron/?utm_source=GMB&utm_medium=organic&utm_campaign=doylestown> (referer: None)
DEBUG: Crawled (200) <GET https://www.fandftrees.com/> (referer: None)
DEBUG: Crawled (200) <GET http://ohiotreecare.com/> (referer: None)
DEBUG: Crawled (200) <GET https://ohiotreeandexcavating.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.treetechohio.com/contact> (referer: https://www.treetechohio.com/)
DEBUG: Crawled (200) <GET https://www.hardwicktreecare.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://ohiotreeandexcavating.com/> from <GET http://ohiotreeandexcavating.com/>
DEBUG: Crawled (403) <GET http://www.hackettstreeservice.com/robots.txt> (referer: None)
DEBUG: driver not found in PATH, trying Selenium Manager
DEBUG: Executing: C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\common\windows\selenium-manager.exe --browser chrome
DEBUG: Using driver at: C:\Users\moffi\.cache\selenium\chromedriver\win32\108.0.5359.71\chromedriver.exe
DEBUG: Started executable: `chromedriver` in a child process with pid: 6068
DEBUG: POST http://localhost:65394/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "pageLoadStrategy": "normal", "goog:chromeOptions": {"extensions": [], "args": []}}}}
DEBUG: Starting new HTTP connection (1): localhost:65394
DEBUG: http://localhost:65394 "POST /session HTTP/1.1" 200 790
DEBUG: Remote response: status=200 | data={"value":{"capabilities":{"acceptInsecureCerts":false,"browserName":"chrome","browserVersion":"108.0.5359.98","chrome":{"chromedriverVersion":"108.0.5359.71 (1e0e3868ee06e91ad636a874420e3ca3ae3756ac-refs/branch-heads/5359@{#1016})","userDataDir":"C:\\Users\\moffi\\AppData\\Local\\Temp\\scoped_dir6068_1461231678"},"goog:chromeOptions":{"debuggerAddress":"localhost:65397"},"networkConnectionEnabled":false,"pageLoadStrategy":"normal","platformName":"windows","proxy":{},"setWindowRect":true,"strictFileInteractability":false,"timeouts":{"implicit":0,"pageLoad":300000,"script":30000},"unhandledPromptBehavior":"dismiss and notify","webauthn:extension:credBlob":true,"webauthn:extension:largeBlob":true,"webauthn:virtualAuthenticators":true},"sessionId":"e69791393866c89d4b40204f35d020a4"}} | headers=HTTPHeaderDict({'Content-Length': '790', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:65394/session/e69791393866c89d4b40204f35d020a4/url {"url": "http://smithgill.com/"}
ERROR: Spider error processing <GET https://www.monstertreeservice.com/akron/?utm_source=GMB&utm_medium=organic&utm_campaign=doylestown> (referer: None)
Traceback (most recent call last):
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\urllib3\connectionpool.py", line 449, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\urllib3\connectionpool.py", line 444, in _make_request
    httplib_response = conn.getresponse()
  File "C:\Users\moffi\AppData\Local\Programs\Python\Python310\lib\http\client.py", line 1368, in getresponse
    response.begin()
  File "C:\Users\moffi\AppData\Local\Programs\Python\Python310\lib\http\client.py", line 317, in begin
    version, status, reason = self._read_status()
  File "C:\Users\moffi\AppData\Local\Programs\Python\Python310\lib\http\client.py", line 278, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "C:\Users\moffi\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\utils\defer.py", line 240, in iter_errback
    yield next(it)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\utils\python.py", line 338, in __next__
    return next(self.data)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\utils\python.py", line 338, in __next__
    return next(self.data)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\spidermw.py", line 79, in process_sync
    for r in iterable:
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\spidermw.py", line 79, in process_sync
    for r in iterable:
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 336, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\spidermw.py", line 79, in process_sync
    for r in iterable:
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\spidermw.py", line 79, in process_sync
    for r in iterable:
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 32, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\spidermw.py", line 79, in process_sync
    for r in iterable:
  File "C:\Users\moffi\PycharmProjects\scraping_web\scraper\scraper\spiders\website_spider.py", line 52, in parse_website
    driver.get("http://smithgill.com/")
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 455, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 442, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\remote\remote_connection.py", line 294, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\remote\remote_connection.py", line 316, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\urllib3\util\retry.py", line 550, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\urllib3\packages\six.py", line 769, in reraise
    raise value.with_traceback(tb)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\urllib3\connectionpool.py", line 449, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\urllib3\connectionpool.py", line 444, in _make_request
    httplib_response = conn.getresponse()
  File "C:\Users\moffi\AppData\Local\Programs\Python\Python310\lib\http\client.py", line 1368, in getresponse
    response.begin()
  File "C:\Users\moffi\AppData\Local\Programs\Python\Python310\lib\http\client.py", line 317, in begin
    version, status, reason = self._read_status()
  File "C:\Users\moffi\AppData\Local\Programs\Python\Python310\lib\http\client.py", line 278, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "C:\Users\moffi\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
urllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))
DEBUG: driver not found in PATH, trying Selenium Manager
DEBUG: Executing: C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\common\windows\selenium-manager.exe --browser chrome
DEBUG: Using driver at: C:\Users\moffi\.cache\selenium\chromedriver\win32\108.0.5359.71\chromedriver.exe
DEBUG: Started executable: `chromedriver` in a child process with pid: 6656
DEBUG: POST http://localhost:65407/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "pageLoadStrategy": "normal", "goog:chromeOptions": {"extensions": [], "args": []}}}}
DEBUG: Starting new HTTP connection (1): localhost:65407
DEBUG: http://localhost:65407 "POST /session HTTP/1.1" 200 789
DEBUG: Remote response: status=200 | data={"value":{"capabilities":{"acceptInsecureCerts":false,"browserName":"chrome","browserVersion":"108.0.5359.98","chrome":{"chromedriverVersion":"108.0.5359.71 (1e0e3868ee06e91ad636a874420e3ca3ae3756ac-refs/branch-heads/5359@{#1016})","userDataDir":"C:\\Users\\moffi\\AppData\\Local\\Temp\\scoped_dir6656_976337431"},"goog:chromeOptions":{"debuggerAddress":"localhost:65410"},"networkConnectionEnabled":false,"pageLoadStrategy":"normal","platformName":"windows","proxy":{},"setWindowRect":true,"strictFileInteractability":false,"timeouts":{"implicit":0,"pageLoad":300000,"script":30000},"unhandledPromptBehavior":"dismiss and notify","webauthn:extension:credBlob":true,"webauthn:extension:largeBlob":true,"webauthn:virtualAuthenticators":true},"sessionId":"8ea4366a8214d6c4e96db61aa217f070"}} | headers=HTTPHeaderDict({'Content-Length': '789', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:65407/session/8ea4366a8214d6c4e96db61aa217f070/url {"url": "http://smithgill.com/"}
DEBUG: http://localhost:65407 "POST /session/8ea4366a8214d6c4e96db61aa217f070/url HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:65407/session/8ea4366a8214d6c4e96db61aa217f070/timeouts {"implicit": 10000}
DEBUG: http://localhost:65407 "POST /session/8ea4366a8214d6c4e96db61aa217f070/timeouts HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: DELETE http://localhost:65407/session/8ea4366a8214d6c4e96db61aa217f070 {}
DEBUG: http://localhost:65407 "DELETE /session/8ea4366a8214d6c4e96db61aa217f070 HTTP/1.1" 200 14
DEBUG: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: driver not found in PATH, trying Selenium Manager
DEBUG: Executing: C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\common\windows\selenium-manager.exe --browser chrome
DEBUG: Using driver at: C:\Users\moffi\.cache\selenium\chromedriver\win32\108.0.5359.71\chromedriver.exe
DEBUG: Started executable: `chromedriver` in a child process with pid: 10520
DEBUG: POST http://localhost:65437/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "pageLoadStrategy": "normal", "goog:chromeOptions": {"extensions": [], "args": []}}}}
DEBUG: Starting new HTTP connection (1): localhost:65437
DEBUG: http://localhost:65437 "POST /session HTTP/1.1" 200 790
DEBUG: Remote response: status=200 | data={"value":{"capabilities":{"acceptInsecureCerts":false,"browserName":"chrome","browserVersion":"108.0.5359.98","chrome":{"chromedriverVersion":"108.0.5359.71 (1e0e3868ee06e91ad636a874420e3ca3ae3756ac-refs/branch-heads/5359@{#1016})","userDataDir":"C:\\Users\\moffi\\AppData\\Local\\Temp\\scoped_dir10520_673439773"},"goog:chromeOptions":{"debuggerAddress":"localhost:65440"},"networkConnectionEnabled":false,"pageLoadStrategy":"normal","platformName":"windows","proxy":{},"setWindowRect":true,"strictFileInteractability":false,"timeouts":{"implicit":0,"pageLoad":300000,"script":30000},"unhandledPromptBehavior":"dismiss and notify","webauthn:extension:credBlob":true,"webauthn:extension:largeBlob":true,"webauthn:virtualAuthenticators":true},"sessionId":"50cbad26a13fd6b61f7f7ff5be5b98ea"}} | headers=HTTPHeaderDict({'Content-Length': '790', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
DEBUG: Finished Request
DEBUG: POST http://localhost:65437/session/50cbad26a13fd6b61f7f7ff5be5b98ea/url {"url": "http://smithgill.com/"}
INFO: Received SIGINT, shutting down gracefully. Send again to force 
ERROR: Spider error processing <GET http://ohiotreecare.com/> (referer: None)
Traceback (most recent call last):
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\urllib3\connectionpool.py", line 449, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\urllib3\connectionpool.py", line 444, in _make_request
    httplib_response = conn.getresponse()
  File "C:\Users\moffi\AppData\Local\Programs\Python\Python310\lib\http\client.py", line 1368, in getresponse
    response.begin()
  File "C:\Users\moffi\AppData\Local\Programs\Python\Python310\lib\http\client.py", line 317, in begin
    version, status, reason = self._read_status()
  File "C:\Users\moffi\AppData\Local\Programs\Python\Python310\lib\http\client.py", line 278, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "C:\Users\moffi\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\utils\defer.py", line 240, in iter_errback
    yield next(it)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\utils\python.py", line 338, in __next__
    return next(self.data)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\utils\python.py", line 338, in __next__
    return next(self.data)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\spidermw.py", line 79, in process_sync
    for r in iterable:
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\spidermw.py", line 79, in process_sync
    for r in iterable:
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 336, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\spidermw.py", line 79, in process_sync
    for r in iterable:
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\spidermw.py", line 79, in process_sync
    for r in iterable:
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 32, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\spidermw.py", line 79, in process_sync
    for r in iterable:
  File "C:\Users\moffi\PycharmProjects\scraping_web\scraper\scraper\spiders\website_spider.py", line 52, in parse_website
    driver.get("http://smithgill.com/")
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 455, in get
    self.execute(Command.GET, {"url": url})
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 442, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\remote\remote_connection.py", line 294, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\remote\remote_connection.py", line 316, in _request
    response = self._conn.request(method, url, body=body, headers=headers)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\urllib3\request.py", line 78, in request
    return self.request_encode_body(
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\urllib3\request.py", line 170, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\urllib3\poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\urllib3\util\retry.py", line 550, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\urllib3\packages\six.py", line 769, in reraise
    raise value.with_traceback(tb)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\urllib3\connectionpool.py", line 449, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\urllib3\connectionpool.py", line 444, in _make_request
    httplib_response = conn.getresponse()
  File "C:\Users\moffi\AppData\Local\Programs\Python\Python310\lib\http\client.py", line 1368, in getresponse
    response.begin()
  File "C:\Users\moffi\AppData\Local\Programs\Python\Python310\lib\http\client.py", line 317, in begin
    version, status, reason = self._read_status()
  File "C:\Users\moffi\AppData\Local\Programs\Python\Python310\lib\http\client.py", line 278, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "C:\Users\moffi\AppData\Local\Programs\Python\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
urllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))
DEBUG: Crawled (200) <GET https://starwoodtree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://ohiotreeandexcavating.com/> (referer: None)
DEBUG: Crawled (403) <GET http://www.hackettstreeservice.com/> (referer: None)
DEBUG: Crawled (403) <GET https://www.tackettstreeservice.com/> (referer: None)
INFO: Closing spider (shutdown)
DEBUG: Scraped from <200 https://www.treetechohio.com/contact>
{'emails': ['8eb368c655b84e029ed79ad7a5c1718e@sentry.wixpress.com',
            'info@mysite.com',
            '831126cb46b74583bf6f72c5061cba9d@sentry-viewer.wixpress.com',
            '605a7baede844d278b89dc95ae0a9123@sentry-next.wixpress.com'],
 'facebook': 'http://www.facebook.com/wix',
 'instagram': '',
 'linkedin': '',
 'twitter': 'http://www.twitter.com/wix'}
DEBUG: driver not found in PATH, trying Selenium Manager
DEBUG: Executing: C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\common\windows\selenium-manager.exe --browser chrome
INFO: Received SIGINT twice, forcing unclean shutdown
DEBUG: Unable to obtain driver using Selenium Manager: Selenium manager failed for: C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\common\windows\selenium-manager.exe --browser chrome. 
ERROR: Spider error processing <GET https://www.hardwicktreecare.com/> (referer: None)
Traceback (most recent call last):
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\common\service.py", line 97, in start
    path = SeleniumManager().driver_location(browser)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\common\selenium_manager.py", line 74, in driver_location
    result = self.run((binary, flag, browser))
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\common\selenium_manager.py", line 93, in run
    raise SeleniumManagerException(f"Selenium manager failed for: {command}. {stderr}")
selenium.common.exceptions.SeleniumManagerException: Message: Selenium manager failed for: C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\common\windows\selenium-manager.exe --browser chrome. 


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\utils\defer.py", line 240, in iter_errback
    yield next(it)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\utils\python.py", line 338, in __next__
    return next(self.data)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\utils\python.py", line 338, in __next__
    return next(self.data)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\spidermw.py", line 79, in process_sync
    for r in iterable:
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\spidermw.py", line 79, in process_sync
    for r in iterable:
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 336, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\spidermw.py", line 79, in process_sync
    for r in iterable:
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\spidermw.py", line 79, in process_sync
    for r in iterable:
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 32, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\spidermw.py", line 79, in process_sync
    for r in iterable:
  File "C:\Users\moffi\PycharmProjects\scraping_web\scraper\scraper\spiders\website_spider.py", line 49, in parse_website
    driver = webdriver.Chrome(desired_capabilities=desired_capabilities)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\chrome\webdriver.py", line 81, in __init__
    super().__init__(
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\chromium\webdriver.py", line 103, in __init__
    self.service.start()
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\common\service.py", line 100, in start
    raise err
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\common\service.py", line 90, in start
    self._start_process(self.path)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\selenium\webdriver\common\service.py", line 213, in _start_process
    raise WebDriverException(
selenium.common.exceptions.WebDriverException: Message: 'chromedriver' executable needs to be in PATH. Please see https://chromedriver.chromium.org/home

DEBUG: Crawled (403) <GET http://deeprootedtreeservice.com/robots.txt> (referer: None)
INFO: Ignoring response <403 http://www.hackettstreeservice.com/>: HTTP status code is not handled or not allowed
INFO: Ignoring response <403 https://www.tackettstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Retrying <GET https://herculestree.com/> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
DEBUG: Retrying <GET https://www.treesaremybusiness.com/> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
DEBUG: Retrying <GET https://specialtytreeohio.com/robots.txt> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: 25ca2007c0ebc7ae
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
DEBUG: Attempting to acquire lock 1390804873184 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 1390804873184 acquired on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Attempting to release lock 1390804873184 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 1390804873184 released on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Crawled (200) <GET http://www.columbustreeservicesllc.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.fandftrees.com/robots.txt> from <GET http://www.fandftrees.com/robots.txt>
DEBUG: Crawled (200) <GET http://www.columbustreeservicesllc.com/> (referer: None)
DEBUG: Crawled (403) <GET https://trapperstreeservice.com/robots.txt> (referer: None)
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Rule at line 14 without any user agent to enforce it on.
DEBUG: Rule at line 16 without any user agent to enforce it on.
DEBUG: Rule at line 35 without any user agent to enforce it on.
DEBUG: Rule at line 42 without any user agent to enforce it on.
DEBUG: Rule at line 43 without any user agent to enforce it on.
DEBUG: Rule at line 44 without any user agent to enforce it on.
DEBUG: Rule at line 45 without any user agent to enforce it on.
DEBUG: Rule at line 46 without any user agent to enforce it on.
DEBUG: Rule at line 47 without any user agent to enforce it on.
DEBUG: Rule at line 69 without any user agent to enforce it on.
DEBUG: Crawled (403) <GET https://www.midohiotree.org/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.treetechohio.com/robots.txt> (referer: None)
DEBUG: Filtered duplicate request: <GET http://hardwicktreecare.com/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG: Crawled (404) <GET http://ohiotreeandexcavating.net/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET https://trapperstreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.fandftrees.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.fandftrees.com/> from <GET http://www.fandftrees.com/>
DEBUG: Crawled (403) <GET https://www.midohiotree.org/> (referer: None)
INFO: Ignoring response <403 https://trapperstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET http://hardwicktreecare.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.fandftrees.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.hardwicktreecare.com/> from <GET http://hardwicktreecare.com/>
DEBUG: Redirecting (301) to <GET https://starwoodtree.com/robots.txt> from <GET http://starwoodtree.com/robots.txt>
INFO: Ignoring response <403 https://www.midohiotree.org/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET http://ohiotreeandexcavating.com/> from <GET http://ohiotreeandexcavating.net/>
DEBUG: Redirecting (301) to <GET https://ohiotreeandexcavating.com/robots.txt> from <GET http://ohiotreeandexcavating.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.russelltreeexperts.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://ohiotreeandexcavating.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET https://www.tackettstreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://www.treesaremybusiness.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://ohiotreeandexcavating.com/> from <GET http://ohiotreeandexcavating.com/>
DEBUG: Crawled (200) <GET https://www.treetechohio.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.hardwicktreecare.com/robots.txt> (referer: None)
DEBUG: Crawled (404) <GET http://ohiotreecare.com/robots.txt> (referer: None)
DEBUG: Rule at line 1 without any user agent to enforce it on.
DEBUG: Rule at line 2 without any user agent to enforce it on.
DEBUG: Rule at line 8 without any user agent to enforce it on.
DEBUG: Rule at line 9 without any user agent to enforce it on.
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Rule at line 11 without any user agent to enforce it on.
DEBUG: Rule at line 12 without any user agent to enforce it on.
DEBUG: Crawled (403) <GET http://www.hackettstreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET https://www.tackettstreeservice.com/> (referer: None)
DEBUG: Crawled (403) <GET http://www.hackettstreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.russelltreeexperts.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/akron/?utm_source=GMB&utm_medium=organic&utm_campaign=doylestown> (referer: None)
DEBUG: Crawled (200) <GET http://herculestree.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.treesaremybusiness.com/> from <GET http://www.treesaremybusiness.com/>
INFO: Ignoring response <403 https://www.tackettstreeservice.com/>: HTTP status code is not handled or not allowed
INFO: Ignoring response <403 http://www.hackettstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (403) <GET http://deeprootedtreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://deeprootedtreeservice.com/> (referer: None)
INFO: Ignoring response <403 http://deeprootedtreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://specialtytreeohio.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.treetechohio.com/contact> (referer: https://www.treetechohio.com/)
DEBUG: Crawled (200) <GET https://ohiotreeandexcavating.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://herculestree.com/> from <GET http://herculestree.com/>
DEBUG: Scraped from <200 https://www.treetechohio.com/contact>
{'emails': ['8eb368c655b84e029ed79ad7a5c1718e@sentry.wixpress.com',
            '831126cb46b74583bf6f72c5061cba9d@sentry-viewer.wixpress.com',
            'info@mysite.com',
            '605a7baede844d278b89dc95ae0a9123@sentry-next.wixpress.com'],
 'facebook': 'http://www.facebook.com/wix',
 'instagram': '',
 'linkedin': '',
 'twitter': 'http://www.twitter.com/wix'}
DEBUG: Crawled (200) <GET https://www.hardwicktreecare.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://treesaremybusiness.com/> from <GET https://www.treesaremybusiness.com/>
DEBUG: Crawled (200) <GET https://www.whymonster.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://starwoodtree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://specialtytreeohio.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.monstertreeservice.com/akron/contact-us/> from <GET https://www.whymonster.com/akron/contact-us/>
DEBUG: Redirecting (301) to <GET https://starwoodtree.com/> from <GET http://starwoodtree.com/>
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/akron/contact-us/> (referer: https://www.monstertreeservice.com/akron/?utm_source=GMB&utm_medium=organic&utm_campaign=doylestown)
DEBUG: Scraped from <200 https://www.monstertreeservice.com/akron/contact-us/>
{'emails': [],
 'facebook': 'https://www.facebook.com/MonsterTreeServiceofAkron/',
 'instagram': 'https://www.instagram.com/monstertreeservices/',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://herculestree.com/> (referer: None)
DEBUG: Crawled (200) <GET https://treesaremybusiness.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://ohiotreecare.com/> (referer: None)
DEBUG: Crawled (200) <GET https://treesaremybusiness.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://herculestree.com/contact/> from <GET https://herculestree.com/contact>
DEBUG: Crawled (200) <GET https://treesaremybusiness.com/contact-us/> (referer: https://treesaremybusiness.com/)
DEBUG: Scraped from <200 https://treesaremybusiness.com/contact-us/>
{'emails': ['millcraft@treesaremybusiness.com',
            'office@treesaremybusiness.com'],
 'facebook': 'https://www.facebook.com/treesaremybusinessohio/',
 'instagram': 'https://www.instagram.com/trees_are_my_business/',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://www.hardwicktreecare.com/contact-hardwick-tree-care> (referer: https://www.hardwicktreecare.com/)
DEBUG: Scraped from <200 https://www.hardwicktreecare.com/contact-hardwick-tree-care>
{'emails': ['8eb368c655b84e029ed79ad7a5c1718e@sentry.wixpress.com',
            '831126cb46b74583bf6f72c5061cba9d@sentry-viewer.wixpress.com',
            'info@HardwickTreeCare.com',
            'f1ffc0b5efe04e9eb9762cd808722520@sentry.wixpress.com',
            '605a7baede844d278b89dc95ae0a9123@sentry-next.wixpress.com'],
 'facebook': 'https://www.facebook.com/hardwicktreecarellc',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Redirecting (301) to <GET https://herculestree.com/contact/request-quote/> from <GET https://herculestree.com/contact/request-quote>
DEBUG: Crawled (200) <GET https://starwoodtree.com/> (referer: None)
DEBUG: Crawled (200) <GET https://herculestree.com/contact/> (referer: https://herculestree.com/)
DEBUG: Scraped from <200 https://herculestree.com/contact/>
{'emails': ['Herculestree@gmail.com', 'HerculesTree@gmail.com'],
 'facebook': 'https://www.facebook.com/HerculesTreeLLC/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://herculestree.com/contact/request-quote/> (referer: https://herculestree.com/)
DEBUG: Scraped from <200 https://herculestree.com/contact/request-quote/>
{'emails': ['info@herculesTrees.com', 'HerculesTree@gmail.com'],
 'facebook': 'https://www.facebook.com/HerculesTreeLLC/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://starwoodtree.com/contact-us/> (referer: https://starwoodtree.com/)
DEBUG: Scraped from <200 https://starwoodtree.com/contact-us/>
{'emails': ['support@starwoodtree.com'],
 'facebook': 'https://www.facebook.com/Starwoodtreeservice/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
INFO: Closing spider (finished)
INFO: Stored json feed (7 items) in: website_spider_output.json
INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 14771,
 'downloader/request_count': 59,
 'downloader/request_method_count/GET': 59,
 'downloader/response_bytes': 850618,
 'downloader/response_count': 59,
 'downloader/response_status_count/200': 33,
 'downloader/response_status_count/301': 14,
 'downloader/response_status_count/403': 10,
 'downloader/response_status_count/404': 2,
 'dupefilter/filtered': 13,
 'elapsed_time_seconds': 5.776223,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 8, 20, 29, 7, 336495),
 'httpcompression/response_bytes': 3778716,
 'httpcompression/response_count': 41,
 'httperror/response_ignored_count': 5,
 'httperror/response_ignored_status_count/403': 5,
 'item_scraped_count': 7,
 'log_count/DEBUG': 92,
 'log_count/INFO': 16,
 'request_depth_max': 1,
 'response_received_count': 45,
 'robotstxt/request_count': 21,
 'robotstxt/response_count': 21,
 'robotstxt/response_status_count/200': 14,
 'robotstxt/response_status_count/403': 5,
 'robotstxt/response_status_count/404': 2,
 'scheduler/dequeued': 35,
 'scheduler/dequeued/memory': 35,
 'scheduler/enqueued': 35,
 'scheduler/enqueued/memory': 35,
 'start_time': datetime.datetime(2022, 12, 8, 20, 29, 1, 560272)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: ff2174d84724c45c
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
DEBUG: Attempting to acquire lock 3180317303680 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 3180317303680 acquired on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Attempting to release lock 3180317303680 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 3180317303680 released on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Crawled (200) <GET http://www.columbustreeservicesllc.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.fandftrees.com/robots.txt> from <GET http://www.fandftrees.com/robots.txt>
DEBUG: Crawled (403) <GET https://trapperstreeservice.com/robots.txt> (referer: None)
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Rule at line 14 without any user agent to enforce it on.
DEBUG: Rule at line 16 without any user agent to enforce it on.
DEBUG: Rule at line 35 without any user agent to enforce it on.
DEBUG: Rule at line 42 without any user agent to enforce it on.
DEBUG: Rule at line 43 without any user agent to enforce it on.
DEBUG: Rule at line 44 without any user agent to enforce it on.
DEBUG: Rule at line 45 without any user agent to enforce it on.
DEBUG: Rule at line 46 without any user agent to enforce it on.
DEBUG: Rule at line 47 without any user agent to enforce it on.
DEBUG: Rule at line 69 without any user agent to enforce it on.
DEBUG: Crawled (403) <GET https://www.midohiotree.org/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://www.columbustreeservicesllc.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.treetechohio.com/robots.txt> (referer: None)
DEBUG: Filtered duplicate request: <GET http://hardwicktreecare.com/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG: Crawled (403) <GET https://trapperstreeservice.com/> (referer: None)
DEBUG: Crawled (403) <GET https://www.midohiotree.org/> (referer: None)
DEBUG: Crawled (200) <GET https://www.fandftrees.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.treetechohio.com/> (referer: None)
INFO: Ignoring response <403 https://trapperstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET https://www.fandftrees.com/> from <GET http://www.fandftrees.com/>
DEBUG: Crawled (200) <GET http://hardwicktreecare.com/robots.txt> (referer: None)
INFO: Ignoring response <403 https://www.midohiotree.org/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://www.fandftrees.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.hardwicktreecare.com/> from <GET http://hardwicktreecare.com/>
DEBUG: Redirecting (301) to <GET https://starwoodtree.com/robots.txt> from <GET http://starwoodtree.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.treetechohio.com/contact> (referer: https://www.treetechohio.com/)
DEBUG: Crawled (404) <GET http://ohiotreeandexcavating.net/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.hardwicktreecare.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.russelltreeexperts.com/robots.txt> (referer: None)
DEBUG: Scraped from <200 https://www.treetechohio.com/contact>
{'emails': ['605a7baede844d278b89dc95ae0a9123@sentry-next.wixpress.com',
            '8eb368c655b84e029ed79ad7a5c1718e@sentry.wixpress.com',
            '831126cb46b74583bf6f72c5061cba9d@sentry-viewer.wixpress.com',
            'info@mysite.com'],
 'facebook': 'http://www.facebook.com/wix',
 'instagram': '',
 'linkedin': '',
 'twitter': 'http://www.twitter.com/wix'}
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://ohiotreeandexcavating.com/> from <GET http://ohiotreeandexcavating.net/>
DEBUG: Crawled (403) <GET https://www.tackettstreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://www.treesaremybusiness.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://ohiotreeandexcavating.com/robots.txt> from <GET http://ohiotreeandexcavating.com/robots.txt>
DEBUG: Crawled (200) <GET http://herculestree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.russelltreeexperts.com/> (referer: None)
DEBUG: Crawled (403) <GET https://www.tackettstreeservice.com/> (referer: None)
DEBUG: Crawled (404) <GET http://ohiotreecare.com/robots.txt> (referer: None)
DEBUG: Rule at line 1 without any user agent to enforce it on.
DEBUG: Rule at line 2 without any user agent to enforce it on.
DEBUG: Rule at line 8 without any user agent to enforce it on.
DEBUG: Rule at line 9 without any user agent to enforce it on.
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Rule at line 11 without any user agent to enforce it on.
DEBUG: Rule at line 12 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/akron/?utm_source=GMB&utm_medium=organic&utm_campaign=doylestown> (referer: None)
DEBUG: Crawled (200) <GET https://ohiotreeandexcavating.com/robots.txt> (referer: None)
INFO: Ignoring response <403 https://www.tackettstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET https://ohiotreeandexcavating.com/> from <GET http://ohiotreeandexcavating.com/>
DEBUG: Crawled (403) <GET http://www.hackettstreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://www.hackettstreeservice.com/> (referer: None)
DEBUG: Crawled (403) <GET http://deeprootedtreeservice.com/robots.txt> (referer: None)
INFO: Ignoring response <403 http://www.hackettstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://specialtytreeohio.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://deeprootedtreeservice.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://herculestree.com/> from <GET http://herculestree.com/>
INFO: Ignoring response <403 http://deeprootedtreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://www.hardwicktreecare.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.treesaremybusiness.com/> from <GET http://www.treesaremybusiness.com/>
DEBUG: Crawled (200) <GET https://ohiotreeandexcavating.com/> (referer: None)
DEBUG: Crawled (200) <GET https://specialtytreeohio.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.whymonster.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://starwoodtree.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.monstertreeservice.com/akron/contact-us/> from <GET https://www.whymonster.com/akron/contact-us/>
DEBUG: Redirecting (301) to <GET https://starwoodtree.com/> from <GET http://starwoodtree.com/>
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/akron/contact-us/> (referer: https://www.monstertreeservice.com/akron/?utm_source=GMB&utm_medium=organic&utm_campaign=doylestown)
DEBUG: Scraped from <200 https://www.monstertreeservice.com/akron/contact-us/>
{'emails': [],
 'facebook': 'https://www.facebook.com/MonsterTreeServiceofAkron/',
 'instagram': 'https://www.instagram.com/monstertreeservices/',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET http://ohiotreecare.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://treesaremybusiness.com/> from <GET https://www.treesaremybusiness.com/>
DEBUG: Crawled (200) <GET https://herculestree.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.hardwicktreecare.com/contact-hardwick-tree-care> (referer: https://www.hardwicktreecare.com/)
DEBUG: Scraped from <200 https://www.hardwicktreecare.com/contact-hardwick-tree-care>
{'emails': ['605a7baede844d278b89dc95ae0a9123@sentry-next.wixpress.com',
            '8eb368c655b84e029ed79ad7a5c1718e@sentry.wixpress.com',
            '831126cb46b74583bf6f72c5061cba9d@sentry-viewer.wixpress.com',
            'info@HardwickTreeCare.com',
            'f1ffc0b5efe04e9eb9762cd808722520@sentry.wixpress.com'],
 'facebook': 'https://www.facebook.com/hardwicktreecarellc',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://herculestree.com/contact/> (referer: https://herculestree.com/)
DEBUG: Scraped from <200 https://herculestree.com/contact/>
{'emails': ['Herculestree@gmail.com', 'HerculesTree@gmail.com'],
 'facebook': 'https://www.facebook.com/HerculesTreeLLC/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://treesaremybusiness.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://herculestree.com/contact> (referer: https://herculestree.com/)
DEBUG: Crawled (200) <GET https://herculestree.com/contact/request-quote> (referer: https://herculestree.com/)
DEBUG: Crawled (200) <GET https://herculestree.com/contact/request-quote/> (referer: https://herculestree.com/)
DEBUG: Crawled (200) <GET https://treesaremybusiness.com/> (referer: None)
DEBUG: Scraped from <200 https://herculestree.com/contact>
{'emails': ['Herculestree@gmail.com', 'HerculesTree@gmail.com'],
 'facebook': 'https://www.facebook.com/HerculesTreeLLC/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Scraped from <200 https://herculestree.com/contact/request-quote>
{'emails': ['info@herculesTrees.com', 'HerculesTree@gmail.com'],
 'facebook': 'https://www.facebook.com/HerculesTreeLLC/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Scraped from <200 https://herculestree.com/contact/request-quote/>
{'emails': ['info@herculesTrees.com', 'HerculesTree@gmail.com'],
 'facebook': 'https://www.facebook.com/HerculesTreeLLC/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://starwoodtree.com/> (referer: None)
DEBUG: Crawled (200) <GET https://treesaremybusiness.com/contact-us/> (referer: https://treesaremybusiness.com/)
DEBUG: Scraped from <200 https://treesaremybusiness.com/contact-us/>
{'emails': ['office@treesaremybusiness.com',
            'millcraft@treesaremybusiness.com'],
 'facebook': 'https://www.facebook.com/treesaremybusinessohio/',
 'instagram': 'https://www.instagram.com/trees_are_my_business/',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://starwoodtree.com/contact-us/> (referer: https://starwoodtree.com/)
DEBUG: Scraped from <200 https://starwoodtree.com/contact-us/>
{'emails': ['support@starwoodtree.com'],
 'facebook': 'https://www.facebook.com/Starwoodtreeservice/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
INFO: Closing spider (finished)
INFO: Stored json feed (9 items) in: website_spider_output.json
INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 14769,
 'downloader/request_count': 59,
 'downloader/request_method_count/GET': 59,
 'downloader/response_bytes': 870249,
 'downloader/response_count': 59,
 'downloader/response_status_count/200': 35,
 'downloader/response_status_count/301': 12,
 'downloader/response_status_count/403': 10,
 'downloader/response_status_count/404': 2,
 'dupefilter/filtered': 11,
 'elapsed_time_seconds': 5.78635,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 8, 20, 31, 54, 760248),
 'httpcompression/response_bytes': 3895744,
 'httpcompression/response_count': 43,
 'httperror/response_ignored_count': 5,
 'httperror/response_ignored_status_count/403': 5,
 'item_scraped_count': 9,
 'log_count/DEBUG': 94,
 'log_count/INFO': 16,
 'request_depth_max': 1,
 'response_received_count': 47,
 'robotstxt/request_count': 21,
 'robotstxt/response_count': 21,
 'robotstxt/response_status_count/200': 14,
 'robotstxt/response_status_count/403': 5,
 'robotstxt/response_status_count/404': 2,
 'scheduler/dequeued': 35,
 'scheduler/dequeued/memory': 35,
 'scheduler/enqueued': 35,
 'scheduler/enqueued/memory': 35,
 'start_time': datetime.datetime(2022, 12, 8, 20, 31, 48, 973898)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: ee3c99733abc97ee
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
DEBUG: Attempting to acquire lock 1823293691728 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 1823293691728 acquired on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Attempting to release lock 1823293691728 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 1823293691728 released on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Crawled (200) <GET http://www.columbustreeservicesllc.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET https://trapperstreeservice.com/robots.txt> (referer: None)
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Rule at line 14 without any user agent to enforce it on.
DEBUG: Rule at line 16 without any user agent to enforce it on.
DEBUG: Rule at line 35 without any user agent to enforce it on.
DEBUG: Rule at line 42 without any user agent to enforce it on.
DEBUG: Rule at line 43 without any user agent to enforce it on.
DEBUG: Rule at line 44 without any user agent to enforce it on.
DEBUG: Rule at line 45 without any user agent to enforce it on.
DEBUG: Rule at line 46 without any user agent to enforce it on.
DEBUG: Rule at line 47 without any user agent to enforce it on.
DEBUG: Rule at line 69 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET http://www.columbustreeservicesllc.com/> (referer: None)
DEBUG: Crawled (403) <GET https://www.midohiotree.org/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.treetechohio.com/robots.txt> (referer: None)
DEBUG: Filtered duplicate request: <GET http://hardwicktreecare.com/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG: Crawled (403) <GET https://trapperstreeservice.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.fandftrees.com/robots.txt> from <GET http://www.fandftrees.com/robots.txt>
DEBUG: Crawled (403) <GET https://www.midohiotree.org/> (referer: None)
DEBUG: Crawled (200) <GET https://www.treetechohio.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.fandftrees.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://hardwicktreecare.com/robots.txt> (referer: None)
INFO: Ignoring response <403 https://trapperstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET https://www.fandftrees.com/> from <GET http://www.fandftrees.com/>
INFO: Ignoring response <403 https://www.midohiotree.org/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET https://www.hardwicktreecare.com/> from <GET http://hardwicktreecare.com/>
DEBUG: Crawled (200) <GET https://www.fandftrees.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://starwoodtree.com/robots.txt> from <GET http://starwoodtree.com/robots.txt>
DEBUG: Crawled (404) <GET http://ohiotreeandexcavating.net/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.treetechohio.com/contact> (referer: https://www.treetechohio.com/)
DEBUG: Crawled (200) <GET https://www.hardwicktreecare.com/robots.txt> (referer: None)
DEBUG: Scraped from <200 https://www.treetechohio.com/contact>
{'emails': ['605a7baede844d278b89dc95ae0a9123@sentry-next.wixpress.com',
            'info@mysite.com',
            '831126cb46b74583bf6f72c5061cba9d@sentry-viewer.wixpress.com',
            '8eb368c655b84e029ed79ad7a5c1718e@sentry.wixpress.com'],
 'facebook': 'http://www.facebook.com/wix',
 'instagram': '',
 'linkedin': '',
 'twitter': 'http://www.twitter.com/wix'}
DEBUG: Crawled (200) <GET https://www.russelltreeexperts.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.hardwicktreecare.com/> (referer: None)
DEBUG: Redirecting (301) to <GET http://ohiotreeandexcavating.com/> from <GET http://ohiotreeandexcavating.net/>
DEBUG: Crawled (200) <GET http://www.treesaremybusiness.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://www.hackettstreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET https://www.tackettstreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://www.hackettstreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/akron/?utm_source=GMB&utm_medium=organic&utm_campaign=doylestown> (referer: None)
DEBUG: Redirecting (301) to <GET https://ohiotreeandexcavating.com/robots.txt> from <GET http://ohiotreeandexcavating.com/robots.txt>
DEBUG: Crawled (200) <GET https://ohiotreeandexcavating.com/robots.txt> (referer: None)
INFO: Ignoring response <403 http://www.hackettstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (403) <GET https://www.tackettstreeservice.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.treesaremybusiness.com/> from <GET http://www.treesaremybusiness.com/>
DEBUG: Crawled (404) <GET http://ohiotreecare.com/robots.txt> (referer: None)
DEBUG: Rule at line 1 without any user agent to enforce it on.
DEBUG: Rule at line 2 without any user agent to enforce it on.
DEBUG: Rule at line 8 without any user agent to enforce it on.
DEBUG: Rule at line 9 without any user agent to enforce it on.
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Rule at line 11 without any user agent to enforce it on.
DEBUG: Rule at line 12 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://www.russelltreeexperts.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://ohiotreeandexcavating.com/> from <GET http://ohiotreeandexcavating.com/>
INFO: Ignoring response <403 https://www.tackettstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (403) <GET http://deeprootedtreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://deeprootedtreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://specialtytreeohio.com/robots.txt> (referer: None)
INFO: Ignoring response <403 http://deeprootedtreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://www.hardwicktreecare.com/contact-hardwick-tree-care> (referer: https://www.hardwicktreecare.com/)
DEBUG: Scraped from <200 https://www.hardwicktreecare.com/contact-hardwick-tree-care>
{'emails': ['605a7baede844d278b89dc95ae0a9123@sentry-next.wixpress.com',
            '831126cb46b74583bf6f72c5061cba9d@sentry-viewer.wixpress.com',
            'f1ffc0b5efe04e9eb9762cd808722520@sentry.wixpress.com',
            'info@HardwickTreeCare.com',
            '8eb368c655b84e029ed79ad7a5c1718e@sentry.wixpress.com'],
 'facebook': 'https://www.facebook.com/hardwicktreecarellc',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Redirecting (301) to <GET https://treesaremybusiness.com/> from <GET https://www.treesaremybusiness.com/>
DEBUG: Crawled (200) <GET https://www.whymonster.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://ohiotreeandexcavating.com/> (referer: None)
DEBUG: Crawled (200) <GET https://specialtytreeohio.com/> (referer: None)
DEBUG: Crawled (200) <GET http://herculestree.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.monstertreeservice.com/akron/contact-us/> from <GET https://www.whymonster.com/akron/contact-us/>
DEBUG: Crawled (200) <GET https://starwoodtree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/akron/contact-us/> (referer: https://www.monstertreeservice.com/akron/?utm_source=GMB&utm_medium=organic&utm_campaign=doylestown)
DEBUG: Redirecting (301) to <GET https://starwoodtree.com/> from <GET http://starwoodtree.com/>
DEBUG: Scraped from <200 https://www.monstertreeservice.com/akron/contact-us/>
{'emails': [],
 'facebook': 'https://www.facebook.com/MonsterTreeServiceofAkron/',
 'instagram': 'https://www.instagram.com/monstertreeservices/',
 'linkedin': '',
 'twitter': ''}
DEBUG: Redirecting (301) to <GET https://herculestree.com/> from <GET http://herculestree.com/>
DEBUG: Crawled (200) <GET https://treesaremybusiness.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://treesaremybusiness.com/> (referer: None)
DEBUG: Crawled (200) <GET https://herculestree.com/> (referer: None)
DEBUG: Crawled (200) <GET http://ohiotreecare.com/> (referer: None)
DEBUG: Crawled (200) <GET https://treesaremybusiness.com/contact-us/> (referer: https://treesaremybusiness.com/)
DEBUG: Scraped from <200 https://treesaremybusiness.com/contact-us/>
{'emails': ['millcraft@treesaremybusiness.com',
            'office@treesaremybusiness.com'],
 'facebook': 'https://www.facebook.com/treesaremybusinessohio/',
 'instagram': 'https://www.instagram.com/trees_are_my_business/',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://herculestree.com/contact> (referer: https://herculestree.com/)
DEBUG: Scraped from <200 https://herculestree.com/contact>
{'emails': ['Herculestree@gmail.com', 'HerculesTree@gmail.com'],
 'facebook': 'https://www.facebook.com/HerculesTreeLLC/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://starwoodtree.com/> (referer: None)
DEBUG: Crawled (200) <GET https://herculestree.com/contact/request-quote/> (referer: https://herculestree.com/)
DEBUG: Crawled (200) <GET https://herculestree.com/contact/> (referer: https://herculestree.com/)
DEBUG: Scraped from <200 https://herculestree.com/contact/request-quote/>
{'emails': ['info@herculesTrees.com', 'HerculesTree@gmail.com'],
 'facebook': 'https://www.facebook.com/HerculesTreeLLC/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://herculestree.com/contact/request-quote> (referer: https://herculestree.com/)
DEBUG: Scraped from <200 https://herculestree.com/contact/>
{'emails': ['Herculestree@gmail.com', 'HerculesTree@gmail.com'],
 'facebook': 'https://www.facebook.com/HerculesTreeLLC/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Scraped from <200 https://herculestree.com/contact/request-quote>
{'emails': ['info@herculesTrees.com', 'HerculesTree@gmail.com'],
 'facebook': 'https://www.facebook.com/HerculesTreeLLC/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://starwoodtree.com/contact-us/> (referer: https://starwoodtree.com/)
DEBUG: Scraped from <200 https://starwoodtree.com/contact-us/>
{'emails': ['support@starwoodtree.com'],
 'facebook': 'https://www.facebook.com/Starwoodtreeservice/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
INFO: Closing spider (finished)
INFO: Stored json feed (9 items) in: website_spider_output.json
INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 14767,
 'downloader/request_count': 59,
 'downloader/request_method_count/GET': 59,
 'downloader/response_bytes': 869431,
 'downloader/response_count': 59,
 'downloader/response_status_count/200': 35,
 'downloader/response_status_count/301': 12,
 'downloader/response_status_count/403': 10,
 'downloader/response_status_count/404': 2,
 'dupefilter/filtered': 11,
 'elapsed_time_seconds': 6.197123,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 8, 20, 39, 3, 545492),
 'httpcompression/response_bytes': 3894458,
 'httpcompression/response_count': 43,
 'httperror/response_ignored_count': 5,
 'httperror/response_ignored_status_count/403': 5,
 'item_scraped_count': 9,
 'log_count/DEBUG': 94,
 'log_count/INFO': 16,
 'request_depth_max': 1,
 'response_received_count': 47,
 'robotstxt/request_count': 21,
 'robotstxt/response_count': 21,
 'robotstxt/response_status_count/200': 14,
 'robotstxt/response_status_count/403': 5,
 'robotstxt/response_status_count/404': 2,
 'scheduler/dequeued': 35,
 'scheduler/dequeued/memory': 35,
 'scheduler/enqueued': 35,
 'scheduler/enqueued/memory': 35,
 'start_time': datetime.datetime(2022, 12, 8, 20, 38, 57, 348369)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: 6705490c6e050424
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
DEBUG: Redirecting (301) to <GET https://www.fandftrees.com/robots.txt> from <GET http://www.fandftrees.com/robots.txt>
DEBUG: Attempting to acquire lock 2808013923872 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2808013923872 acquired on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Attempting to release lock 2808013923872 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2808013923872 released on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Crawled (200) <GET http://www.columbustreeservicesllc.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET https://trapperstreeservice.com/robots.txt> (referer: None)
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Rule at line 14 without any user agent to enforce it on.
DEBUG: Rule at line 16 without any user agent to enforce it on.
DEBUG: Rule at line 35 without any user agent to enforce it on.
DEBUG: Rule at line 42 without any user agent to enforce it on.
DEBUG: Rule at line 43 without any user agent to enforce it on.
DEBUG: Rule at line 44 without any user agent to enforce it on.
DEBUG: Rule at line 45 without any user agent to enforce it on.
DEBUG: Rule at line 46 without any user agent to enforce it on.
DEBUG: Rule at line 47 without any user agent to enforce it on.
DEBUG: Rule at line 69 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://www.treetechohio.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET https://trapperstreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET http://www.columbustreeservicesllc.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.fandftrees.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.fandftrees.com/> from <GET http://www.fandftrees.com/>
DEBUG: Crawled (403) <GET https://www.midohiotree.org/robots.txt> (referer: None)
DEBUG: Filtered duplicate request: <GET http://hardwicktreecare.com/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG: Crawled (200) <GET https://www.treetechohio.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.fandftrees.com/> (referer: None)
DEBUG: Crawled (200) <GET http://www.charteroakscompany.com/robots.txt> (referer: None)
INFO: Ignoring response <403 https://trapperstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (403) <GET https://www.midohiotree.org/> (referer: None)
DEBUG: Crawled (200) <GET http://hardwicktreecare.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://starwoodtree.com/robots.txt> from <GET http://starwoodtree.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.treetechohio.com/contact> (referer: https://www.treetechohio.com/)
INFO: Ignoring response <403 https://www.midohiotree.org/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET https://www.hardwicktreecare.com/> from <GET http://hardwicktreecare.com/>
DEBUG: Crawled (404) <GET http://ohiotreeandexcavating.net/robots.txt> (referer: None)
DEBUG: Scraped from <200 https://www.treetechohio.com/contact>
{'emails': ['info@mysite.com',
            '605a7baede844d278b89dc95ae0a9123@sentry-next.wixpress.com',
            '8eb368c655b84e029ed79ad7a5c1718e@sentry.wixpress.com',
            '831126cb46b74583bf6f72c5061cba9d@sentry-viewer.wixpress.com'],
 'facebook': 'http://www.facebook.com/wix',
 'instagram': '',
 'linkedin': '',
 'twitter': 'http://www.twitter.com/wix'}
DEBUG: Crawled (200) <GET https://www.russelltreeexperts.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.hardwicktreecare.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://ohiotreeandexcavating.com/> from <GET http://ohiotreeandexcavating.net/>
DEBUG: Crawled (200) <GET https://www.hardwicktreecare.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.russelltreeexperts.com/> (referer: None)
DEBUG: Crawled (403) <GET https://www.tackettstreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://ohiotreeandexcavating.com/robots.txt> from <GET http://ohiotreeandexcavating.com/robots.txt>
DEBUG: Crawled (200) <GET http://www.treesaremybusiness.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET https://www.tackettstreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://ohiotreeandexcavating.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://ohiotreeandexcavating.com/> from <GET http://ohiotreeandexcavating.com/>
DEBUG: Crawled (200) <GET https://www.hardwicktreecare.com/contact-hardwick-tree-care> (referer: https://www.hardwicktreecare.com/)
DEBUG: Crawled (403) <GET http://www.hackettstreeservice.com/robots.txt> (referer: None)
INFO: Ignoring response <403 https://www.tackettstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (403) <GET http://www.hackettstreeservice.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.charteroakscompany.com/> from <GET http://www.charteroakscompany.com/>
DEBUG: Scraped from <200 https://www.hardwicktreecare.com/contact-hardwick-tree-care>
{'emails': ['f1ffc0b5efe04e9eb9762cd808722520@sentry.wixpress.com',
            'info@HardwickTreeCare.com',
            '605a7baede844d278b89dc95ae0a9123@sentry-next.wixpress.com',
            '8eb368c655b84e029ed79ad7a5c1718e@sentry.wixpress.com',
            '831126cb46b74583bf6f72c5061cba9d@sentry-viewer.wixpress.com'],
 'facebook': 'https://www.facebook.com/hardwicktreecarellc',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/akron/?utm_source=GMB&utm_medium=organic&utm_campaign=doylestown> (referer: None)
DEBUG: Crawled (404) <GET http://ohiotreecare.com/robots.txt> (referer: None)
DEBUG: Rule at line 1 without any user agent to enforce it on.
DEBUG: Rule at line 2 without any user agent to enforce it on.
DEBUG: Rule at line 8 without any user agent to enforce it on.
DEBUG: Rule at line 9 without any user agent to enforce it on.
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Rule at line 11 without any user agent to enforce it on.
DEBUG: Rule at line 12 without any user agent to enforce it on.
DEBUG: Redirecting (301) to <GET https://www.treesaremybusiness.com/> from <GET http://www.treesaremybusiness.com/>
INFO: Ignoring response <403 http://www.hackettstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://specialtytreeohio.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://deeprootedtreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://deeprootedtreeservice.com/> (referer: None)
INFO: Ignoring response <403 http://deeprootedtreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET http://www.timberlandtreeohio.com/robots.txt> from <GET http://timberlandtreeohio.com/robots.txt>
DEBUG: Crawled (200) <GET https://starwoodtree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://ohiotreeandexcavating.com/> (referer: None)
DEBUG: Crawled (200) <GET https://specialtytreeohio.com/> (referer: None)
DEBUG: Crawled (200) <GET http://herculestree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.charteroakscompany.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://starwoodtree.com/> from <GET http://starwoodtree.com/>
DEBUG: Crawled (200) <GET https://www.independenttree.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://cottstrees.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://treesaremybusiness.com/> from <GET https://www.treesaremybusiness.com/>
DEBUG: Crawled (403) <GET http://cottstrees.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://herculestree.com/> from <GET http://herculestree.com/>
DEBUG: Redirecting (301) to <GET http://jstreeservicesllc.com/robots.txt> from <GET http://www.jstreeservicesllc.com/robots.txt>
DEBUG: Crawled (200) <GET http://www.timberlandtreeohio.com/robots.txt> (referer: None)
INFO: Ignoring response <403 http://cottstrees.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://www.whymonster.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.ewsmithtree.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.monstertreeservice.com/akron/contact-us/> from <GET https://www.whymonster.com/akron/contact-us/>
DEBUG: Redirecting (301) to <GET http://www.timberlandtreeohio.com/> from <GET http://timberlandtreeohio.com/>
DEBUG: Crawled (200) <GET https://www.independenttree.com/?utm_source=gmb&utm_medium=local&utm_campaign=website> (referer: None)
DEBUG: Redirecting (301) to <GET https://roquetree.com/robots.txt> from <GET http://roquetree.com/robots.txt>
DEBUG: Crawled (200) <GET http://www.timberlandtreeohio.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.ewsmithtree.com/> (referer: None)
DEBUG: Crawled (200) <GET http://ohiotreecare.com/> (referer: None)
DEBUG: Crawled (200) <GET https://treesaremybusiness.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/akron/contact-us/> (referer: https://www.monstertreeservice.com/akron/?utm_source=GMB&utm_medium=organic&utm_campaign=doylestown)
DEBUG: Crawled (200) <GET http://jstreeservicesllc.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.basictreecare.com/robots.txt> from <GET http://www.basictreecare.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.independenttree.com/contact-us/> (referer: https://www.independenttree.com/?utm_source=gmb&utm_medium=local&utm_campaign=website)
DEBUG: Scraped from <200 https://www.monstertreeservice.com/akron/contact-us/>
{'emails': [],
 'facebook': 'https://www.facebook.com/MonsterTreeServiceofAkron/',
 'instagram': 'https://www.instagram.com/monstertreeservices/',
 'linkedin': '',
 'twitter': ''}
DEBUG: Scraped from <200 https://www.independenttree.com/contact-us/>
{'emails': ['info@independenttree.com'],
 'facebook': 'https://www.facebook.com/IndependentTreeOH/',
 'instagram': 'https://www.instagram.com/independenttreeservice/',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://herculestree.com/> (referer: None)
DEBUG: Crawled (200) <GET http://www.kiddertreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.ewsmithtree.com/contact-us/> (referer: https://www.ewsmithtree.com/)
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/cleveland/?utm_source=GMB&utm_medium=organic&utm_campaign=AvonLake> (referer: None)
DEBUG: Redirecting (301) to <GET https://jstreeservicesllc.com/> from <GET http://www.jstreeservicesllc.com/>
DEBUG: Redirecting (301) to <GET https://www.kiddertreemov.com/> from <GET http://www.kiddertreeservice.com/>
DEBUG: Scraped from <200 https://www.ewsmithtree.com/contact-us/>
{'emails': [], 'facebook': '', 'instagram': '', 'linkedin': '', 'twitter': ''}
DEBUG: Crawled (200) <GET https://roquetree.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://haneytreeservice.com/robots.txt> from <GET http://www.haneytreeservice.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.basictreecare.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://treesaremybusiness.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://roquetree.com/> from <GET http://roquetree.com/>
DEBUG: Crawled (200) <GET https://starwoodtree.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.kiddertreemov.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.basictreecare.com/> from <GET http://www.basictreecare.com/>
DEBUG: Redirecting (301) to <GET https://www.jttreeservicellc.com/robots.txt> from <GET http://www.jttreeservicellc.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://www.roquetree.com/> from <GET https://roquetree.com/>
DEBUG: Crawled (200) <GET http://www.timberlandtreeohio.com/> (referer: None)
DEBUG: Crawled (200) <GET https://treesaremybusiness.com/contact-us/> (referer: https://treesaremybusiness.com/)
DEBUG: Crawled (200) <GET https://www.kiddertreemov.com/> (referer: None)
DEBUG: Crawled (200) <GET http://haneytreeservice.com/robots.txt> (referer: None)
DEBUG: Scraped from <200 https://treesaremybusiness.com/contact-us/>
{'emails': ['office@treesaremybusiness.com',
            'millcraft@treesaremybusiness.com'],
 'facebook': 'https://www.facebook.com/treesaremybusinessohio/',
 'instagram': 'https://www.instagram.com/trees_are_my_business/',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://www.basictreecare.com/> (referer: None)
DEBUG: Redirecting (301) to <GET http://haneytreeservice.com/> from <GET http://www.haneytreeservice.com/>
DEBUG: Crawled (200) <GET https://www.davey.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://herculestree.com/contact/> from <GET https://herculestree.com/contact>
DEBUG: Crawled (200) <GET https://jstreeservicesllc.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://haneytreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET https://www.kandatreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://haneytreeservice.com/index.html> from <GET http://haneytreeservice.com/>
DEBUG: Crawled (403) <GET https://www.kandatreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.roquetree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.jttreeservicellc.com/robots.txt> (referer: None)
INFO: Ignoring response <403 https://www.kandatreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET https://www.ripleytreeservice.com/robots.txt> from <GET http://www.ripleytreeservice.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.roquetree.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.davey.com/residential-tree-services/local-offices/columbus-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Columbus> (referer: None)
DEBUG: Crawled (403) <GET http://www.jjsfamilytreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://www.jjsfamilytreeservice.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.jttreeservicellc.com/> from <GET http://www.jttreeservicellc.com/>
DEBUG: Crawled (200) <GET http://haneytreeservice.com/index.html> (referer: None)
INFO: Ignoring response <403 http://www.jjsfamilytreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://parkstree.net/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://jstreeservicesllc.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.davey.com/residential-tree-services/local-offices/cincinnati-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Cincinnati> (referer: None)
DEBUG: Crawled (200) <GET https://starwoodtree.com/contact-us/> (referer: https://starwoodtree.com/)
DEBUG: Crawled (200) <GET https://parkstree.net/> (referer: None)
DEBUG: Crawled (200) <GET https://www.ripleytreeservice.com/robots.txt> (referer: None)
DEBUG: Scraped from <200 https://starwoodtree.com/contact-us/>
{'emails': ['support@starwoodtree.com'],
 'facebook': 'https://www.facebook.com/Starwoodtreeservice/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Redirecting (301) to <GET https://www.ripleytreeservice.com/> from <GET http://www.ripleytreeservice.com/>
DEBUG: Crawled (200) <GET https://www.rawtreeserv.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.jttreeservicellc.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.ripleytreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://oak-tree-services-tree-service.business.site/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://rawtreeserv.com/> from <GET https://www.rawtreeserv.com/>
DEBUG: Redirecting (301) to <GET https://extremetreeservicestoledo.com/robots.txt> from <GET https://www.extremetreeservicestoledo.com/robots.txt>
DEBUG: Redirecting (301) to <GET http://jstreeservicesllc.com/contact-us/> from <GET http://www.jstreeservicesllc.com/contact-us>
DEBUG: Crawled (200) <GET https://oak-tree-services-tree-service.business.site/?utm_source=gmb&utm_medium=referral> (referer: None)
DEBUG: Crawled (200) <GET https://jstreeservicesllc.com/contact-us/> (referer: https://jstreeservicesllc.com/)
DEBUG: Scraped from <200 https://jstreeservicesllc.com/contact-us/>
{'emails': ['j_s.tree@yahoo.com'],
 'facebook': 'http://www.facebook.com/jandstreeservices',
 'instagram': 'http://www.instagram.com/jandstreeservices',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://www.davey.com/about/contact-us/?type=nonresidential> (referer: https://www.davey.com/residential-tree-services/local-offices/columbus-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Columbus)
DEBUG: Scraped from <200 https://www.davey.com/about/contact-us/?type=nonresidential>
{'emails': [],
 'facebook': 'https://www.facebook.com/DaveyTree',
 'instagram': 'https://instagram.com/DaveyTree/',
 'linkedin': 'http://www.linkedin.com/company/the-davey-tree-expert-company',
 'twitter': 'https://twitter.com/DaveyTree'}
DEBUG: Retrying <GET https://challengerstreeservice.com/robots.txt> (failed 1 times): Connection was refused by other side: 10061: No connection could be made because the target machine actively refused it..
DEBUG: Crawled (200) <GET https://parkstree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://jstreeservicesllc.com/contact-us/> (referer: None)
DEBUG: Crawled (200) <GET https://rawtreeserv.com/robots.txt> (referer: None)
DEBUG: Scraped from <200 http://jstreeservicesllc.com/contact-us/>
{'emails': ['j_s.tree@yahoo.com'],
 'facebook': 'http://www.facebook.com/jandstreeservices',
 'instagram': 'http://www.instagram.com/jandstreeservices',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://parkstree.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.savatree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://extremetreeservicestoledo.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://rawtreeserv.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.stevestoledotree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://blackstreemov.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.blackstreemov.com/> from <GET http://blackstreemov.com/>
DEBUG: Crawled (403) <GET http://www.haymakertreeandlawn.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://www.haymakertreeandlawn.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.davey.com/about/contact-us/?type=nonservicerequest> (referer: https://www.davey.com/residential-tree-services/local-offices/columbus-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Columbus)
INFO: Ignoring response <403 http://www.haymakertreeandlawn.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://www.davey.com/residential-tree-services/local-offices/northeast-cleveland-tree-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Northeast%20Cleveland> (referer: None)
DEBUG: Crawled (200) <GET https://www.blackstreemov.com/robots.txt> (referer: None)
DEBUG: Scraped from <200 https://www.davey.com/about/contact-us/?type=nonservicerequest>
{'emails': [],
 'facebook': 'https://www.facebook.com/DaveyTree',
 'instagram': 'https://instagram.com/DaveyTree/',
 'linkedin': 'http://www.linkedin.com/company/the-davey-tree-expert-company',
 'twitter': 'https://twitter.com/DaveyTree'}
DEBUG: Crawled (200) <GET https://www.davey.com/about/contact-us/?type=residential> (referer: https://www.davey.com/residential-tree-services/local-offices/columbus-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Columbus)
DEBUG: Crawled (200) <GET https://affordabletreeservicesofohiollc.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://extremetreeservicestoledo.com/> from <GET https://www.extremetreeservicestoledo.com/>
DEBUG: Crawled (200) <GET https://affordabletreeservicesofohiollc.com/> (referer: None)
DEBUG: Crawled (403) <GET http://www.cstreemulch.com/robots.txt> (referer: None)
DEBUG: Scraped from <200 https://www.davey.com/about/contact-us/?type=residential>
{'emails': [],
 'facebook': 'https://www.facebook.com/DaveyTree',
 'instagram': 'https://instagram.com/DaveyTree/',
 'linkedin': 'http://www.linkedin.com/company/the-davey-tree-expert-company',
 'twitter': 'https://twitter.com/DaveyTree'}
DEBUG: Crawled (403) <GET http://www.cstreemulch.com/> (referer: None)
DEBUG: Crawled (200) <GET https://extremetreeservicestoledo.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://treesrus-treeservice.business.site/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://www.ashtreeservicepro.com/robots.txt> (referer: None)
INFO: Ignoring response <403 http://www.cstreemulch.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (403) <GET http://www.ashtreeservicepro.com/> (referer: None)
INFO: Ignoring response <403 http://www.ashtreeservicepro.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://www.savatree.com/dayton-ohio-tree-service-lawn-care?utm_source=GMB&utm_medium=organic&utm_campaign=dayton> (referer: None)
DEBUG: Crawled (200) <GET https://www.blackstreemov.com/> (referer: None)
DEBUG: Crawled (200) <GET https://treesrus-treeservice.business.site/?utm_source=gmb&utm_medium=referral> (referer: None)
DEBUG: Redirecting (301) to <GET http://www.toddstreeservice.com/robots.txt> from <GET http://toddstreeservice.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://www.roguetreesolutions.com/robots.txt> from <GET http://www.roguetreesolutions.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://hoffmantreeservice.com/robots.txt> from <GET http://hoffmantreeservice.com/robots.txt>
DEBUG: Crawled (403) <GET http://www.toddstreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (404) <GET https://www.roguetreesolutions.com/robots.txt> (referer: None)
DEBUG: Rule at line 1 without any user agent to enforce it on.
DEBUG: Rule at line 2 without any user agent to enforce it on.
DEBUG: Rule at line 14 without any user agent to enforce it on.
DEBUG: Rule at line 15 without any user agent to enforce it on.
DEBUG: Rule at line 23 without any user agent to enforce it on.
DEBUG: Rule at line 28 without any user agent to enforce it on.
DEBUG: Rule at line 29 without any user agent to enforce it on.
DEBUG: Rule at line 32 without any user agent to enforce it on.
DEBUG: Rule at line 33 without any user agent to enforce it on.
DEBUG: Rule at line 45 without any user agent to enforce it on.
DEBUG: Rule at line 46 without any user agent to enforce it on.
DEBUG: Rule at line 47 without any user agent to enforce it on.
DEBUG: Rule at line 48 without any user agent to enforce it on.
DEBUG: Rule at line 49 without any user agent to enforce it on.
DEBUG: Rule at line 50 without any user agent to enforce it on.
DEBUG: Rule at line 51 without any user agent to enforce it on.
DEBUG: Rule at line 52 without any user agent to enforce it on.
DEBUG: Rule at line 53 without any user agent to enforce it on.
DEBUG: Rule at line 54 without any user agent to enforce it on.
DEBUG: Rule at line 55 without any user agent to enforce it on.
DEBUG: Rule at line 57 without any user agent to enforce it on.
DEBUG: Rule at line 58 without any user agent to enforce it on.
DEBUG: Rule at line 59 without any user agent to enforce it on.
DEBUG: Rule at line 61 without any user agent to enforce it on.
DEBUG: Rule at line 62 without any user agent to enforce it on.
DEBUG: Rule at line 63 without any user agent to enforce it on.
DEBUG: Rule at line 64 without any user agent to enforce it on.
DEBUG: Rule at line 65 without any user agent to enforce it on.
DEBUG: Rule at line 66 without any user agent to enforce it on.
DEBUG: Rule at line 67 without any user agent to enforce it on.
DEBUG: Rule at line 70 without any user agent to enforce it on.
DEBUG: Rule at line 72 without any user agent to enforce it on.
DEBUG: Rule at line 78 without any user agent to enforce it on.
DEBUG: Rule at line 79 without any user agent to enforce it on.
DEBUG: Rule at line 80 without any user agent to enforce it on.
DEBUG: Rule at line 81 without any user agent to enforce it on.
DEBUG: Rule at line 87 without any user agent to enforce it on.
DEBUG: Rule at line 88 without any user agent to enforce it on.
DEBUG: Rule at line 92 without any user agent to enforce it on.
DEBUG: Rule at line 94 without any user agent to enforce it on.
DEBUG: Rule at line 96 without any user agent to enforce it on.
DEBUG: Rule at line 98 without any user agent to enforce it on.
DEBUG: Rule at line 100 without any user agent to enforce it on.
DEBUG: Rule at line 101 without any user agent to enforce it on.
DEBUG: Rule at line 104 without any user agent to enforce it on.
DEBUG: Rule at line 106 without any user agent to enforce it on.
DEBUG: Rule at line 107 without any user agent to enforce it on.
DEBUG: Rule at line 108 without any user agent to enforce it on.
DEBUG: Rule at line 109 without any user agent to enforce it on.
DEBUG: Rule at line 110 without any user agent to enforce it on.
DEBUG: Rule at line 111 without any user agent to enforce it on.
DEBUG: Rule at line 112 without any user agent to enforce it on.
DEBUG: Rule at line 119 without any user agent to enforce it on.
DEBUG: Rule at line 120 without any user agent to enforce it on.
DEBUG: Rule at line 121 without any user agent to enforce it on.
DEBUG: Rule at line 122 without any user agent to enforce it on.
DEBUG: Rule at line 123 without any user agent to enforce it on.
DEBUG: Rule at line 128 without any user agent to enforce it on.
DEBUG: Rule at line 145 without any user agent to enforce it on.
DEBUG: Rule at line 155 without any user agent to enforce it on.
DEBUG: Rule at line 165 without any user agent to enforce it on.
DEBUG: Redirecting (301) to <GET http://www.toddstreeservice.com/> from <GET http://toddstreeservice.com/>
DEBUG: Crawled (200) <GET https://hoffmantreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.roguetreesolutions.com/> from <GET http://www.roguetreesolutions.com/>
DEBUG: Crawled (200) <GET https://www.roguetreesolutions.com/> (referer: None)
DEBUG: Crawled (403) <GET http://www.toddstreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://hoffmantreeservice.com/> from <GET http://hoffmantreeservice.com/>
DEBUG: Crawled (200) <GET https://extremetreeservicestoledo.com/> (referer: None)
DEBUG: Crawled (200) <GET http://urbanloggersohio.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://www.toddstreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://hoffmantreeservice.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://herculestree.com/contact/request-quote/> from <GET https://herculestree.com/contact/request-quote>
DEBUG: Redirecting (301) to <GET https://urbanloggersohio.com/> from <GET http://urbanloggersohio.com/>
INFO: Ignoring response <403 http://www.toddstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://extremetreeohio.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://extremetreeohio.com/> (referer: None)
DEBUG: Crawled (200) <GET http://rstreeservicellc.com/robots.txt> (referer: None)
DEBUG: Rule at line 1 without any user agent to enforce it on.
DEBUG: Rule at line 5 without any user agent to enforce it on.
DEBUG: Rule at line 9 without any user agent to enforce it on.
DEBUG: Rule at line 14 without any user agent to enforce it on.
DEBUG: Rule at line 19 without any user agent to enforce it on.
DEBUG: Rule at line 21 without any user agent to enforce it on.
DEBUG: Rule at line 22 without any user agent to enforce it on.
DEBUG: Rule at line 23 without any user agent to enforce it on.
DEBUG: Rule at line 26 without any user agent to enforce it on.
DEBUG: Rule at line 28 without any user agent to enforce it on.
DEBUG: Rule at line 29 without any user agent to enforce it on.
DEBUG: Rule at line 31 without any user agent to enforce it on.
DEBUG: Rule at line 32 without any user agent to enforce it on.
DEBUG: Rule at line 33 without any user agent to enforce it on.
DEBUG: Rule at line 34 without any user agent to enforce it on.
DEBUG: Rule at line 35 without any user agent to enforce it on.
DEBUG: Rule at line 36 without any user agent to enforce it on.
DEBUG: Rule at line 37 without any user agent to enforce it on.
DEBUG: Rule at line 39 without any user agent to enforce it on.
DEBUG: Rule at line 40 without any user agent to enforce it on.
DEBUG: Rule at line 43 without any user agent to enforce it on.
DEBUG: Rule at line 44 without any user agent to enforce it on.
DEBUG: Rule at line 46 without any user agent to enforce it on.
DEBUG: Rule at line 48 without any user agent to enforce it on.
DEBUG: Rule at line 51 without any user agent to enforce it on.
DEBUG: Rule at line 53 without any user agent to enforce it on.
DEBUG: Rule at line 54 without any user agent to enforce it on.
DEBUG: Rule at line 55 without any user agent to enforce it on.
DEBUG: Rule at line 56 without any user agent to enforce it on.
DEBUG: Rule at line 57 without any user agent to enforce it on.
DEBUG: Rule at line 58 without any user agent to enforce it on.
DEBUG: Rule at line 60 without any user agent to enforce it on.
DEBUG: Rule at line 61 without any user agent to enforce it on.
DEBUG: Rule at line 62 without any user agent to enforce it on.
DEBUG: Rule at line 63 without any user agent to enforce it on.
DEBUG: Rule at line 64 without any user agent to enforce it on.
DEBUG: Rule at line 65 without any user agent to enforce it on.
DEBUG: Rule at line 66 without any user agent to enforce it on.
DEBUG: Rule at line 67 without any user agent to enforce it on.
DEBUG: Rule at line 68 without any user agent to enforce it on.
DEBUG: Rule at line 69 without any user agent to enforce it on.
DEBUG: Rule at line 70 without any user agent to enforce it on.
DEBUG: Rule at line 71 without any user agent to enforce it on.
DEBUG: Rule at line 72 without any user agent to enforce it on.
DEBUG: Rule at line 73 without any user agent to enforce it on.
DEBUG: Rule at line 74 without any user agent to enforce it on.
DEBUG: Rule at line 75 without any user agent to enforce it on.
DEBUG: Rule at line 76 without any user agent to enforce it on.
DEBUG: Rule at line 77 without any user agent to enforce it on.
DEBUG: Rule at line 78 without any user agent to enforce it on.
DEBUG: Rule at line 79 without any user agent to enforce it on.
DEBUG: Rule at line 80 without any user agent to enforce it on.
DEBUG: Rule at line 81 without any user agent to enforce it on.
DEBUG: Rule at line 83 without any user agent to enforce it on.
DEBUG: Rule at line 85 without any user agent to enforce it on.
DEBUG: Rule at line 86 without any user agent to enforce it on.
DEBUG: Rule at line 87 without any user agent to enforce it on.
DEBUG: Rule at line 88 without any user agent to enforce it on.
DEBUG: Rule at line 89 without any user agent to enforce it on.
DEBUG: Rule at line 90 without any user agent to enforce it on.
DEBUG: Rule at line 91 without any user agent to enforce it on.
DEBUG: Rule at line 92 without any user agent to enforce it on.
DEBUG: Rule at line 93 without any user agent to enforce it on.
DEBUG: Rule at line 94 without any user agent to enforce it on.
DEBUG: Rule at line 95 without any user agent to enforce it on.
DEBUG: Rule at line 96 without any user agent to enforce it on.
DEBUG: Rule at line 98 without any user agent to enforce it on.
DEBUG: Rule at line 99 without any user agent to enforce it on.
DEBUG: Rule at line 100 without any user agent to enforce it on.
DEBUG: Rule at line 101 without any user agent to enforce it on.
DEBUG: Rule at line 102 without any user agent to enforce it on.
DEBUG: Rule at line 103 without any user agent to enforce it on.
DEBUG: Rule at line 104 without any user agent to enforce it on.
DEBUG: Rule at line 106 without any user agent to enforce it on.
DEBUG: Rule at line 107 without any user agent to enforce it on.
DEBUG: Rule at line 108 without any user agent to enforce it on.
DEBUG: Rule at line 110 without any user agent to enforce it on.
DEBUG: Rule at line 112 without any user agent to enforce it on.
DEBUG: Rule at line 113 without any user agent to enforce it on.
DEBUG: Rule at line 114 without any user agent to enforce it on.
DEBUG: Rule at line 115 without any user agent to enforce it on.
DEBUG: Rule at line 116 without any user agent to enforce it on.
DEBUG: Rule at line 117 without any user agent to enforce it on.
DEBUG: Rule at line 121 without any user agent to enforce it on.
DEBUG: Rule at line 122 without any user agent to enforce it on.
DEBUG: Crawled (404) <GET http://www.boonestreeservice.com/robots.txt> (referer: None)
DEBUG: Rule at line 2 without any user agent to enforce it on.
DEBUG: Rule at line 8 without any user agent to enforce it on.
DEBUG: Rule at line 9 without any user agent to enforce it on.
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Retrying <GET https://challengerstreeservice.com/robots.txt> (failed 2 times): Connection was refused by other side: 10061: No connection could be made because the target machine actively refused it..
DEBUG: Crawled (200) <GET http://rstreeservicellc.com/?utm_source=gmb&utm_medium=referral> (referer: None)
DEBUG: Crawled (200) <GET http://www.boonestreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.blackstreemov.com/contact> (referer: https://www.blackstreemov.com/)
DEBUG: Scraped from <200 https://www.blackstreemov.com/contact>
{'emails': ['blackstreeservice140@gmail.com',
            '605a7baede844d278b89dc95ae0a9123@sentry-next.wixpress.com',
            'BlacksTreeService140@gmail.com',
            '831126cb46b74583bf6f72c5061cba9d@sentry-viewer.wixpress.com'],
 'facebook': 'https://www.facebook.com/pages/Blacks-Tree-Service/205784306214398',
 'instagram': 'http://instagram.com/wix',
 'linkedin': '',
 'twitter': 'https://twitter.com/BlacksTreeMOV'}
DEBUG: Redirecting (301) to <GET https://barbertontree.com/robots.txt> from <GET https://www.barbertontree.com/robots.txt>
DEBUG: Redirecting (302) to <GET https://www.greatdanetreeexperts.com/404.html> from <GET https://www.greatdanetreeexperts.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://tree-works-ohio.com/robots.txt> from <GET http://tree-works-ohio.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.greatdanetreeexperts.com/404.html> (referer: None)
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Rule at line 11 without any user agent to enforce it on.
DEBUG: Rule at line 12 without any user agent to enforce it on.
DEBUG: Rule at line 15 without any user agent to enforce it on.
DEBUG: Rule at line 17 without any user agent to enforce it on.
DEBUG: Rule at line 28 without any user agent to enforce it on.
DEBUG: Rule at line 43 without any user agent to enforce it on.
DEBUG: Rule at line 44 without any user agent to enforce it on.
DEBUG: Rule at line 45 without any user agent to enforce it on.
DEBUG: Rule at line 46 without any user agent to enforce it on.
DEBUG: Rule at line 47 without any user agent to enforce it on.
DEBUG: Rule at line 48 without any user agent to enforce it on.
DEBUG: Rule at line 49 without any user agent to enforce it on.
DEBUG: Rule at line 50 without any user agent to enforce it on.
DEBUG: Rule at line 54 without any user agent to enforce it on.
DEBUG: Rule at line 57 without any user agent to enforce it on.
DEBUG: Rule at line 62 without any user agent to enforce it on.
DEBUG: Rule at line 63 without any user agent to enforce it on.
DEBUG: Rule at line 67 without any user agent to enforce it on.
DEBUG: Rule at line 88 without any user agent to enforce it on.
DEBUG: Rule at line 136 without any user agent to enforce it on.
DEBUG: Rule at line 141 without any user agent to enforce it on.
DEBUG: Rule at line 144 without any user agent to enforce it on.
DEBUG: Rule at line 147 without any user agent to enforce it on.
DEBUG: Rule at line 156 without any user agent to enforce it on.
DEBUG: Rule at line 157 without any user agent to enforce it on.
DEBUG: Rule at line 191 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://premiertreesllc.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://extremetreeservicestoledo.com/contact/> (referer: https://extremetreeservicestoledo.com/)
DEBUG: Crawled (200) <GET https://www.greatdanetreeexperts.com/> (referer: None)
DEBUG: Retrying <GET https://premiertreesllc.com/> (failed 1 times): 429 Unknown Status
DEBUG: Scraped from <200 https://extremetreeservicestoledo.com/contact/>
{'emails': ['extremetreeswanton@gmail.com'],
 'facebook': 'https://www.facebook.com/ToledosExtremeTree/',
 'instagram': '',
 'linkedin': '',
 'twitter': 'https://twitter.com/SuperClimber101'}
DEBUG: Crawled (200) <GET https://urbanloggersohio.com/> (referer: None)
DEBUG: Retrying <GET https://premiertreesllc.com/> (failed 2 times): 429 Unknown Status
DEBUG: Redirecting (301) to <GET https://www.treeservicenow.net/robots.txt> from <GET http://www.treeservicenow.net/robots.txt>
DEBUG: Crawled (200) <GET https://www.davey.com/residential-tree-services/local-offices/akron-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Akron> (referer: None)
DEBUG: Crawled (200) <GET http://www.treeservicedelawareoh.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://brushbandittree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://barbertontree.com/robots.txt> (referer: None)
DEBUG: Rule at line 3 without any user agent to enforce it on.
DEBUG: Redirecting (301) to <GET https://barbertontree.com/> from <GET https://www.barbertontree.com/>
DEBUG: Crawled (200) <GET http://www.treeservicedelawareoh.com/> (referer: None)
DEBUG: Retrying <GET https://barbertontree.com/robots.txt> (failed 1 times): 429 Unknown Status
DEBUG: Redirecting (301) to <GET https://lamannatreeservice.com/robots.txt> from <GET http://www.lamannatreeservice.com/robots.txt>
DEBUG: Retrying <GET https://barbertontree.com/robots.txt> (failed 2 times): 429 Unknown Status
DEBUG: Crawled (200) <GET https://herculestree.com/contact/> (referer: https://herculestree.com/)
DEBUG: Crawled (200) <GET https://urbanloggersohio.com/contact-urban-loggers/> (referer: https://urbanloggersohio.com/)
DEBUG: Crawled (200) <GET https://premiertreesllc.com/> (referer: None)
DEBUG: Scraped from <200 https://herculestree.com/contact/>
{'emails': ['Herculestree@gmail.com', 'HerculesTree@gmail.com'],
 'facebook': 'https://www.facebook.com/HerculesTreeLLC/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Scraped from <200 https://urbanloggersohio.com/contact-urban-loggers/>
{'emails': ['Info@urbanloggersohio.com'],
 'facebook': '',
 'instagram': 'https://www.instagram.com/urbanloggersllc/',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://barbertontree.com/robots.txt> (referer: None)
DEBUG: Rule at line 3 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://brushbandittree.com/> (referer: None)
DEBUG: Crawled (200) <GET http://www.treeservicedelawareoh.com/contact-us/> (referer: http://www.treeservicedelawareoh.com/)
DEBUG: Retrying <GET https://barbertontree.com/> (failed 1 times): 429 Unknown Status
DEBUG: Crawled (200) <GET https://herculestree.com/contact/request-quote/> (referer: https://herculestree.com/)
DEBUG: Scraped from <200 http://www.treeservicedelawareoh.com/contact-us/>
{'emails': [],
 'facebook': 'https://www.facebook.com/pages/biz/43015/James-Tree-Service/194987980849287/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Redirecting (301) to <GET https://treeservicenow.net/robots.txt> from <GET https://www.treeservicenow.net/robots.txt>
DEBUG: Crawled (200) <GET https://larochetree.com/robots.txt> (referer: None)
DEBUG: Retrying <GET https://premiertreesllc.com/contact-us/> (failed 1 times): 429 Unknown Status
DEBUG: Scraped from <200 https://herculestree.com/contact/request-quote/>
{'emails': ['info@herculesTrees.com', 'HerculesTree@gmail.com'],
 'facebook': 'https://www.facebook.com/HerculesTreeLLC/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Retrying <GET https://barbertontree.com/> (failed 2 times): 429 Unknown Status
DEBUG: Crawled (200) <GET https://www.stevestoledotree.com/?utm_source=googlelocal&utm_medium=organic> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.larochetree.com/> from <GET https://larochetree.com/>
DEBUG: Retrying <GET https://premiertreesllc.com/contact-us/> (failed 2 times): 429 Unknown Status
ERROR: Gave up retrying <GET https://barbertontree.com/> (failed 3 times): 429 Unknown Status
DEBUG: Crawled (429) <GET https://barbertontree.com/> (referer: None)
DEBUG: Crawled (403) <GET http://tomcotreecare.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.delmartreeservices.com/robots.txt> (referer: None)
INFO: Ignoring response <429 https://barbertontree.com/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET https://lamannatreeservice.com/?robots=1> from <GET https://lamannatreeservice.com/robots.txt>
ERROR: Gave up retrying <GET https://premiertreesllc.com/contact-us/> (failed 3 times): 429 Unknown Status
DEBUG: Crawled (429) <GET https://premiertreesllc.com/contact-us/> (referer: https://premiertreesllc.com/)
DEBUG: Crawled (403) <GET http://tomcotreecare.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.larochetree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://brushbandittree.com/contact-us/> (referer: https://brushbandittree.com/)
INFO: Ignoring response <429 https://premiertreesllc.com/contact-us/>: HTTP status code is not handled or not allowed
INFO: Ignoring response <403 http://tomcotreecare.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (403) <GET http://www.dolcestreeservice.com/robots.txt> (referer: None)
ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\engine.py", line 152, in _next_request
    request = next(self.slot.start_requests)
  File "C:\Users\moffi\PycharmProjects\scraping_web\scraper\scraper\spiders\website_spider.py", line 38, in start_requests
    yield scrapy.Request(url=url, callback=self.parse_website)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\http\request\__init__.py", line 60, in __init__
    self._set_url(url)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\http\request\__init__.py", line 108, in _set_url
    raise ValueError(f'Missing scheme in request url: {self._url}')
ValueError: Missing scheme in request url: 
DEBUG: Crawled (403) <GET http://www.dolcestreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://treeservicenow.net/robots.txt> (referer: None)
DEBUG: Scraped from <200 https://brushbandittree.com/contact-us/>
{'emails': [],
 'facebook': 'https://www.facebook.com/brushbandittree/',
 'instagram': 'https://www.instagram.com/brushband1t/',
 'linkedin': '',
 'twitter': 'https://twitter.com/brushband1t%20'}
DEBUG: Crawled (200) <GET https://lamannatreeservice.com/?robots=1> (referer: None)
INFO: Ignoring response <403 http://www.dolcestreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET https://www.treeservicenow.net/> from <GET http://www.treeservicenow.net/>
DEBUG: Crawled (200) <GET https://www.delmartreeservices.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://lamannatreeservice.com/> from <GET http://www.lamannatreeservice.com/>
DEBUG: Redirecting (301) to <GET https://lamannatreeservice.com/?robots=1> from <GET https://lamannatreeservice.com/robots.txt>
DEBUG: Crawled (200) <GET https://lamannatreeservice.com/?robots=1> (referer: None)
DEBUG: Redirecting (301) to <GET https://treeservicenow.net/> from <GET https://www.treeservicenow.net/>
ERROR: Gave up retrying <GET https://challengerstreeservice.com/robots.txt> (failed 3 times): Connection was refused by other side: 10061: No connection could be made because the target machine actively refused it..
ERROR: Error downloading <GET https://challengerstreeservice.com/robots.txt>: Connection was refused by other side: 10061: No connection could be made because the target machine actively refused it..
Traceback (most recent call last):
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 49, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061: No connection could be made because the target machine actively refused it..
DEBUG: Crawled (404) <GET https://www.napierandson.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://treeservicecolumbusohio.net/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://treeservicenow.net/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://lamannatreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.napierandson.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.larochetree.com/> (referer: None)
DEBUG: Crawled (200) <GET https://treeservicecolumbusohio.net/> (referer: None)
DEBUG: Crawled (200) <GET https://www.larochetree.com/contact> (referer: https://www.larochetree.com/)
DEBUG: Scraped from <200 https://www.larochetree.com/contact>
{'emails': ['ContactUs@larochetree.com',
            'contactus@larochetree.com',
            '605a7baede844d278b89dc95ae0a9123@sentry-next.wixpress.com',
            '831126cb46b74583bf6f72c5061cba9d@sentry-viewer.wixpress.com',
            'dd0a55ccb8124b9c9d938e3acf41f8aa@sentry.wixpress.com'],
 'facebook': 'https://www.facebook.com/LaRocheTree',
 'instagram': 'https://www.instagram.com/LaRochetree/',
 'linkedin': 'https://www.linkedin.com/company/laroche-tree-service-inc',
 'twitter': 'https://twitter.com/larochetree'}
DEBUG: Crawled (200) <GET https://lamannatreeservice.com/contact/> (referer: https://lamannatreeservice.com/)
DEBUG: Scraped from <200 https://lamannatreeservice.com/contact/>
{'emails': [],
 'facebook': 'https://www.facebook.com/lamannatreeservice/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://treeservicenow.net/> (referer: None)
DEBUG: Crawled (200) <GET https://tree-works-ohio.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://tree-works-ohio.com/> from <GET http://tree-works-ohio.com/>
DEBUG: Crawled (200) <GET https://tree-works-ohio.com/> (referer: None)
DEBUG: Crawled (200) <GET https://treeservicecolumbusohio.net/contact-us/> (referer: https://treeservicecolumbusohio.net/)
DEBUG: Retrying <GET https://challengerstreeservice.com/> (failed 1 times): Connection was refused by other side: 10061: No connection could be made because the target machine actively refused it..
DEBUG: Scraped from <200 https://treeservicecolumbusohio.net/contact-us/>
{'emails': [], 'facebook': '', 'instagram': '', 'linkedin': '', 'twitter': ''}
DEBUG: Crawled (200) <GET https://tree-works-ohio.com/contact-tree-works-for-all-your-emergency-services-tree-removal-stump-grinding-etc/> (referer: https://tree-works-ohio.com/)
DEBUG: Crawled (200) <GET https://treeservicenow.net/contact_us_1> (referer: https://treeservicenow.net/)
DEBUG: Scraped from <200 https://tree-works-ohio.com/contact-tree-works-for-all-your-emergency-services-tree-removal-stump-grinding-etc/>
{'emails': [], 'facebook': '', 'instagram': '', 'linkedin': '', 'twitter': ''}
DEBUG: Scraped from <200 https://treeservicenow.net/contact_us_1>
{'emails': [], 'facebook': '', 'instagram': '', 'linkedin': '', 'twitter': ''}
DEBUG: Retrying <GET https://challengerstreeservice.com/> (failed 2 times): Connection was refused by other side: 10061: No connection could be made because the target machine actively refused it..
ERROR: Gave up retrying <GET https://challengerstreeservice.com/> (failed 3 times): Connection was refused by other side: 10061: No connection could be made because the target machine actively refused it..
ERROR: Error downloading <GET https://challengerstreeservice.com/>
Traceback (most recent call last):
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 49, in process_request
    return (yield download_func(request=request, spider=spider))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061: No connection could be made because the target machine actively refused it..
INFO: Closing spider (finished)
INFO: Stored json feed (24 items) in: website_spider_output.json
INFO: Dumping Scrapy stats:
{'downloader/exception_count': 6,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 6,
 'downloader/request_bytes': 66794,
 'downloader/request_count': 250,
 'downloader/request_method_count/GET': 250,
 'downloader/response_bytes': 3462199,
 'downloader/response_count': 244,
 'downloader/response_status_count/200': 142,
 'downloader/response_status_count/301': 57,
 'downloader/response_status_count/302': 1,
 'downloader/response_status_count/403': 29,
 'downloader/response_status_count/404': 5,
 'downloader/response_status_count/429': 10,
 'dupefilter/filtered': 61,
 'elapsed_time_seconds': 22.584587,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 8, 22, 24, 33, 573578),
 'httpcompression/response_bytes': 14647156,
 'httpcompression/response_count': 153,
 'httperror/response_ignored_count': 16,
 'httperror/response_ignored_status_count/403': 14,
 'httperror/response_ignored_status_count/429': 2,
 'item_scraped_count': 24,
 'log_count/DEBUG': 477,
 'log_count/ERROR': 7,
 'log_count/INFO': 27,
 'request_depth_max': 1,
 'response_received_count': 178,
 'retry/count': 12,
 'retry/max_reached': 4,
 'retry/reason_count/429 Unknown Status': 8,
 'retry/reason_count/twisted.internet.error.ConnectionRefusedError': 4,
 "robotstxt/exception_count/<class 'twisted.internet.error.ConnectionRefusedError'>": 1,
 'robotstxt/request_count': 84,
 'robotstxt/response_count': 83,
 'robotstxt/response_status_count/200': 63,
 'robotstxt/response_status_count/403': 15,
 'robotstxt/response_status_count/404': 5,
 'scheduler/dequeued': 140,
 'scheduler/dequeued/memory': 140,
 'scheduler/enqueued': 140,
 'scheduler/enqueued/memory': 140,
 'start_time': datetime.datetime(2022, 12, 8, 22, 24, 10, 988991)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: f072a400819b4b92
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
DEBUG: Redirecting (301) to <GET https://www.fandftrees.com/robots.txt> from <GET http://www.fandftrees.com/robots.txt>
DEBUG: Crawled (403) <GET https://trapperstreeservice.com/robots.txt> (referer: None)
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Rule at line 14 without any user agent to enforce it on.
DEBUG: Rule at line 16 without any user agent to enforce it on.
DEBUG: Rule at line 35 without any user agent to enforce it on.
DEBUG: Rule at line 42 without any user agent to enforce it on.
DEBUG: Rule at line 43 without any user agent to enforce it on.
DEBUG: Rule at line 44 without any user agent to enforce it on.
DEBUG: Rule at line 45 without any user agent to enforce it on.
DEBUG: Rule at line 46 without any user agent to enforce it on.
DEBUG: Rule at line 47 without any user agent to enforce it on.
DEBUG: Rule at line 69 without any user agent to enforce it on.
DEBUG: Attempting to acquire lock 1681995291408 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 1681995291408 acquired on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Attempting to release lock 1681995291408 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 1681995291408 released on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Crawled (200) <GET http://www.columbustreeservicesllc.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET https://trapperstreeservice.com/> (referer: None)
DEBUG: Crawled (403) <GET https://www.midohiotree.org/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://www.columbustreeservicesllc.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.treetechohio.com/robots.txt> (referer: None)
INFO: Ignoring response <403 https://trapperstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET http://www.charteroakscompany.com/robots.txt> (referer: None)
DEBUG: Filtered duplicate request: <GET http://hardwicktreecare.com/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG: Crawled (403) <GET https://www.midohiotree.org/> (referer: None)
INFO: Ignoring response <403 https://www.midohiotree.org/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET https://starwoodtree.com/robots.txt> from <GET http://starwoodtree.com/robots.txt>
DEBUG: Crawled (200) <GET http://hardwicktreecare.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.hardwicktreecare.com/> from <GET http://hardwicktreecare.com/>
DEBUG: Crawled (404) <GET http://ohiotreeandexcavating.net/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.fandftrees.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.russelltreeexperts.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.fandftrees.com/> from <GET http://www.fandftrees.com/>
DEBUG: Crawled (200) <GET https://www.hardwicktreecare.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://ohiotreeandexcavating.com/> from <GET http://ohiotreeandexcavating.net/>
DEBUG: Crawled (200) <GET https://www.treetechohio.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.russelltreeexperts.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://ohiotreeandexcavating.com/robots.txt> from <GET http://ohiotreeandexcavating.com/robots.txt>
DEBUG: Crawled (403) <GET https://www.tackettstreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.charteroakscompany.com/> from <GET http://www.charteroakscompany.com/>
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/akron/?utm_source=GMB&utm_medium=organic&utm_campaign=doylestown> (referer: None)
DEBUG: Crawled (200) <GET http://www.treesaremybusiness.com/robots.txt> (referer: None)
DEBUG: Crawled (404) <GET http://ohiotreecare.com/robots.txt> (referer: None)
DEBUG: Rule at line 1 without any user agent to enforce it on.
DEBUG: Rule at line 2 without any user agent to enforce it on.
DEBUG: Rule at line 8 without any user agent to enforce it on.
DEBUG: Rule at line 9 without any user agent to enforce it on.
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Rule at line 11 without any user agent to enforce it on.
DEBUG: Rule at line 12 without any user agent to enforce it on.
DEBUG: Crawled (403) <GET https://www.tackettstreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.fandftrees.com/> (referer: None)
INFO: Ignoring response <403 https://www.tackettstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET https://www.treesaremybusiness.com/> from <GET http://www.treesaremybusiness.com/>
DEBUG: Crawled (403) <GET http://www.hackettstreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://www.hackettstreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.hardwicktreecare.com/> (referer: None)
INFO: Ignoring response <403 http://www.hackettstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://starwoodtree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.treetechohio.com/contact> (referer: https://www.treetechohio.com/)
DEBUG: Crawled (403) <GET http://deeprootedtreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.charteroakscompany.com/> (referer: None)
DEBUG: Crawled (200) <GET https://ohiotreeandexcavating.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://specialtytreeohio.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://herculestree.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://ohiotreeandexcavating.com/> from <GET http://ohiotreeandexcavating.com/>
DEBUG: Scraped from <200 https://www.treetechohio.com/contact>
{'emails': ['8eb368c655b84e029ed79ad7a5c1718e@sentry.wixpress.com',
            '605a7baede844d278b89dc95ae0a9123@sentry-next.wixpress.com',
            'info@mysite.com',
            '831126cb46b74583bf6f72c5061cba9d@sentry-viewer.wixpress.com'],
 'facebook': 'http://www.facebook.com/wix',
 'instagram': '',
 'linkedin': '',
 'twitter': 'http://www.twitter.com/wix'}
DEBUG: Crawled (403) <GET http://deeprootedtreeservice.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://starwoodtree.com/> from <GET http://starwoodtree.com/>
INFO: Ignoring response <403 http://deeprootedtreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET https://treesaremybusiness.com/> from <GET https://www.treesaremybusiness.com/>
DEBUG: Crawled (200) <GET https://www.whymonster.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.monstertreeservice.com/akron/contact-us/> from <GET https://www.whymonster.com/akron/contact-us/>
DEBUG: Crawled (200) <GET https://specialtytreeohio.com/> (referer: None)
DEBUG: Redirecting (301) to <GET http://www.timberlandtreeohio.com/robots.txt> from <GET http://timberlandtreeohio.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://herculestree.com/> from <GET http://herculestree.com/>
DEBUG: Crawled (200) <GET https://www.hardwicktreecare.com/contact-hardwick-tree-care> (referer: https://www.hardwicktreecare.com/)
DEBUG: Crawled (200) <GET https://ohiotreeandexcavating.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/akron/contact-us/> (referer: https://www.monstertreeservice.com/akron/?utm_source=GMB&utm_medium=organic&utm_campaign=doylestown)
DEBUG: Crawled (200) <GET https://www.independenttree.com/robots.txt> (referer: None)
DEBUG: Scraped from <200 https://www.hardwicktreecare.com/contact-hardwick-tree-care>
{'emails': ['8eb368c655b84e029ed79ad7a5c1718e@sentry.wixpress.com',
            '605a7baede844d278b89dc95ae0a9123@sentry-next.wixpress.com',
            'f1ffc0b5efe04e9eb9762cd808722520@sentry.wixpress.com',
            '831126cb46b74583bf6f72c5061cba9d@sentry-viewer.wixpress.com',
            'info@HardwickTreeCare.com'],
 'facebook': 'https://www.facebook.com/hardwicktreecarellc',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://www.ewsmithtree.com/robots.txt> (referer: None)
DEBUG: Scraped from <200 https://www.monstertreeservice.com/akron/contact-us/>
{'emails': [],
 'facebook': 'https://www.facebook.com/MonsterTreeServiceofAkron/',
 'instagram': 'https://www.instagram.com/monstertreeservices/',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://treesaremybusiness.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://cottstrees.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://www.timberlandtreeohio.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://cottstrees.com/> (referer: None)
INFO: Ignoring response <403 http://cottstrees.com/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET http://www.timberlandtreeohio.com/> from <GET http://timberlandtreeohio.com/>
DEBUG: Redirecting (301) to <GET https://roquetree.com/robots.txt> from <GET http://roquetree.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.ewsmithtree.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.independenttree.com/?utm_source=gmb&utm_medium=local&utm_campaign=website> (referer: None)
DEBUG: Crawled (200) <GET https://treesaremybusiness.com/> (referer: None)
DEBUG: Crawled (200) <GET http://www.timberlandtreeohio.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.basictreecare.com/robots.txt> from <GET http://www.basictreecare.com/robots.txt>
DEBUG: Redirecting (301) to <GET http://haneytreeservice.com/robots.txt> from <GET http://www.haneytreeservice.com/robots.txt>
DEBUG: Crawled (200) <GET https://treesaremybusiness.com/contact-us/> (referer: https://treesaremybusiness.com/)
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/cleveland/?utm_source=GMB&utm_medium=organic&utm_campaign=AvonLake> (referer: None)
DEBUG: Scraped from <200 https://treesaremybusiness.com/contact-us/>
{'emails': ['millcraft@treesaremybusiness.com',
            'office@treesaremybusiness.com'],
 'facebook': 'https://www.facebook.com/treesaremybusinessohio/',
 'instagram': 'https://www.instagram.com/trees_are_my_business/',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://roquetree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://starwoodtree.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.basictreecare.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://ohiotreecare.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://roquetree.com/> from <GET http://roquetree.com/>
DEBUG: Crawled (200) <GET http://www.kiddertreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.ewsmithtree.com/contact-us/> (referer: https://www.ewsmithtree.com/)
DEBUG: Redirecting (301) to <GET https://www.kiddertreemov.com/> from <GET http://www.kiddertreeservice.com/>
DEBUG: Redirecting (301) to <GET https://www.basictreecare.com/> from <GET http://www.basictreecare.com/>
DEBUG: Crawled (200) <GET http://haneytreeservice.com/robots.txt> (referer: None)
DEBUG: Scraped from <200 https://www.ewsmithtree.com/contact-us/>
{'emails': [], 'facebook': '', 'instagram': '', 'linkedin': '', 'twitter': ''}
DEBUG: Redirecting (301) to <GET https://www.roquetree.com/> from <GET https://roquetree.com/>
DEBUG: Redirecting (301) to <GET http://haneytreeservice.com/> from <GET http://www.haneytreeservice.com/>
DEBUG: Crawled (200) <GET https://www.basictreecare.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.independenttree.com/contact-us/> (referer: https://www.independenttree.com/?utm_source=gmb&utm_medium=local&utm_campaign=website)
DEBUG: Crawled (200) <GET http://www.timberlandtreeohio.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.jttreeservicellc.com/robots.txt> from <GET http://www.jttreeservicellc.com/robots.txt>
DEBUG: Crawled (200) <GET https://challengerstreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.kiddertreemov.com/robots.txt> (referer: None)
DEBUG: Scraped from <200 https://www.independenttree.com/contact-us/>
{'emails': ['info@independenttree.com'],
 'facebook': 'https://www.facebook.com/IndependentTreeOH/',
 'instagram': 'https://www.instagram.com/independenttreeservice/',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET http://haneytreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://haneytreeservice.com/index.html> from <GET http://haneytreeservice.com/>
DEBUG: Crawled (403) <GET https://www.kandatreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.kiddertreemov.com/> (referer: None)
DEBUG: Crawled (403) <GET https://www.kandatreeservice.com/> (referer: None)
DEBUG: Redirecting (301) to <GET http://jstreeservicesllc.com/robots.txt> from <GET http://www.jstreeservicesllc.com/robots.txt>
INFO: Ignoring response <403 https://www.kandatreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (403) <GET http://www.jjsfamilytreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.ripleytreeservice.com/robots.txt> from <GET http://www.ripleytreeservice.com/robots.txt>
DEBUG: Crawled (403) <GET http://www.jjsfamilytreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.davey.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.roquetree.com/robots.txt> (referer: None)
INFO: Ignoring response <403 http://www.jjsfamilytreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET http://haneytreeservice.com/index.html> (referer: None)
DEBUG: Crawled (200) <GET https://starwoodtree.com/contact-us/> (referer: https://starwoodtree.com/)
DEBUG: Crawled (200) <GET https://www.jttreeservicellc.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.roquetree.com/> (referer: None)
DEBUG: Scraped from <200 https://starwoodtree.com/contact-us/>
{'emails': ['support@starwoodtree.com'],
 'facebook': 'https://www.facebook.com/Starwoodtreeservice/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://parkstree.net/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.jttreeservicellc.com/> from <GET http://www.jttreeservicellc.com/>
DEBUG: Crawled (200) <GET https://parkstree.net/> (referer: None)
DEBUG: Crawled (200) <GET https://oak-tree-services-tree-service.business.site/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.davey.com/residential-tree-services/local-offices/columbus-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Columbus> (referer: None)
DEBUG: Crawled (200) <GET https://www.ripleytreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.rawtreeserv.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://jstreeservicesllc.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.jttreeservicellc.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.ripleytreeservice.com/> from <GET http://www.ripleytreeservice.com/>
DEBUG: Crawled (200) <GET https://oak-tree-services-tree-service.business.site/?utm_source=gmb&utm_medium=referral> (referer: None)
DEBUG: Redirecting (301) to <GET https://rawtreeserv.com/> from <GET https://www.rawtreeserv.com/>
DEBUG: Crawled (200) <GET https://parkstree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://parkstree.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.ripleytreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.davey.com/residential-tree-services/local-offices/cincinnati-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Cincinnati> (referer: None)
DEBUG: Redirecting (301) to <GET https://jstreeservicesllc.com/> from <GET http://www.jstreeservicesllc.com/>
DEBUG: Crawled (200) <GET https://rawtreeserv.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://challengerstreeservice.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://extremetreeservicestoledo.com/robots.txt> from <GET https://www.extremetreeservicestoledo.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.stevestoledotree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.savatree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://rawtreeserv.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.davey.com/about/contact-us/?type=residential> (referer: https://www.davey.com/residential-tree-services/local-offices/columbus-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Columbus)
DEBUG: Crawled (200) <GET https://www.stevestoledotree.com/?utm_source=googlelocal&utm_medium=organic> (referer: None)
DEBUG: Crawled (200) <GET http://blackstreemov.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://www.haymakertreeandlawn.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://www.haymakertreeandlawn.com/> (referer: None)
DEBUG: Scraped from <200 https://www.davey.com/about/contact-us/?type=residential>
{'emails': [],
 'facebook': 'https://www.facebook.com/DaveyTree',
 'instagram': 'https://instagram.com/DaveyTree/',
 'linkedin': 'http://www.linkedin.com/company/the-davey-tree-expert-company',
 'twitter': 'https://twitter.com/DaveyTree'}
DEBUG: Redirecting (301) to <GET https://www.blackstreemov.com/> from <GET http://blackstreemov.com/>
INFO: Ignoring response <403 http://www.haymakertreeandlawn.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://jstreeservicesllc.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://www.cstreemulch.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://www.cstreemulch.com/> (referer: None)
DEBUG: Crawled (200) <GET https://affordabletreeservicesofohiollc.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.davey.com/residential-tree-services/local-offices/northeast-cleveland-tree-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Northeast%20Cleveland> (referer: None)
INFO: Ignoring response <403 http://www.cstreemulch.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://affordabletreeservicesofohiollc.com/> (referer: None)
DEBUG: Crawled (200) <GET https://treesrus-treeservice.business.site/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.blackstreemov.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.blackstreemov.com/> (referer: None)
DEBUG: Crawled (403) <GET http://www.ashtreeservicepro.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://www.ashtreeservicepro.com/> (referer: None)
DEBUG: Crawled (200) <GET https://extremetreeservicestoledo.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://treesrus-treeservice.business.site/?utm_source=gmb&utm_medium=referral> (referer: None)
INFO: Ignoring response <403 http://www.ashtreeservicepro.com/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET http://www.toddstreeservice.com/robots.txt> from <GET http://toddstreeservice.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://www.roguetreesolutions.com/robots.txt> from <GET http://www.roguetreesolutions.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://hoffmantreeservice.com/robots.txt> from <GET http://hoffmantreeservice.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://extremetreeservicestoledo.com/> from <GET https://www.extremetreeservicestoledo.com/>
DEBUG: Crawled (404) <GET https://www.roguetreesolutions.com/robots.txt> (referer: None)
DEBUG: Rule at line 1 without any user agent to enforce it on.
DEBUG: Rule at line 2 without any user agent to enforce it on.
DEBUG: Rule at line 14 without any user agent to enforce it on.
DEBUG: Rule at line 15 without any user agent to enforce it on.
DEBUG: Rule at line 23 without any user agent to enforce it on.
DEBUG: Rule at line 28 without any user agent to enforce it on.
DEBUG: Rule at line 29 without any user agent to enforce it on.
DEBUG: Rule at line 32 without any user agent to enforce it on.
DEBUG: Rule at line 33 without any user agent to enforce it on.
DEBUG: Rule at line 45 without any user agent to enforce it on.
DEBUG: Rule at line 46 without any user agent to enforce it on.
DEBUG: Rule at line 47 without any user agent to enforce it on.
DEBUG: Rule at line 48 without any user agent to enforce it on.
DEBUG: Rule at line 49 without any user agent to enforce it on.
DEBUG: Rule at line 50 without any user agent to enforce it on.
DEBUG: Rule at line 51 without any user agent to enforce it on.
DEBUG: Rule at line 52 without any user agent to enforce it on.
DEBUG: Rule at line 53 without any user agent to enforce it on.
DEBUG: Rule at line 54 without any user agent to enforce it on.
DEBUG: Rule at line 55 without any user agent to enforce it on.
DEBUG: Rule at line 57 without any user agent to enforce it on.
DEBUG: Rule at line 58 without any user agent to enforce it on.
DEBUG: Rule at line 59 without any user agent to enforce it on.
DEBUG: Rule at line 61 without any user agent to enforce it on.
DEBUG: Rule at line 62 without any user agent to enforce it on.
DEBUG: Rule at line 63 without any user agent to enforce it on.
DEBUG: Rule at line 64 without any user agent to enforce it on.
DEBUG: Rule at line 65 without any user agent to enforce it on.
DEBUG: Rule at line 66 without any user agent to enforce it on.
DEBUG: Rule at line 67 without any user agent to enforce it on.
DEBUG: Rule at line 70 without any user agent to enforce it on.
DEBUG: Rule at line 72 without any user agent to enforce it on.
DEBUG: Rule at line 78 without any user agent to enforce it on.
DEBUG: Rule at line 79 without any user agent to enforce it on.
DEBUG: Rule at line 80 without any user agent to enforce it on.
DEBUG: Rule at line 81 without any user agent to enforce it on.
DEBUG: Rule at line 87 without any user agent to enforce it on.
DEBUG: Rule at line 88 without any user agent to enforce it on.
DEBUG: Rule at line 92 without any user agent to enforce it on.
DEBUG: Rule at line 94 without any user agent to enforce it on.
DEBUG: Rule at line 96 without any user agent to enforce it on.
DEBUG: Rule at line 98 without any user agent to enforce it on.
DEBUG: Rule at line 100 without any user agent to enforce it on.
DEBUG: Rule at line 101 without any user agent to enforce it on.
DEBUG: Rule at line 104 without any user agent to enforce it on.
DEBUG: Rule at line 106 without any user agent to enforce it on.
DEBUG: Rule at line 107 without any user agent to enforce it on.
DEBUG: Rule at line 108 without any user agent to enforce it on.
DEBUG: Rule at line 109 without any user agent to enforce it on.
DEBUG: Rule at line 110 without any user agent to enforce it on.
DEBUG: Rule at line 111 without any user agent to enforce it on.
DEBUG: Rule at line 112 without any user agent to enforce it on.
DEBUG: Rule at line 119 without any user agent to enforce it on.
DEBUG: Rule at line 120 without any user agent to enforce it on.
DEBUG: Rule at line 121 without any user agent to enforce it on.
DEBUG: Rule at line 122 without any user agent to enforce it on.
DEBUG: Rule at line 123 without any user agent to enforce it on.
DEBUG: Rule at line 128 without any user agent to enforce it on.
DEBUG: Rule at line 145 without any user agent to enforce it on.
DEBUG: Rule at line 155 without any user agent to enforce it on.
DEBUG: Rule at line 165 without any user agent to enforce it on.
DEBUG: Crawled (403) <GET http://www.toddstreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://jstreeservicesllc.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.savatree.com/dayton-ohio-tree-service-lawn-care?utm_source=GMB&utm_medium=organic&utm_campaign=dayton> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.roguetreesolutions.com/> from <GET http://www.roguetreesolutions.com/>
DEBUG: Crawled (200) <GET https://extremetreeservicestoledo.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://urbanloggersohio.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.roguetreesolutions.com/> (referer: None)
DEBUG: Redirecting (301) to <GET http://www.toddstreeservice.com/> from <GET http://toddstreeservice.com/>
DEBUG: Crawled (200) <GET https://hoffmantreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://urbanloggersohio.com/> from <GET http://urbanloggersohio.com/>
DEBUG: Redirecting (301) to <GET https://hoffmantreeservice.com/> from <GET http://hoffmantreeservice.com/>
DEBUG: Crawled (403) <GET http://www.toddstreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://extremetreeohio.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://extremetreeservicestoledo.com/> (referer: None)
DEBUG: Crawled (200) <GET https://hoffmantreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://extremetreeohio.com/> (referer: None)
DEBUG: Crawled (403) <GET http://www.toddstreeservice.com/> (referer: None)
INFO: Ignoring response <403 http://www.toddstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (404) <GET http://www.boonestreeservice.com/robots.txt> (referer: None)
DEBUG: Rule at line 2 without any user agent to enforce it on.
DEBUG: Rule at line 8 without any user agent to enforce it on.
DEBUG: Rule at line 9 without any user agent to enforce it on.
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET http://rstreeservicellc.com/robots.txt> (referer: None)
DEBUG: Rule at line 1 without any user agent to enforce it on.
DEBUG: Rule at line 5 without any user agent to enforce it on.
DEBUG: Rule at line 9 without any user agent to enforce it on.
DEBUG: Rule at line 14 without any user agent to enforce it on.
DEBUG: Rule at line 19 without any user agent to enforce it on.
DEBUG: Rule at line 21 without any user agent to enforce it on.
DEBUG: Rule at line 22 without any user agent to enforce it on.
DEBUG: Rule at line 23 without any user agent to enforce it on.
DEBUG: Rule at line 26 without any user agent to enforce it on.
DEBUG: Rule at line 28 without any user agent to enforce it on.
DEBUG: Rule at line 29 without any user agent to enforce it on.
DEBUG: Rule at line 31 without any user agent to enforce it on.
DEBUG: Rule at line 32 without any user agent to enforce it on.
DEBUG: Rule at line 33 without any user agent to enforce it on.
DEBUG: Rule at line 34 without any user agent to enforce it on.
DEBUG: Rule at line 35 without any user agent to enforce it on.
DEBUG: Rule at line 36 without any user agent to enforce it on.
DEBUG: Rule at line 37 without any user agent to enforce it on.
DEBUG: Rule at line 39 without any user agent to enforce it on.
DEBUG: Rule at line 40 without any user agent to enforce it on.
DEBUG: Rule at line 43 without any user agent to enforce it on.
DEBUG: Rule at line 44 without any user agent to enforce it on.
DEBUG: Rule at line 46 without any user agent to enforce it on.
DEBUG: Rule at line 48 without any user agent to enforce it on.
DEBUG: Rule at line 51 without any user agent to enforce it on.
DEBUG: Rule at line 53 without any user agent to enforce it on.
DEBUG: Rule at line 54 without any user agent to enforce it on.
DEBUG: Rule at line 55 without any user agent to enforce it on.
DEBUG: Rule at line 56 without any user agent to enforce it on.
DEBUG: Rule at line 57 without any user agent to enforce it on.
DEBUG: Rule at line 58 without any user agent to enforce it on.
DEBUG: Rule at line 60 without any user agent to enforce it on.
DEBUG: Rule at line 61 without any user agent to enforce it on.
DEBUG: Rule at line 62 without any user agent to enforce it on.
DEBUG: Rule at line 63 without any user agent to enforce it on.
DEBUG: Rule at line 64 without any user agent to enforce it on.
DEBUG: Rule at line 65 without any user agent to enforce it on.
DEBUG: Rule at line 66 without any user agent to enforce it on.
DEBUG: Rule at line 67 without any user agent to enforce it on.
DEBUG: Rule at line 68 without any user agent to enforce it on.
DEBUG: Rule at line 69 without any user agent to enforce it on.
DEBUG: Rule at line 70 without any user agent to enforce it on.
DEBUG: Rule at line 71 without any user agent to enforce it on.
DEBUG: Rule at line 72 without any user agent to enforce it on.
DEBUG: Rule at line 73 without any user agent to enforce it on.
DEBUG: Rule at line 74 without any user agent to enforce it on.
DEBUG: Rule at line 75 without any user agent to enforce it on.
DEBUG: Rule at line 76 without any user agent to enforce it on.
DEBUG: Rule at line 77 without any user agent to enforce it on.
DEBUG: Rule at line 78 without any user agent to enforce it on.
DEBUG: Rule at line 79 without any user agent to enforce it on.
DEBUG: Rule at line 80 without any user agent to enforce it on.
DEBUG: Rule at line 81 without any user agent to enforce it on.
DEBUG: Rule at line 83 without any user agent to enforce it on.
DEBUG: Rule at line 85 without any user agent to enforce it on.
DEBUG: Rule at line 86 without any user agent to enforce it on.
DEBUG: Rule at line 87 without any user agent to enforce it on.
DEBUG: Rule at line 88 without any user agent to enforce it on.
DEBUG: Rule at line 89 without any user agent to enforce it on.
DEBUG: Rule at line 90 without any user agent to enforce it on.
DEBUG: Rule at line 91 without any user agent to enforce it on.
DEBUG: Rule at line 92 without any user agent to enforce it on.
DEBUG: Rule at line 93 without any user agent to enforce it on.
DEBUG: Rule at line 94 without any user agent to enforce it on.
DEBUG: Rule at line 95 without any user agent to enforce it on.
DEBUG: Rule at line 96 without any user agent to enforce it on.
DEBUG: Rule at line 98 without any user agent to enforce it on.
DEBUG: Rule at line 99 without any user agent to enforce it on.
DEBUG: Rule at line 100 without any user agent to enforce it on.
DEBUG: Rule at line 101 without any user agent to enforce it on.
DEBUG: Rule at line 102 without any user agent to enforce it on.
DEBUG: Rule at line 103 without any user agent to enforce it on.
DEBUG: Rule at line 104 without any user agent to enforce it on.
DEBUG: Rule at line 106 without any user agent to enforce it on.
DEBUG: Rule at line 107 without any user agent to enforce it on.
DEBUG: Rule at line 108 without any user agent to enforce it on.
DEBUG: Rule at line 110 without any user agent to enforce it on.
DEBUG: Rule at line 112 without any user agent to enforce it on.
DEBUG: Rule at line 113 without any user agent to enforce it on.
DEBUG: Rule at line 114 without any user agent to enforce it on.
DEBUG: Rule at line 115 without any user agent to enforce it on.
DEBUG: Rule at line 116 without any user agent to enforce it on.
DEBUG: Rule at line 117 without any user agent to enforce it on.
DEBUG: Rule at line 121 without any user agent to enforce it on.
DEBUG: Rule at line 122 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://www.davey.com/about/contact-us/?type=nonservicerequest> (referer: https://www.davey.com/residential-tree-services/local-offices/columbus-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Columbus)
DEBUG: Crawled (200) <GET http://www.boonestreeservice.com/> (referer: None)
DEBUG: Scraped from <200 https://www.davey.com/about/contact-us/?type=nonservicerequest>
{'emails': [],
 'facebook': 'https://www.facebook.com/DaveyTree',
 'instagram': 'https://instagram.com/DaveyTree/',
 'linkedin': 'http://www.linkedin.com/company/the-davey-tree-expert-company',
 'twitter': 'https://twitter.com/DaveyTree'}
DEBUG: Redirecting (301) to <GET http://jstreeservicesllc.com/contact-us/> from <GET http://www.jstreeservicesllc.com/contact-us>
DEBUG: Crawled (200) <GET https://extremetreeservicestoledo.com/contact/> (referer: https://extremetreeservicestoledo.com/)
DEBUG: Crawled (200) <GET http://rstreeservicellc.com/?utm_source=gmb&utm_medium=referral> (referer: None)
DEBUG: Scraped from <200 https://extremetreeservicestoledo.com/contact/>
{'emails': ['extremetreeswanton@gmail.com'],
 'facebook': 'https://www.facebook.com/ToledosExtremeTree/',
 'instagram': '',
 'linkedin': '',
 'twitter': 'https://twitter.com/SuperClimber101'}
DEBUG: Redirecting (302) to <GET https://www.greatdanetreeexperts.com/404.html> from <GET https://www.greatdanetreeexperts.com/robots.txt>
DEBUG: Crawled (200) <GET https://jstreeservicesllc.com/contact-us/> (referer: https://jstreeservicesllc.com/)
DEBUG: Redirecting (301) to <GET https://barbertontree.com/robots.txt> from <GET https://www.barbertontree.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://tree-works-ohio.com/robots.txt> from <GET http://tree-works-ohio.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.greatdanetreeexperts.com/404.html> (referer: None)
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Rule at line 11 without any user agent to enforce it on.
DEBUG: Rule at line 12 without any user agent to enforce it on.
DEBUG: Rule at line 15 without any user agent to enforce it on.
DEBUG: Rule at line 17 without any user agent to enforce it on.
DEBUG: Rule at line 28 without any user agent to enforce it on.
DEBUG: Rule at line 43 without any user agent to enforce it on.
DEBUG: Rule at line 44 without any user agent to enforce it on.
DEBUG: Rule at line 45 without any user agent to enforce it on.
DEBUG: Rule at line 46 without any user agent to enforce it on.
DEBUG: Rule at line 47 without any user agent to enforce it on.
DEBUG: Rule at line 48 without any user agent to enforce it on.
DEBUG: Rule at line 49 without any user agent to enforce it on.
DEBUG: Rule at line 50 without any user agent to enforce it on.
DEBUG: Rule at line 54 without any user agent to enforce it on.
DEBUG: Rule at line 57 without any user agent to enforce it on.
DEBUG: Rule at line 62 without any user agent to enforce it on.
DEBUG: Rule at line 63 without any user agent to enforce it on.
DEBUG: Rule at line 67 without any user agent to enforce it on.
DEBUG: Rule at line 88 without any user agent to enforce it on.
DEBUG: Rule at line 136 without any user agent to enforce it on.
DEBUG: Rule at line 141 without any user agent to enforce it on.
DEBUG: Rule at line 144 without any user agent to enforce it on.
DEBUG: Rule at line 147 without any user agent to enforce it on.
DEBUG: Rule at line 156 without any user agent to enforce it on.
DEBUG: Rule at line 157 without any user agent to enforce it on.
DEBUG: Rule at line 191 without any user agent to enforce it on.
DEBUG: Scraped from <200 https://jstreeservicesllc.com/contact-us/>
{'emails': ['j_s.tree@yahoo.com'],
 'facebook': 'http://www.facebook.com/jandstreeservices',
 'instagram': 'http://www.instagram.com/jandstreeservices',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://premiertreesllc.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.davey.com/about/contact-us/?type=nonresidential> (referer: https://www.davey.com/residential-tree-services/local-offices/columbus-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Columbus)
DEBUG: Crawled (200) <GET https://www.greatdanetreeexperts.com/> (referer: None)
DEBUG: Crawled (200) <GET http://jstreeservicesllc.com/contact-us/> (referer: None)
DEBUG: Scraped from <200 https://www.davey.com/about/contact-us/?type=nonresidential>
{'emails': [],
 'facebook': 'https://www.facebook.com/DaveyTree',
 'instagram': 'https://instagram.com/DaveyTree/',
 'linkedin': 'http://www.linkedin.com/company/the-davey-tree-expert-company',
 'twitter': 'https://twitter.com/DaveyTree'}
DEBUG: Redirecting (301) to <GET https://www.treeservicenow.net/robots.txt> from <GET http://www.treeservicenow.net/robots.txt>
DEBUG: Retrying <GET https://premiertreesllc.com/> (failed 1 times): 429 Unknown Status
DEBUG: Scraped from <200 http://jstreeservicesllc.com/contact-us/>
{'emails': ['j_s.tree@yahoo.com'],
 'facebook': 'http://www.facebook.com/jandstreeservices',
 'instagram': 'http://www.instagram.com/jandstreeservices',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://urbanloggersohio.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.davey.com/residential-tree-services/local-offices/akron-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Akron> (referer: None)
DEBUG: Retrying <GET https://premiertreesllc.com/> (failed 2 times): 429 Unknown Status
DEBUG: Redirecting (301) to <GET https://lamannatreeservice.com/robots.txt> from <GET http://www.lamannatreeservice.com/robots.txt>
DEBUG: Crawled (200) <GET https://tree-works-ohio.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://brushbandittree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://larochetree.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.larochetree.com/> from <GET https://larochetree.com/>
DEBUG: Crawled (200) <GET https://barbertontree.com/robots.txt> (referer: None)
DEBUG: Rule at line 3 without any user agent to enforce it on.
DEBUG: Redirecting (301) to <GET https://tree-works-ohio.com/> from <GET http://tree-works-ohio.com/>
DEBUG: Redirecting (301) to <GET https://barbertontree.com/> from <GET https://www.barbertontree.com/>
DEBUG: Retrying <GET https://barbertontree.com/robots.txt> (failed 1 times): 429 Unknown Status
DEBUG: Crawled (200) <GET https://premiertreesllc.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.larochetree.com/robots.txt> (referer: None)
DEBUG: Retrying <GET https://barbertontree.com/robots.txt> (failed 2 times): 429 Unknown Status
DEBUG: Crawled (403) <GET http://tomcotreecare.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://urbanloggersohio.com/contact-urban-loggers/> (referer: https://urbanloggersohio.com/)
DEBUG: Crawled (403) <GET http://tomcotreecare.com/> (referer: None)
DEBUG: Crawled (200) <GET https://barbertontree.com/robots.txt> (referer: None)
DEBUG: Rule at line 3 without any user agent to enforce it on.
DEBUG: Scraped from <200 https://urbanloggersohio.com/contact-urban-loggers/>
{'emails': ['Info@urbanloggersohio.com'],
 'facebook': '',
 'instagram': 'https://www.instagram.com/urbanloggersllc/',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://www.larochetree.com/> (referer: None)
INFO: Ignoring response <403 http://tomcotreecare.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://www.delmartreeservices.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://lamannatreeservice.com/?robots=1> from <GET https://lamannatreeservice.com/robots.txt>
DEBUG: Retrying <GET https://barbertontree.com/> (failed 1 times): 429 Unknown Status
DEBUG: Retrying <GET https://premiertreesllc.com/contact-us/> (failed 1 times): 429 Unknown Status
DEBUG: Crawled (403) <GET http://www.dolcestreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://www.dolcestreeservice.com/> (referer: None)
DEBUG: Retrying <GET https://barbertontree.com/> (failed 2 times): 429 Unknown Status
DEBUG: Crawled (200) <GET https://tree-works-ohio.com/> (referer: None)
DEBUG: Retrying <GET https://premiertreesllc.com/contact-us/> (failed 2 times): 429 Unknown Status
INFO: Ignoring response <403 http://www.dolcestreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://brushbandittree.com/> (referer: None)
DEBUG: Crawled (200) <GET https://lamannatreeservice.com/?robots=1> (referer: None)
ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\engine.py", line 152, in _next_request
    request = next(self.slot.start_requests)
  File "C:\Users\moffi\PycharmProjects\scraping_web\scraper\scraper\spiders\website_spider.py", line 38, in start_requests
    yield scrapy.Request(url=url, callback=self.parse_website)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\http\request\__init__.py", line 60, in __init__
    self._set_url(url)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\http\request\__init__.py", line 108, in _set_url
    raise ValueError(f'Missing scheme in request url: {self._url}')
ValueError: Missing scheme in request url: 
ERROR: Gave up retrying <GET https://barbertontree.com/> (failed 3 times): 429 Unknown Status
DEBUG: Crawled (429) <GET https://barbertontree.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.blackstreemov.com/contact> (referer: https://www.blackstreemov.com/)
ERROR: Gave up retrying <GET https://premiertreesllc.com/contact-us/> (failed 3 times): 429 Unknown Status
DEBUG: Crawled (429) <GET https://premiertreesllc.com/contact-us/> (referer: https://premiertreesllc.com/)
DEBUG: Redirecting (301) to <GET https://lamannatreeservice.com/> from <GET http://www.lamannatreeservice.com/>
DEBUG: Redirecting (301) to <GET https://lamannatreeservice.com/?robots=1> from <GET https://lamannatreeservice.com/robots.txt>
INFO: Ignoring response <429 https://barbertontree.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://lamannatreeservice.com/?robots=1> (referer: None)
DEBUG: Crawled (200) <GET https://www.larochetree.com/contact> (referer: https://www.larochetree.com/)
DEBUG: Scraped from <200 https://www.blackstreemov.com/contact>
{'emails': ['605a7baede844d278b89dc95ae0a9123@sentry-next.wixpress.com',
            'BlacksTreeService140@gmail.com',
            '831126cb46b74583bf6f72c5061cba9d@sentry-viewer.wixpress.com',
            'blackstreeservice140@gmail.com'],
 'facebook': 'https://www.facebook.com/pages/Blacks-Tree-Service/205784306214398',
 'instagram': 'http://instagram.com/wix',
 'linkedin': '',
 'twitter': 'https://twitter.com/BlacksTreeMOV'}
INFO: Ignoring response <429 https://premiertreesllc.com/contact-us/>: HTTP status code is not handled or not allowed
DEBUG: Scraped from <200 https://www.larochetree.com/contact>
{'emails': ['ContactUs@larochetree.com',
            '605a7baede844d278b89dc95ae0a9123@sentry-next.wixpress.com',
            '831126cb46b74583bf6f72c5061cba9d@sentry-viewer.wixpress.com',
            'contactus@larochetree.com',
            'dd0a55ccb8124b9c9d938e3acf41f8aa@sentry.wixpress.com'],
 'facebook': 'https://www.facebook.com/LaRocheTree',
 'instagram': 'https://www.instagram.com/LaRochetree/',
 'linkedin': 'https://www.linkedin.com/company/laroche-tree-service-inc',
 'twitter': 'https://twitter.com/larochetree'}
DEBUG: Crawled (200) <GET https://treeservicecolumbusohio.net/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://brushbandittree.com/contact-us/> (referer: https://brushbandittree.com/)
DEBUG: Crawled (200) <GET https://www.delmartreeservices.com/> (referer: None)
DEBUG: Crawled (200) <GET https://tree-works-ohio.com/contact-tree-works-for-all-your-emergency-services-tree-removal-stump-grinding-etc/> (referer: https://tree-works-ohio.com/)
DEBUG: Scraped from <200 https://brushbandittree.com/contact-us/>
{'emails': [],
 'facebook': 'https://www.facebook.com/brushbandittree/',
 'instagram': 'https://www.instagram.com/brushband1t/',
 'linkedin': '',
 'twitter': 'https://twitter.com/brushband1t%20'}
DEBUG: Crawled (200) <GET http://www.treeservicedelawareoh.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://lamannatreeservice.com/> (referer: None)
DEBUG: Crawled (404) <GET https://www.napierandson.com/robots.txt> (referer: None)
DEBUG: Scraped from <200 https://tree-works-ohio.com/contact-tree-works-for-all-your-emergency-services-tree-removal-stump-grinding-etc/>
{'emails': [], 'facebook': '', 'instagram': '', 'linkedin': '', 'twitter': ''}
DEBUG: Crawled (200) <GET https://treeservicecolumbusohio.net/> (referer: None)
DEBUG: Crawled (200) <GET http://www.treeservicedelawareoh.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.napierandson.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://treeservicenow.net/robots.txt> from <GET https://www.treeservicenow.net/robots.txt>
DEBUG: Crawled (200) <GET http://www.treeservicedelawareoh.com/contact-us/> (referer: http://www.treeservicedelawareoh.com/)
DEBUG: Crawled (200) <GET https://lamannatreeservice.com/contact/> (referer: https://lamannatreeservice.com/)
DEBUG: Scraped from <200 http://www.treeservicedelawareoh.com/contact-us/>
{'emails': [],
 'facebook': 'https://www.facebook.com/pages/biz/43015/James-Tree-Service/194987980849287/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://herculestree.com/> (referer: None)
DEBUG: Scraped from <200 https://lamannatreeservice.com/contact/>
{'emails': [],
 'facebook': 'https://www.facebook.com/lamannatreeservice/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://treeservicenow.net/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.treeservicenow.net/> from <GET http://www.treeservicenow.net/>
DEBUG: Redirecting (301) to <GET https://treeservicenow.net/> from <GET https://www.treeservicenow.net/>
DEBUG: Crawled (200) <GET https://treeservicenow.net/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://herculestree.com/contact/> (referer: https://herculestree.com/)
DEBUG: Crawled (200) <GET https://herculestree.com/contact> (referer: https://herculestree.com/)
DEBUG: Crawled (200) <GET https://treeservicenow.net/> (referer: None)
DEBUG: Scraped from <200 https://herculestree.com/contact/>
{'emails': ['Herculestree@gmail.com', 'HerculesTree@gmail.com'],
 'facebook': 'https://www.facebook.com/HerculesTreeLLC/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Scraped from <200 https://herculestree.com/contact>
{'emails': ['Herculestree@gmail.com', 'HerculesTree@gmail.com'],
 'facebook': 'https://www.facebook.com/HerculesTreeLLC/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Redirecting (301) to <GET https://herculestree.com/contact/request-quote/> from <GET https://herculestree.com/contact/request-quote>
DEBUG: Crawled (200) <GET https://treeservicenow.net/contact_us_1> (referer: https://treeservicenow.net/)
DEBUG: Scraped from <200 https://treeservicenow.net/contact_us_1>
{'emails': [], 'facebook': '', 'instagram': '', 'linkedin': '', 'twitter': ''}
DEBUG: Crawled (200) <GET https://herculestree.com/contact/request-quote/> (referer: https://herculestree.com/)
DEBUG: Crawled (200) <GET https://treeservicecolumbusohio.net/contact-us/> (referer: https://treeservicecolumbusohio.net/)
DEBUG: Scraped from <200 https://herculestree.com/contact/request-quote/>
{'emails': ['info@herculesTrees.com', 'HerculesTree@gmail.com'],
 'facebook': 'https://www.facebook.com/HerculesTreeLLC/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Scraped from <200 https://treeservicecolumbusohio.net/contact-us/>
{'emails': [], 'facebook': '', 'instagram': '', 'linkedin': '', 'twitter': ''}
INFO: Closing spider (finished)
INFO: Stored json feed (25 items) in: website_spider_output.json
INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 64905,
 'downloader/request_count': 246,
 'downloader/request_method_count/GET': 246,
 'downloader/response_bytes': 3495263,
 'downloader/response_count': 246,
 'downloader/response_status_count/200': 145,
 'downloader/response_status_count/301': 56,
 'downloader/response_status_count/302': 1,
 'downloader/response_status_count/403': 29,
 'downloader/response_status_count/404': 5,
 'downloader/response_status_count/429': 10,
 'dupefilter/filtered': 60,
 'elapsed_time_seconds': 14.443245,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 9, 10, 30, 45, 864570),
 'httpcompression/response_bytes': 14816381,
 'httpcompression/response_count': 156,
 'httperror/response_ignored_count': 16,
 'httperror/response_ignored_status_count/403': 14,
 'httperror/response_ignored_status_count/429': 2,
 'item_scraped_count': 25,
 'log_count/DEBUG': 476,
 'log_count/ERROR': 3,
 'log_count/INFO': 27,
 'request_depth_max': 1,
 'response_received_count': 181,
 'retry/count': 8,
 'retry/max_reached': 2,
 'retry/reason_count/429 Unknown Status': 8,
 'robotstxt/request_count': 84,
 'robotstxt/response_count': 84,
 'robotstxt/response_status_count/200': 64,
 'robotstxt/response_status_count/403': 15,
 'robotstxt/response_status_count/404': 5,
 'scheduler/dequeued': 138,
 'scheduler/dequeued/memory': 138,
 'scheduler/enqueued': 138,
 'scheduler/enqueued/memory': 138,
 'start_time': datetime.datetime(2022, 12, 9, 10, 30, 31, 421325)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: 13cb8b81a9ecd7cd
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
DEBUG: Redirecting (301) to <GET https://www.fandftrees.com/robots.txt> from <GET http://www.fandftrees.com/robots.txt>
DEBUG: Crawled (403) <GET https://trapperstreeservice.com/robots.txt> (referer: None)
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Rule at line 14 without any user agent to enforce it on.
DEBUG: Rule at line 16 without any user agent to enforce it on.
DEBUG: Rule at line 35 without any user agent to enforce it on.
DEBUG: Rule at line 42 without any user agent to enforce it on.
DEBUG: Rule at line 43 without any user agent to enforce it on.
DEBUG: Rule at line 44 without any user agent to enforce it on.
DEBUG: Rule at line 45 without any user agent to enforce it on.
DEBUG: Rule at line 46 without any user agent to enforce it on.
DEBUG: Rule at line 47 without any user agent to enforce it on.
DEBUG: Rule at line 69 without any user agent to enforce it on.
DEBUG: Attempting to acquire lock 2251446444736 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2251446444736 acquired on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Attempting to release lock 2251446444736 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2251446444736 released on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Crawled (200) <GET http://www.columbustreeservicesllc.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET https://trapperstreeservice.com/> (referer: None)
DEBUG: Crawled (403) <GET https://www.midohiotree.org/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.treetechohio.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.fandftrees.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.fandftrees.com/> from <GET http://www.fandftrees.com/>
INFO: Ignoring response <403 https://trapperstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET http://www.charteroakscompany.com/robots.txt> (referer: None)
DEBUG: Filtered duplicate request: <GET http://hardwicktreecare.com/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG: Crawled (200) <GET https://www.fandftrees.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.treetechohio.com/> (referer: None)
DEBUG: Crawled (403) <GET https://www.midohiotree.org/> (referer: None)
DEBUG: Crawled (200) <GET http://www.columbustreeservicesllc.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://starwoodtree.com/robots.txt> from <GET http://starwoodtree.com/robots.txt>
INFO: Ignoring response <403 https://www.midohiotree.org/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET http://hardwicktreecare.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.hardwicktreecare.com/> from <GET http://hardwicktreecare.com/>
DEBUG: Crawled (404) <GET http://ohiotreeandexcavating.net/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.russelltreeexperts.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.treetechohio.com/contact> (referer: https://www.treetechohio.com/)
DEBUG: Crawled (200) <GET https://www.hardwicktreecare.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.hardwicktreecare.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.russelltreeexperts.com/> (referer: None)
DEBUG: Redirecting (301) to <GET http://ohiotreeandexcavating.com/> from <GET http://ohiotreeandexcavating.net/>
DEBUG: Crawled (200) <GET http://www.treesaremybusiness.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://ohiotreeandexcavating.com/robots.txt> from <GET http://ohiotreeandexcavating.com/robots.txt>
DEBUG: Crawled (403) <GET https://www.tackettstreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://ohiotreeandexcavating.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.treesaremybusiness.com/> from <GET http://www.treesaremybusiness.com/>
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/akron/?utm_source=GMB&utm_medium=organic&utm_campaign=doylestown> (referer: None)
DEBUG: Redirecting (301) to <GET https://ohiotreeandexcavating.com/> from <GET http://ohiotreeandexcavating.com/>
DEBUG: Crawled (404) <GET http://ohiotreecare.com/robots.txt> (referer: None)
DEBUG: Rule at line 1 without any user agent to enforce it on.
DEBUG: Rule at line 2 without any user agent to enforce it on.
DEBUG: Rule at line 8 without any user agent to enforce it on.
DEBUG: Rule at line 9 without any user agent to enforce it on.
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Rule at line 11 without any user agent to enforce it on.
DEBUG: Rule at line 12 without any user agent to enforce it on.
DEBUG: Crawled (403) <GET https://www.tackettstreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.hardwicktreecare.com/contact-hardwick-tree-care> (referer: https://www.hardwicktreecare.com/)
DEBUG: Crawled (403) <GET http://www.hackettstreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://www.hackettstreeservice.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.charteroakscompany.com/> from <GET http://www.charteroakscompany.com/>
INFO: Ignoring response <403 https://www.tackettstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://specialtytreeohio.com/robots.txt> (referer: None)
INFO: Ignoring response <403 http://www.hackettstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (403) <GET http://deeprootedtreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://www.timberlandtreeohio.com/robots.txt> from <GET http://timberlandtreeohio.com/robots.txt>
DEBUG: Crawled (403) <GET http://deeprootedtreeservice.com/> (referer: None)
INFO: Ignoring response <403 http://deeprootedtreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://ohiotreeandexcavating.com/> (referer: None)
DEBUG: Crawled (200) <GET https://starwoodtree.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://treesaremybusiness.com/> from <GET https://www.treesaremybusiness.com/>
DEBUG: Crawled (200) <GET https://specialtytreeohio.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.charteroakscompany.com/> (referer: None)
DEBUG: Crawled (200) <GET http://www.timberlandtreeohio.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://starwoodtree.com/> from <GET http://starwoodtree.com/>
DEBUG: Crawled (200) <GET https://www.independenttree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.whymonster.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://www.timberlandtreeohio.com/> from <GET http://timberlandtreeohio.com/>
DEBUG: Redirecting (301) to <GET https://www.monstertreeservice.com/akron/contact-us/> from <GET https://www.whymonster.com/akron/contact-us/>
DEBUG: Redirecting (301) to <GET http://jstreeservicesllc.com/robots.txt> from <GET http://www.jstreeservicesllc.com/robots.txt>
DEBUG: Crawled (403) <GET http://cottstrees.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://cottstrees.com/> (referer: None)
DEBUG: Crawled (200) <GET http://www.timberlandtreeohio.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.ewsmithtree.com/robots.txt> (referer: None)
INFO: Ignoring response <403 http://cottstrees.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://treesaremybusiness.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/akron/contact-us/> (referer: https://www.monstertreeservice.com/akron/?utm_source=GMB&utm_medium=organic&utm_campaign=doylestown)
DEBUG: Crawled (200) <GET https://www.independenttree.com/?utm_source=gmb&utm_medium=local&utm_campaign=website> (referer: None)
DEBUG: Crawled (200) <GET http://ohiotreecare.com/> (referer: None)
DEBUG: Crawled (200) <GET http://jstreeservicesllc.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://roquetree.com/robots.txt> from <GET http://roquetree.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.ewsmithtree.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.basictreecare.com/robots.txt> from <GET http://www.basictreecare.com/robots.txt>
DEBUG: Crawled (200) <GET https://treesaremybusiness.com/> (referer: None)
DEBUG: Crawled (200) <GET http://www.kiddertreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://haneytreeservice.com/robots.txt> from <GET http://www.haneytreeservice.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://www.kiddertreemov.com/> from <GET http://www.kiddertreeservice.com/>
DEBUG: Crawled (200) <GET https://www.independenttree.com/contact-us/> (referer: https://www.independenttree.com/?utm_source=gmb&utm_medium=local&utm_campaign=website)
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/cleveland/?utm_source=GMB&utm_medium=organic&utm_campaign=AvonLake> (referer: None)
DEBUG: Redirecting (301) to <GET https://jstreeservicesllc.com/> from <GET http://www.jstreeservicesllc.com/>
DEBUG: Crawled (200) <GET https://www.kiddertreemov.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.basictreecare.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://treesaremybusiness.com/contact-us/> (referer: https://treesaremybusiness.com/)
DEBUG: Crawled (200) <GET https://roquetree.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.basictreecare.com/> from <GET http://www.basictreecare.com/>
DEBUG: Crawled (200) <GET http://haneytreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://www.timberlandtreeohio.com/> (referer: None)
DEBUG: Crawled (200) <GET https://challengerstreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://roquetree.com/> from <GET http://roquetree.com/>
DEBUG: Crawled (200) <GET https://www.kiddertreemov.com/> (referer: None)
DEBUG: Redirecting (301) to <GET http://haneytreeservice.com/> from <GET http://www.haneytreeservice.com/>
DEBUG: Crawled (200) <GET https://www.ewsmithtree.com/contact-us/> (referer: https://www.ewsmithtree.com/)
DEBUG: Redirecting (301) to <GET https://www.roquetree.com/> from <GET https://roquetree.com/>
DEBUG: Crawled (200) <GET https://www.basictreecare.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.jttreeservicellc.com/robots.txt> from <GET http://www.jttreeservicellc.com/robots.txt>
DEBUG: Crawled (403) <GET https://www.kandatreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://starwoodtree.com/> (referer: None)
DEBUG: Crawled (403) <GET https://www.kandatreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET http://haneytreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://jstreeservicesllc.com/robots.txt> (referer: None)
INFO: Ignoring response <403 https://www.kandatreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET http://haneytreeservice.com/index.html> from <GET http://haneytreeservice.com/>
DEBUG: Crawled (403) <GET http://www.jjsfamilytreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://www.jjsfamilytreeservice.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.ripleytreeservice.com/robots.txt> from <GET http://www.ripleytreeservice.com/robots.txt>
INFO: Ignoring response <403 http://www.jjsfamilytreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET http://haneytreeservice.com/index.html> (referer: None)
DEBUG: Crawled (200) <GET https://www.davey.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://extremetreeservicestoledo.com/robots.txt> from <GET https://www.extremetreeservicestoledo.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.roquetree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.jttreeservicellc.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.roquetree.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.jttreeservicellc.com/> from <GET http://www.jttreeservicellc.com/>
DEBUG: Crawled (200) <GET https://jstreeservicesllc.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.ripleytreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://parkstree.net/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://extremetreeservicestoledo.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.ripleytreeservice.com/> from <GET http://www.ripleytreeservice.com/>
DEBUG: Crawled (200) <GET https://www.jttreeservicellc.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.davey.com/residential-tree-services/local-offices/columbus-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Columbus> (referer: None)
DEBUG: Redirecting (301) to <GET https://extremetreeservicestoledo.com/> from <GET https://www.extremetreeservicestoledo.com/>
DEBUG: Crawled (200) <GET https://www.savatree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://parkstree.net/> (referer: None)
DEBUG: Crawled (200) <GET https://oak-tree-services-tree-service.business.site/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.rawtreeserv.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://extremetreeservicestoledo.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.ripleytreeservice.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://rawtreeserv.com/> from <GET https://www.rawtreeserv.com/>
DEBUG: Crawled (200) <GET https://parkstree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://oak-tree-services-tree-service.business.site/?utm_source=gmb&utm_medium=referral> (referer: None)
DEBUG: Crawled (200) <GET https://extremetreeservicestoledo.com/> (referer: None)
DEBUG: Crawled (200) <GET https://challengerstreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://parkstree.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.davey.com/residential-tree-services/local-offices/cincinnati-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Cincinnati> (referer: None)
DEBUG: Crawled (200) <GET https://starwoodtree.com/contact-us/> (referer: https://starwoodtree.com/)
DEBUG: Crawled (200) <GET https://www.savatree.com/dayton-ohio-tree-service-lawn-care?utm_source=GMB&utm_medium=organic&utm_campaign=dayton> (referer: None)
DEBUG: Crawled (200) <GET https://extremetreeservicestoledo.com/contact/> (referer: https://extremetreeservicestoledo.com/)
DEBUG: Redirecting (301) to <GET http://jstreeservicesllc.com/contact-us/> from <GET http://www.jstreeservicesllc.com/contact-us>
DEBUG: Crawled (200) <GET https://jstreeservicesllc.com/contact-us/> (referer: https://jstreeservicesllc.com/)
DEBUG: Crawled (200) <GET http://blackstreemov.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://rawtreeserv.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.blackstreemov.com/> from <GET http://blackstreemov.com/>
DEBUG: Crawled (403) <GET http://www.haymakertreeandlawn.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://www.haymakertreeandlawn.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.davey.com/residential-tree-services/local-offices/northeast-cleveland-tree-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Northeast%20Cleveland> (referer: None)
DEBUG: Crawled (200) <GET https://www.stevestoledotree.com/robots.txt> (referer: None)
INFO: Ignoring response <403 http://www.haymakertreeandlawn.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://affordabletreeservicesofohiollc.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.blackstreemov.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://affordabletreeservicesofohiollc.com/> (referer: None)
DEBUG: Crawled (200) <GET http://jstreeservicesllc.com/contact-us/> (referer: None)
DEBUG: Crawled (403) <GET http://www.cstreemulch.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.blackstreemov.com/> (referer: None)
DEBUG: Crawled (200) <GET https://rawtreeserv.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.stevestoledotree.com/?utm_source=googlelocal&utm_medium=organic> (referer: None)
DEBUG: Crawled (200) <GET https://treesrus-treeservice.business.site/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://www.cstreemulch.com/> (referer: None)
INFO: Ignoring response <403 http://www.cstreemulch.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (403) <GET http://www.ashtreeservicepro.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://www.ashtreeservicepro.com/> (referer: None)
DEBUG: Crawled (200) <GET https://treesrus-treeservice.business.site/?utm_source=gmb&utm_medium=referral> (referer: None)
DEBUG: Crawled (200) <GET https://www.blackstreemov.com/contact> (referer: https://www.blackstreemov.com/)
DEBUG: Redirecting (301) to <GET https://www.roguetreesolutions.com/robots.txt> from <GET http://www.roguetreesolutions.com/robots.txt>
INFO: Ignoring response <403 http://www.ashtreeservicepro.com/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET http://www.toddstreeservice.com/robots.txt> from <GET http://toddstreeservice.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://hoffmantreeservice.com/robots.txt> from <GET http://hoffmantreeservice.com/robots.txt>
DEBUG: Crawled (200) <GET https://extremetreeohio.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://extremetreeohio.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.davey.com/about/contact-us/?type=nonservicerequest> (referer: https://www.davey.com/residential-tree-services/local-offices/columbus-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Columbus)
DEBUG: Crawled (200) <GET http://urbanloggersohio.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://hoffmantreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://urbanloggersohio.com/> from <GET http://urbanloggersohio.com/>
DEBUG: Crawled (403) <GET http://www.toddstreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (404) <GET https://www.roguetreesolutions.com/robots.txt> (referer: None)
DEBUG: Rule at line 1 without any user agent to enforce it on.
DEBUG: Rule at line 2 without any user agent to enforce it on.
DEBUG: Rule at line 14 without any user agent to enforce it on.
DEBUG: Rule at line 15 without any user agent to enforce it on.
DEBUG: Rule at line 23 without any user agent to enforce it on.
DEBUG: Rule at line 28 without any user agent to enforce it on.
DEBUG: Rule at line 29 without any user agent to enforce it on.
DEBUG: Rule at line 32 without any user agent to enforce it on.
DEBUG: Rule at line 33 without any user agent to enforce it on.
DEBUG: Rule at line 45 without any user agent to enforce it on.
DEBUG: Rule at line 46 without any user agent to enforce it on.
DEBUG: Rule at line 47 without any user agent to enforce it on.
DEBUG: Rule at line 48 without any user agent to enforce it on.
DEBUG: Rule at line 49 without any user agent to enforce it on.
DEBUG: Rule at line 50 without any user agent to enforce it on.
DEBUG: Rule at line 51 without any user agent to enforce it on.
DEBUG: Rule at line 52 without any user agent to enforce it on.
DEBUG: Rule at line 53 without any user agent to enforce it on.
DEBUG: Rule at line 54 without any user agent to enforce it on.
DEBUG: Rule at line 55 without any user agent to enforce it on.
DEBUG: Rule at line 57 without any user agent to enforce it on.
DEBUG: Rule at line 58 without any user agent to enforce it on.
DEBUG: Rule at line 59 without any user agent to enforce it on.
DEBUG: Rule at line 61 without any user agent to enforce it on.
DEBUG: Rule at line 62 without any user agent to enforce it on.
DEBUG: Rule at line 63 without any user agent to enforce it on.
DEBUG: Rule at line 64 without any user agent to enforce it on.
DEBUG: Rule at line 65 without any user agent to enforce it on.
DEBUG: Rule at line 66 without any user agent to enforce it on.
DEBUG: Rule at line 67 without any user agent to enforce it on.
DEBUG: Rule at line 70 without any user agent to enforce it on.
DEBUG: Rule at line 72 without any user agent to enforce it on.
DEBUG: Rule at line 78 without any user agent to enforce it on.
DEBUG: Rule at line 79 without any user agent to enforce it on.
DEBUG: Rule at line 80 without any user agent to enforce it on.
DEBUG: Rule at line 81 without any user agent to enforce it on.
DEBUG: Rule at line 87 without any user agent to enforce it on.
DEBUG: Rule at line 88 without any user agent to enforce it on.
DEBUG: Rule at line 92 without any user agent to enforce it on.
DEBUG: Rule at line 94 without any user agent to enforce it on.
DEBUG: Rule at line 96 without any user agent to enforce it on.
DEBUG: Rule at line 98 without any user agent to enforce it on.
DEBUG: Rule at line 100 without any user agent to enforce it on.
DEBUG: Rule at line 101 without any user agent to enforce it on.
DEBUG: Rule at line 104 without any user agent to enforce it on.
DEBUG: Rule at line 106 without any user agent to enforce it on.
DEBUG: Rule at line 107 without any user agent to enforce it on.
DEBUG: Rule at line 108 without any user agent to enforce it on.
DEBUG: Rule at line 109 without any user agent to enforce it on.
DEBUG: Rule at line 110 without any user agent to enforce it on.
DEBUG: Rule at line 111 without any user agent to enforce it on.
DEBUG: Rule at line 112 without any user agent to enforce it on.
DEBUG: Rule at line 119 without any user agent to enforce it on.
DEBUG: Rule at line 120 without any user agent to enforce it on.
DEBUG: Rule at line 121 without any user agent to enforce it on.
DEBUG: Rule at line 122 without any user agent to enforce it on.
DEBUG: Rule at line 123 without any user agent to enforce it on.
DEBUG: Rule at line 128 without any user agent to enforce it on.
DEBUG: Rule at line 145 without any user agent to enforce it on.
DEBUG: Rule at line 155 without any user agent to enforce it on.
DEBUG: Rule at line 165 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET http://herculestree.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://hoffmantreeservice.com/> from <GET http://hoffmantreeservice.com/>
DEBUG: Redirecting (301) to <GET https://www.roguetreesolutions.com/> from <GET http://www.roguetreesolutions.com/>
DEBUG: Crawled (200) <GET http://rstreeservicellc.com/robots.txt> (referer: None)
DEBUG: Rule at line 1 without any user agent to enforce it on.
DEBUG: Rule at line 5 without any user agent to enforce it on.
DEBUG: Rule at line 9 without any user agent to enforce it on.
DEBUG: Rule at line 14 without any user agent to enforce it on.
DEBUG: Rule at line 19 without any user agent to enforce it on.
DEBUG: Rule at line 21 without any user agent to enforce it on.
DEBUG: Rule at line 22 without any user agent to enforce it on.
DEBUG: Rule at line 23 without any user agent to enforce it on.
DEBUG: Rule at line 26 without any user agent to enforce it on.
DEBUG: Rule at line 28 without any user agent to enforce it on.
DEBUG: Rule at line 29 without any user agent to enforce it on.
DEBUG: Rule at line 31 without any user agent to enforce it on.
DEBUG: Rule at line 32 without any user agent to enforce it on.
DEBUG: Rule at line 33 without any user agent to enforce it on.
DEBUG: Rule at line 34 without any user agent to enforce it on.
DEBUG: Rule at line 35 without any user agent to enforce it on.
DEBUG: Rule at line 36 without any user agent to enforce it on.
DEBUG: Rule at line 37 without any user agent to enforce it on.
DEBUG: Rule at line 39 without any user agent to enforce it on.
DEBUG: Rule at line 40 without any user agent to enforce it on.
DEBUG: Rule at line 43 without any user agent to enforce it on.
DEBUG: Rule at line 44 without any user agent to enforce it on.
DEBUG: Rule at line 46 without any user agent to enforce it on.
DEBUG: Rule at line 48 without any user agent to enforce it on.
DEBUG: Rule at line 51 without any user agent to enforce it on.
DEBUG: Rule at line 53 without any user agent to enforce it on.
DEBUG: Rule at line 54 without any user agent to enforce it on.
DEBUG: Rule at line 55 without any user agent to enforce it on.
DEBUG: Rule at line 56 without any user agent to enforce it on.
DEBUG: Rule at line 57 without any user agent to enforce it on.
DEBUG: Rule at line 58 without any user agent to enforce it on.
DEBUG: Rule at line 60 without any user agent to enforce it on.
DEBUG: Rule at line 61 without any user agent to enforce it on.
DEBUG: Rule at line 62 without any user agent to enforce it on.
DEBUG: Rule at line 63 without any user agent to enforce it on.
DEBUG: Rule at line 64 without any user agent to enforce it on.
DEBUG: Rule at line 65 without any user agent to enforce it on.
DEBUG: Rule at line 66 without any user agent to enforce it on.
DEBUG: Rule at line 67 without any user agent to enforce it on.
DEBUG: Rule at line 68 without any user agent to enforce it on.
DEBUG: Rule at line 69 without any user agent to enforce it on.
DEBUG: Rule at line 70 without any user agent to enforce it on.
DEBUG: Rule at line 71 without any user agent to enforce it on.
DEBUG: Rule at line 72 without any user agent to enforce it on.
DEBUG: Rule at line 73 without any user agent to enforce it on.
DEBUG: Rule at line 74 without any user agent to enforce it on.
DEBUG: Rule at line 75 without any user agent to enforce it on.
DEBUG: Rule at line 76 without any user agent to enforce it on.
DEBUG: Rule at line 77 without any user agent to enforce it on.
DEBUG: Rule at line 78 without any user agent to enforce it on.
DEBUG: Rule at line 79 without any user agent to enforce it on.
DEBUG: Rule at line 80 without any user agent to enforce it on.
DEBUG: Rule at line 81 without any user agent to enforce it on.
DEBUG: Rule at line 83 without any user agent to enforce it on.
DEBUG: Rule at line 85 without any user agent to enforce it on.
DEBUG: Rule at line 86 without any user agent to enforce it on.
DEBUG: Rule at line 87 without any user agent to enforce it on.
DEBUG: Rule at line 88 without any user agent to enforce it on.
DEBUG: Rule at line 89 without any user agent to enforce it on.
DEBUG: Rule at line 90 without any user agent to enforce it on.
DEBUG: Rule at line 91 without any user agent to enforce it on.
DEBUG: Rule at line 92 without any user agent to enforce it on.
DEBUG: Rule at line 93 without any user agent to enforce it on.
DEBUG: Rule at line 94 without any user agent to enforce it on.
DEBUG: Rule at line 95 without any user agent to enforce it on.
DEBUG: Rule at line 96 without any user agent to enforce it on.
DEBUG: Rule at line 98 without any user agent to enforce it on.
DEBUG: Rule at line 99 without any user agent to enforce it on.
DEBUG: Rule at line 100 without any user agent to enforce it on.
DEBUG: Rule at line 101 without any user agent to enforce it on.
DEBUG: Rule at line 102 without any user agent to enforce it on.
DEBUG: Rule at line 103 without any user agent to enforce it on.
DEBUG: Rule at line 104 without any user agent to enforce it on.
DEBUG: Rule at line 106 without any user agent to enforce it on.
DEBUG: Rule at line 107 without any user agent to enforce it on.
DEBUG: Rule at line 108 without any user agent to enforce it on.
DEBUG: Rule at line 110 without any user agent to enforce it on.
DEBUG: Rule at line 112 without any user agent to enforce it on.
DEBUG: Rule at line 113 without any user agent to enforce it on.
DEBUG: Rule at line 114 without any user agent to enforce it on.
DEBUG: Rule at line 115 without any user agent to enforce it on.
DEBUG: Rule at line 116 without any user agent to enforce it on.
DEBUG: Rule at line 117 without any user agent to enforce it on.
DEBUG: Rule at line 121 without any user agent to enforce it on.
DEBUG: Rule at line 122 without any user agent to enforce it on.
DEBUG: Redirecting (301) to <GET http://www.toddstreeservice.com/> from <GET http://toddstreeservice.com/>
DEBUG: Crawled (200) <GET https://www.roguetreesolutions.com/> (referer: None)
DEBUG: Crawled (404) <GET http://www.boonestreeservice.com/robots.txt> (referer: None)
DEBUG: Rule at line 2 without any user agent to enforce it on.
DEBUG: Rule at line 8 without any user agent to enforce it on.
DEBUG: Rule at line 9 without any user agent to enforce it on.
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://hoffmantreeservice.com/> (referer: None)
DEBUG: Crawled (403) <GET http://www.toddstreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://rstreeservicellc.com/?utm_source=gmb&utm_medium=referral> (referer: None)
DEBUG: Crawled (200) <GET http://www.boonestreeservice.com/> (referer: None)
DEBUG: Crawled (403) <GET http://www.toddstreeservice.com/> (referer: None)
INFO: Ignoring response <403 http://www.toddstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET https://herculestree.com/> from <GET http://herculestree.com/>
DEBUG: Redirecting (302) to <GET https://www.greatdanetreeexperts.com/404.html> from <GET https://www.greatdanetreeexperts.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://www.treeservicenow.net/robots.txt> from <GET http://www.treeservicenow.net/robots.txt>
DEBUG: Crawled (200) <GET https://www.davey.com/about/contact-us/?type=residential> (referer: https://www.davey.com/residential-tree-services/local-offices/columbus-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Columbus)
DEBUG: Redirecting (301) to <GET https://tree-works-ohio.com/robots.txt> from <GET http://tree-works-ohio.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.greatdanetreeexperts.com/404.html> (referer: None)
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Rule at line 11 without any user agent to enforce it on.
DEBUG: Rule at line 12 without any user agent to enforce it on.
DEBUG: Rule at line 15 without any user agent to enforce it on.
DEBUG: Rule at line 17 without any user agent to enforce it on.
DEBUG: Rule at line 28 without any user agent to enforce it on.
DEBUG: Rule at line 43 without any user agent to enforce it on.
DEBUG: Rule at line 44 without any user agent to enforce it on.
DEBUG: Rule at line 45 without any user agent to enforce it on.
DEBUG: Rule at line 46 without any user agent to enforce it on.
DEBUG: Rule at line 47 without any user agent to enforce it on.
DEBUG: Rule at line 48 without any user agent to enforce it on.
DEBUG: Rule at line 49 without any user agent to enforce it on.
DEBUG: Rule at line 50 without any user agent to enforce it on.
DEBUG: Rule at line 54 without any user agent to enforce it on.
DEBUG: Rule at line 57 without any user agent to enforce it on.
DEBUG: Rule at line 62 without any user agent to enforce it on.
DEBUG: Rule at line 63 without any user agent to enforce it on.
DEBUG: Rule at line 67 without any user agent to enforce it on.
DEBUG: Rule at line 88 without any user agent to enforce it on.
DEBUG: Rule at line 136 without any user agent to enforce it on.
DEBUG: Rule at line 141 without any user agent to enforce it on.
DEBUG: Rule at line 144 without any user agent to enforce it on.
DEBUG: Rule at line 147 without any user agent to enforce it on.
DEBUG: Rule at line 156 without any user agent to enforce it on.
DEBUG: Rule at line 157 without any user agent to enforce it on.
DEBUG: Rule at line 191 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://urbanloggersohio.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.davey.com/about/contact-us/?type=nonresidential> (referer: https://www.davey.com/residential-tree-services/local-offices/columbus-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Columbus)
DEBUG: Crawled (200) <GET https://premiertreesllc.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.greatdanetreeexperts.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://barbertontree.com/robots.txt> from <GET https://www.barbertontree.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.davey.com/residential-tree-services/local-offices/akron-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Akron> (referer: None)
DEBUG: Crawled (200) <GET https://brushbandittree.com/robots.txt> (referer: None)
DEBUG: Retrying <GET https://premiertreesllc.com/> (failed 1 times): 429 Unknown Status
DEBUG: Retrying <GET https://premiertreesllc.com/> (failed 2 times): 429 Unknown Status
DEBUG: Redirecting (301) to <GET https://lamannatreeservice.com/robots.txt> from <GET http://www.lamannatreeservice.com/robots.txt>
DEBUG: Crawled (200) <GET https://larochetree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://urbanloggersohio.com/contact-urban-loggers/> (referer: https://urbanloggersohio.com/)
DEBUG: Redirecting (301) to <GET https://www.larochetree.com/> from <GET https://larochetree.com/>
DEBUG: Crawled (200) <GET https://herculestree.com/> (referer: None)
DEBUG: Crawled (200) <GET https://tree-works-ohio.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://premiertreesllc.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.larochetree.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://tomcotreecare.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.delmartreeservices.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://tomcotreecare.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.delmartreeservices.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://tree-works-ohio.com/> from <GET http://tree-works-ohio.com/>
DEBUG: Retrying <GET https://premiertreesllc.com/contact-us/> (failed 1 times): 429 Unknown Status
DEBUG: Crawled (200) <GET https://barbertontree.com/robots.txt> (referer: None)
DEBUG: Rule at line 3 without any user agent to enforce it on.
INFO: Ignoring response <403 http://tomcotreecare.com/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET https://barbertontree.com/> from <GET https://www.barbertontree.com/>
DEBUG: Redirecting (301) to <GET https://lamannatreeservice.com/?robots=1> from <GET https://lamannatreeservice.com/robots.txt>
DEBUG: Crawled (200) <GET https://brushbandittree.com/> (referer: None)
DEBUG: Retrying <GET https://premiertreesllc.com/contact-us/> (failed 2 times): 429 Unknown Status
DEBUG: Crawled (200) <GET https://www.larochetree.com/> (referer: None)
DEBUG: Crawled (403) <GET http://www.dolcestreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://herculestree.com/contact/> from <GET https://herculestree.com/contact>
DEBUG: Crawled (403) <GET http://www.dolcestreeservice.com/> (referer: None)
DEBUG: Retrying <GET https://barbertontree.com/robots.txt> (failed 1 times): 429 Unknown Status
ERROR: Gave up retrying <GET https://premiertreesllc.com/contact-us/> (failed 3 times): 429 Unknown Status
DEBUG: Crawled (429) <GET https://premiertreesllc.com/contact-us/> (referer: https://premiertreesllc.com/)
INFO: Ignoring response <403 http://www.dolcestreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://lamannatreeservice.com/?robots=1> (referer: None)
DEBUG: Crawled (200) <GET https://barbertontree.com/robots.txt> (referer: None)
DEBUG: Rule at line 3 without any user agent to enforce it on.
INFO: Ignoring response <429 https://premiertreesllc.com/contact-us/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET https://lamannatreeservice.com/> from <GET http://www.lamannatreeservice.com/>
DEBUG: Redirecting (301) to <GET https://lamannatreeservice.com/?robots=1> from <GET https://lamannatreeservice.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.larochetree.com/contact> (referer: https://www.larochetree.com/)
DEBUG: Retrying <GET https://barbertontree.com/> (failed 1 times): 429 Unknown Status
DEBUG: Crawled (200) <GET https://lamannatreeservice.com/?robots=1> (referer: None)
DEBUG: Crawled (200) <GET https://tree-works-ohio.com/> (referer: None)
ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\engine.py", line 152, in _next_request
    request = next(self.slot.start_requests)
  File "C:\Users\moffi\PycharmProjects\scraping_web\scraper\scraper\spiders\website_spider.py", line 40, in start_requests
    yield scrapy.Request(url=url, callback=self.parse_website)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\http\request\__init__.py", line 60, in __init__
    self._set_url(url)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\http\request\__init__.py", line 108, in _set_url
    raise ValueError(f'Missing scheme in request url: {self._url}')
ValueError: Missing scheme in request url: 
DEBUG: Crawled (404) <GET https://www.napierandson.com/robots.txt> (referer: None)
DEBUG: Retrying <GET https://barbertontree.com/> (failed 2 times): 429 Unknown Status
DEBUG: Crawled (200) <GET https://brushbandittree.com/contact-us/> (referer: https://brushbandittree.com/)
ERROR: Gave up retrying <GET https://barbertontree.com/> (failed 3 times): 429 Unknown Status
DEBUG: Crawled (429) <GET https://barbertontree.com/> (referer: None)
DEBUG: Crawled (200) <GET https://treeservicecolumbusohio.net/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.napierandson.com/> (referer: None)
INFO: Ignoring response <429 https://barbertontree.com/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET https://herculestree.com/contact/request-quote/> from <GET https://herculestree.com/contact/request-quote>
DEBUG: Crawled (200) <GET https://lamannatreeservice.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://treeservicenow.net/robots.txt> from <GET https://www.treeservicenow.net/robots.txt>
DEBUG: Crawled (200) <GET https://treeservicecolumbusohio.net/> (referer: None)
DEBUG: Crawled (200) <GET https://tree-works-ohio.com/contact-tree-works-for-all-your-emergency-services-tree-removal-stump-grinding-etc/> (referer: https://tree-works-ohio.com/)
DEBUG: Crawled (200) <GET http://www.treeservicedelawareoh.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://treeservicecolumbusohio.net/contact-us/> (referer: https://treeservicecolumbusohio.net/)
DEBUG: Crawled (200) <GET http://www.treeservicedelawareoh.com/> (referer: None)
DEBUG: Crawled (200) <GET https://treeservicenow.net/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://lamannatreeservice.com/contact/> (referer: https://lamannatreeservice.com/)
DEBUG: Redirecting (301) to <GET https://www.treeservicenow.net/> from <GET http://www.treeservicenow.net/>
DEBUG: Crawled (200) <GET http://www.treeservicedelawareoh.com/contact-us/> (referer: http://www.treeservicedelawareoh.com/)
DEBUG: Redirecting (301) to <GET https://treeservicenow.net/> from <GET https://www.treeservicenow.net/>
DEBUG: Crawled (200) <GET https://treeservicenow.net/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://treeservicenow.net/> (referer: None)
DEBUG: Crawled (200) <GET https://treeservicenow.net/contact_us_1> (referer: https://treeservicenow.net/)
DEBUG: Crawled (200) <GET https://herculestree.com/contact/> (referer: https://herculestree.com/)
DEBUG: Crawled (200) <GET https://herculestree.com/contact/request-quote/> (referer: https://herculestree.com/)
INFO: Closing spider (finished)
INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 64674,
 'downloader/request_count': 245,
 'downloader/request_method_count/GET': 245,
 'downloader/response_bytes': 3481509,
 'downloader/response_count': 245,
 'downloader/response_status_count/200': 144,
 'downloader/response_status_count/301': 57,
 'downloader/response_status_count/302': 1,
 'downloader/response_status_count/403': 29,
 'downloader/response_status_count/404': 5,
 'downloader/response_status_count/429': 9,
 'dupefilter/filtered': 61,
 'elapsed_time_seconds': 17.912499,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 9, 12, 23, 35, 562524),
 'httpcompression/response_bytes': 14785681,
 'httpcompression/response_count': 155,
 'httperror/response_ignored_count': 16,
 'httperror/response_ignored_status_count/403': 14,
 'httperror/response_ignored_status_count/429': 2,
 'log_count/DEBUG': 450,
 'log_count/ERROR': 3,
 'log_count/INFO': 26,
 'request_depth_max': 1,
 'response_received_count': 180,
 'retry/count': 7,
 'retry/max_reached': 2,
 'retry/reason_count/429 Unknown Status': 7,
 'robotstxt/request_count': 84,
 'robotstxt/response_count': 84,
 'robotstxt/response_status_count/200': 64,
 'robotstxt/response_status_count/403': 15,
 'robotstxt/response_status_count/404': 5,
 'scheduler/dequeued': 138,
 'scheduler/dequeued/memory': 138,
 'scheduler/enqueued': 138,
 'scheduler/enqueued/memory': 138,
 'start_time': datetime.datetime(2022, 12, 9, 12, 23, 17, 650025)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: 181c3c7ded670f7f
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
DEBUG: Attempting to acquire lock 2731534273888 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2731534273888 acquired on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Attempting to release lock 2731534273888 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2731534273888 released on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Crawled (200) <GET http://www.columbustreeservicesllc.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.fandftrees.com/robots.txt> from <GET http://www.fandftrees.com/robots.txt>
DEBUG: Crawled (403) <GET https://trapperstreeservice.com/robots.txt> (referer: None)
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Rule at line 14 without any user agent to enforce it on.
DEBUG: Rule at line 16 without any user agent to enforce it on.
DEBUG: Rule at line 35 without any user agent to enforce it on.
DEBUG: Rule at line 42 without any user agent to enforce it on.
DEBUG: Rule at line 43 without any user agent to enforce it on.
DEBUG: Rule at line 44 without any user agent to enforce it on.
DEBUG: Rule at line 45 without any user agent to enforce it on.
DEBUG: Rule at line 46 without any user agent to enforce it on.
DEBUG: Rule at line 47 without any user agent to enforce it on.
DEBUG: Rule at line 69 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://www.treetechohio.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET https://www.midohiotree.org/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET https://trapperstreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET http://www.columbustreeservicesllc.com/> (referer: None)
DEBUG: Crawled (404) <GET http://ohiotreeandexcavating.net/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.fandftrees.com/robots.txt> (referer: None)
DEBUG: Filtered duplicate request: <GET http://hardwicktreecare.com/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG: Crawled (200) <GET https://www.treetechohio.com/> (referer: None)
DEBUG: Crawled (200) <GET http://www.charteroakscompany.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET https://www.midohiotree.org/> (referer: None)
INFO: Ignoring response <403 https://trapperstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET https://www.fandftrees.com/> from <GET http://www.fandftrees.com/>
DEBUG: Crawled (200) <GET http://hardwicktreecare.com/robots.txt> (referer: None)
INFO: Ignoring response <403 https://www.midohiotree.org/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://www.fandftrees.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.hardwicktreecare.com/> from <GET http://hardwicktreecare.com/>
DEBUG: Redirecting (301) to <GET https://starwoodtree.com/robots.txt> from <GET http://starwoodtree.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.treetechohio.com/contact> (referer: https://www.treetechohio.com/)
DEBUG: Crawled (200) <GET https://www.hardwicktreecare.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://ohiotreeandexcavating.com/> from <GET http://ohiotreeandexcavating.net/>
DEBUG: Scraped from <200 https://www.treetechohio.com/contact>
{'emails': ['831126cb46b74583bf6f72c5061cba9d@sentry-viewer.wixpress.com',
            '605a7baede844d278b89dc95ae0a9123@sentry-next.wixpress.com',
            '8eb368c655b84e029ed79ad7a5c1718e@sentry.wixpress.com',
            'info@mysite.com'],
 'facebook': 'http://www.facebook.com/wix',
 'instagram': '',
 'linkedin': '',
 'twitter': 'http://www.twitter.com/wix'}
DEBUG: Crawled (200) <GET https://www.hardwicktreecare.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.russelltreeexperts.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://www.treesaremybusiness.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://ohiotreeandexcavating.com/robots.txt> from <GET http://ohiotreeandexcavating.com/robots.txt>
DEBUG: Crawled (403) <GET https://www.tackettstreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.hardwicktreecare.com/contact-hardwick-tree-care> (referer: https://www.hardwicktreecare.com/)
DEBUG: Crawled (200) <GET https://ohiotreeandexcavating.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.treesaremybusiness.com/> from <GET http://www.treesaremybusiness.com/>
DEBUG: Crawled (200) <GET http://herculestree.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://ohiotreeandexcavating.com/> from <GET http://ohiotreeandexcavating.com/>
DEBUG: Crawled (403) <GET https://www.tackettstreeservice.com/> (referer: None)
DEBUG: Crawled (404) <GET http://ohiotreecare.com/robots.txt> (referer: None)
DEBUG: Rule at line 1 without any user agent to enforce it on.
DEBUG: Rule at line 2 without any user agent to enforce it on.
DEBUG: Rule at line 8 without any user agent to enforce it on.
DEBUG: Rule at line 9 without any user agent to enforce it on.
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Rule at line 11 without any user agent to enforce it on.
DEBUG: Rule at line 12 without any user agent to enforce it on.
DEBUG: Crawled (403) <GET http://www.hackettstreeservice.com/robots.txt> (referer: None)
DEBUG: Scraped from <200 https://www.hardwicktreecare.com/contact-hardwick-tree-care>
{'emails': ['831126cb46b74583bf6f72c5061cba9d@sentry-viewer.wixpress.com',
            'info@HardwickTreeCare.com',
            '605a7baede844d278b89dc95ae0a9123@sentry-next.wixpress.com',
            '8eb368c655b84e029ed79ad7a5c1718e@sentry.wixpress.com',
            'f1ffc0b5efe04e9eb9762cd808722520@sentry.wixpress.com'],
 'facebook': 'https://www.facebook.com/hardwicktreecarellc',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://www.russelltreeexperts.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/akron/?utm_source=GMB&utm_medium=organic&utm_campaign=doylestown> (referer: None)
DEBUG: Crawled (403) <GET http://www.hackettstreeservice.com/> (referer: None)
INFO: Ignoring response <403 https://www.tackettstreeservice.com/>: HTTP status code is not handled or not allowed
INFO: Ignoring response <403 http://www.hackettstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://www.independenttree.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://deeprootedtreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://www.timberlandtreeohio.com/robots.txt> from <GET http://timberlandtreeohio.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://herculestree.com/> from <GET http://herculestree.com/>
DEBUG: Crawled (403) <GET http://deeprootedtreeservice.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.charteroakscompany.com/> from <GET http://www.charteroakscompany.com/>
DEBUG: Crawled (200) <GET https://specialtytreeohio.com/robots.txt> (referer: None)
INFO: Ignoring response <403 http://deeprootedtreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET https://treesaremybusiness.com/> from <GET https://www.treesaremybusiness.com/>
DEBUG: Crawled (200) <GET https://ohiotreeandexcavating.com/> (referer: None)
DEBUG: Redirecting (301) to <GET http://jstreeservicesllc.com/robots.txt> from <GET http://www.jstreeservicesllc.com/robots.txt>
DEBUG: Crawled (200) <GET http://www.timberlandtreeohio.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.independenttree.com/?utm_source=gmb&utm_medium=local&utm_campaign=website> (referer: None)
DEBUG: Crawled (200) <GET https://www.whymonster.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://specialtytreeohio.com/> (referer: None)
DEBUG: Redirecting (301) to <GET http://www.timberlandtreeohio.com/> from <GET http://timberlandtreeohio.com/>
DEBUG: Crawled (200) <GET https://starwoodtree.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.monstertreeservice.com/akron/contact-us/> from <GET https://www.whymonster.com/akron/contact-us/>
DEBUG: Crawled (200) <GET https://www.charteroakscompany.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.ewsmithtree.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://cottstrees.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://starwoodtree.com/> from <GET http://starwoodtree.com/>
DEBUG: Crawled (200) <GET http://www.timberlandtreeohio.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://herculestree.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/akron/contact-us/> (referer: https://www.monstertreeservice.com/akron/?utm_source=GMB&utm_medium=organic&utm_campaign=doylestown)
DEBUG: Crawled (403) <GET http://cottstrees.com/> (referer: None)
DEBUG: Crawled (200) <GET http://jstreeservicesllc.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://treesaremybusiness.com/robots.txt> (referer: None)
DEBUG: Scraped from <200 https://www.monstertreeservice.com/akron/contact-us/>
{'emails': [],
 'facebook': 'https://www.facebook.com/MonsterTreeServiceofAkron/',
 'instagram': 'https://www.instagram.com/monstertreeservices/',
 'linkedin': '',
 'twitter': ''}
INFO: Ignoring response <403 http://cottstrees.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://www.independenttree.com/contact-us/> (referer: https://www.independenttree.com/?utm_source=gmb&utm_medium=local&utm_campaign=website)
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/cleveland/?utm_source=GMB&utm_medium=organic&utm_campaign=AvonLake> (referer: None)
DEBUG: Redirecting (301) to <GET https://roquetree.com/robots.txt> from <GET http://roquetree.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.ewsmithtree.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.basictreecare.com/robots.txt> from <GET http://www.basictreecare.com/robots.txt>
DEBUG: Scraped from <200 https://www.independenttree.com/contact-us/>
{'emails': ['info@independenttree.com'],
 'facebook': 'https://www.facebook.com/IndependentTreeOH/',
 'instagram': 'https://www.instagram.com/independenttreeservice/',
 'linkedin': '',
 'twitter': ''}
DEBUG: Redirecting (301) to <GET http://haneytreeservice.com/robots.txt> from <GET http://www.haneytreeservice.com/robots.txt>
DEBUG: Crawled (200) <GET https://herculestree.com/contact> (referer: https://herculestree.com/)
DEBUG: Redirecting (301) to <GET https://jstreeservicesllc.com/> from <GET http://www.jstreeservicesllc.com/>
DEBUG: Crawled (200) <GET https://treesaremybusiness.com/> (referer: None)
DEBUG: Crawled (200) <GET http://ohiotreecare.com/> (referer: None)
DEBUG: Scraped from <200 https://herculestree.com/contact>
{'emails': ['Herculestree@gmail.com', 'HerculesTree@gmail.com'],
 'facebook': 'https://www.facebook.com/HerculesTreeLLC/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://www.basictreecare.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://haneytreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://herculestree.com/contact/> (referer: https://herculestree.com/)
DEBUG: Crawled (200) <GET http://www.kiddertreeservice.com/robots.txt> (referer: None)
DEBUG: Scraped from <200 https://herculestree.com/contact/>
{'emails': ['Herculestree@gmail.com', 'HerculesTree@gmail.com'],
 'facebook': 'https://www.facebook.com/HerculesTreeLLC/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Redirecting (301) to <GET https://www.kiddertreemov.com/> from <GET http://www.kiddertreeservice.com/>
DEBUG: Redirecting (301) to <GET https://www.basictreecare.com/> from <GET http://www.basictreecare.com/>
DEBUG: Crawled (200) <GET https://www.ewsmithtree.com/contact-us/> (referer: https://www.ewsmithtree.com/)
DEBUG: Redirecting (301) to <GET http://haneytreeservice.com/> from <GET http://www.haneytreeservice.com/>
DEBUG: Crawled (200) <GET https://roquetree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://treesaremybusiness.com/contact-us/> (referer: https://treesaremybusiness.com/)
DEBUG: Scraped from <200 https://www.ewsmithtree.com/contact-us/>
{'emails': [], 'facebook': '', 'instagram': '', 'linkedin': '', 'twitter': ''}
DEBUG: Scraped from <200 https://treesaremybusiness.com/contact-us/>
{'emails': ['millcraft@treesaremybusiness.com',
            'office@treesaremybusiness.com'],
 'facebook': 'https://www.facebook.com/treesaremybusinessohio/',
 'instagram': 'https://www.instagram.com/trees_are_my_business/',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET http://haneytreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://herculestree.com/contact/request-quote> (referer: https://herculestree.com/)
DEBUG: Redirecting (301) to <GET https://roquetree.com/> from <GET http://roquetree.com/>
DEBUG: Crawled (200) <GET https://www.kiddertreemov.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://www.timberlandtreeohio.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.basictreecare.com/> (referer: None)
DEBUG: Crawled (200) <GET https://herculestree.com/contact/request-quote/> (referer: https://herculestree.com/)
DEBUG: Scraped from <200 https://herculestree.com/contact/request-quote>
{'emails': ['info@herculesTrees.com', 'HerculesTree@gmail.com'],
 'facebook': 'https://www.facebook.com/HerculesTreeLLC/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://jstreeservicesllc.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.jttreeservicellc.com/robots.txt> from <GET http://www.jttreeservicellc.com/robots.txt>
DEBUG: Redirecting (301) to <GET http://haneytreeservice.com/index.html> from <GET http://haneytreeservice.com/>
DEBUG: Crawled (403) <GET https://www.kandatreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET https://www.kandatreeservice.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.roquetree.com/> from <GET https://roquetree.com/>
DEBUG: Crawled (403) <GET http://www.jjsfamilytreeservice.com/robots.txt> (referer: None)
DEBUG: Scraped from <200 https://herculestree.com/contact/request-quote/>
{'emails': ['info@herculesTrees.com', 'HerculesTree@gmail.com'],
 'facebook': 'https://www.facebook.com/HerculesTreeLLC/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (403) <GET http://www.jjsfamilytreeservice.com/> (referer: None)
INFO: Ignoring response <403 https://www.kandatreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET https://www.ripleytreeservice.com/robots.txt> from <GET http://www.ripleytreeservice.com/robots.txt>
INFO: Ignoring response <403 http://www.jjsfamilytreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET http://haneytreeservice.com/index.html> (referer: None)
DEBUG: Crawled (200) <GET https://challengerstreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://extremetreeservicestoledo.com/robots.txt> from <GET https://www.extremetreeservicestoledo.com/robots.txt>
DEBUG: Crawled (200) <GET https://jstreeservicesllc.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.davey.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://starwoodtree.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.roquetree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://challengerstreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.jttreeservicellc.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.ripleytreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.rawtreeserv.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://parkstree.net/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.savatree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://oak-tree-services-tree-service.business.site/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://extremetreeservicestoledo.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://parkstree.net/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.ripleytreeservice.com/> from <GET http://www.ripleytreeservice.com/>
DEBUG: Redirecting (301) to <GET https://www.jttreeservicellc.com/> from <GET http://www.jttreeservicellc.com/>
DEBUG: Redirecting (301) to <GET https://rawtreeserv.com/> from <GET https://www.rawtreeserv.com/>
DEBUG: Crawled (200) <GET https://www.roquetree.com/> (referer: None)
DEBUG: Crawled (200) <GET https://parkstree.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://extremetreeservicestoledo.com/> from <GET https://www.extremetreeservicestoledo.com/>
DEBUG: Crawled (200) <GET https://parkstree.com/> (referer: None)
DEBUG: Crawled (200) <GET https://oak-tree-services-tree-service.business.site/?utm_source=gmb&utm_medium=referral> (referer: None)
DEBUG: Crawled (200) <GET https://www.savatree.com/dayton-ohio-tree-service-lawn-care?utm_source=GMB&utm_medium=organic&utm_campaign=dayton> (referer: None)
DEBUG: Crawled (200) <GET https://extremetreeservicestoledo.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.kiddertreemov.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.jttreeservicellc.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.davey.com/residential-tree-services/local-offices/columbus-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Columbus> (referer: None)
DEBUG: Crawled (200) <GET https://www.ripleytreeservice.com/> (referer: None)
DEBUG: Crawled (403) <GET http://www.haymakertreeandlawn.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://www.haymakertreeandlawn.com/> (referer: None)
DEBUG: Crawled (200) <GET https://extremetreeservicestoledo.com/> (referer: None)
DEBUG: Crawled (200) <GET https://rawtreeserv.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://blackstreemov.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://jstreeservicesllc.com/contact-us/> from <GET http://www.jstreeservicesllc.com/contact-us>
INFO: Ignoring response <403 http://www.haymakertreeandlawn.com/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET https://www.blackstreemov.com/> from <GET http://blackstreemov.com/>
DEBUG: Crawled (200) <GET https://affordabletreeservicesofohiollc.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://affordabletreeservicesofohiollc.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.stevestoledotree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.blackstreemov.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://jstreeservicesllc.com/contact-us/> (referer: https://jstreeservicesllc.com/)
DEBUG: Crawled (200) <GET https://rawtreeserv.com/> (referer: None)
DEBUG: Crawled (403) <GET http://www.cstreemulch.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.blackstreemov.com/> (referer: None)
DEBUG: Crawled (403) <GET http://www.cstreemulch.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.davey.com/residential-tree-services/local-offices/cincinnati-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Cincinnati> (referer: None)
DEBUG: Scraped from <200 https://jstreeservicesllc.com/contact-us/>
{'emails': ['j_s.tree@yahoo.com'],
 'facebook': 'http://www.facebook.com/jandstreeservices',
 'instagram': 'http://www.instagram.com/jandstreeservices',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://extremetreeservicestoledo.com/contact/> (referer: https://extremetreeservicestoledo.com/)
DEBUG: Crawled (403) <GET http://www.ashtreeservicepro.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://treesrus-treeservice.business.site/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://www.ashtreeservicepro.com/> (referer: None)
INFO: Ignoring response <403 http://www.cstreemulch.com/>: HTTP status code is not handled or not allowed
DEBUG: Scraped from <200 https://extremetreeservicestoledo.com/contact/>
{'emails': ['extremetreeswanton@gmail.com'],
 'facebook': 'https://www.facebook.com/ToledosExtremeTree/',
 'instagram': '',
 'linkedin': '',
 'twitter': 'https://twitter.com/SuperClimber101'}
DEBUG: Redirecting (301) to <GET https://www.roguetreesolutions.com/robots.txt> from <GET http://www.roguetreesolutions.com/robots.txt>
DEBUG: Crawled (200) <GET http://jstreeservicesllc.com/contact-us/> (referer: None)
INFO: Ignoring response <403 http://www.ashtreeservicepro.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://treesrus-treeservice.business.site/?utm_source=gmb&utm_medium=referral> (referer: None)
DEBUG: Crawled (200) <GET https://www.blackstreemov.com/contact> (referer: https://www.blackstreemov.com/)
DEBUG: Scraped from <200 http://jstreeservicesllc.com/contact-us/>
{'emails': ['j_s.tree@yahoo.com'],
 'facebook': 'http://www.facebook.com/jandstreeservices',
 'instagram': 'http://www.instagram.com/jandstreeservices',
 'linkedin': '',
 'twitter': ''}
DEBUG: Redirecting (301) to <GET http://www.toddstreeservice.com/robots.txt> from <GET http://toddstreeservice.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://hoffmantreeservice.com/robots.txt> from <GET http://hoffmantreeservice.com/robots.txt>
DEBUG: Scraped from <200 https://www.blackstreemov.com/contact>
{'emails': ['BlacksTreeService140@gmail.com',
            'blackstreeservice140@gmail.com',
            '831126cb46b74583bf6f72c5061cba9d@sentry-viewer.wixpress.com',
            '605a7baede844d278b89dc95ae0a9123@sentry-next.wixpress.com'],
 'facebook': 'https://www.facebook.com/pages/Blacks-Tree-Service/205784306214398',
 'instagram': 'http://instagram.com/wix',
 'linkedin': '',
 'twitter': 'https://twitter.com/BlacksTreeMOV'}
DEBUG: Crawled (200) <GET https://starwoodtree.com/contact-us/> (referer: https://starwoodtree.com/)
DEBUG: Crawled (200) <GET https://www.stevestoledotree.com/?utm_source=googlelocal&utm_medium=organic> (referer: None)
DEBUG: Scraped from <200 https://starwoodtree.com/contact-us/>
{'emails': ['support@starwoodtree.com'],
 'facebook': 'https://www.facebook.com/Starwoodtreeservice/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://extremetreeohio.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.davey.com/residential-tree-services/local-offices/northeast-cleveland-tree-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Northeast%20Cleveland> (referer: None)
DEBUG: Crawled (200) <GET http://urbanloggersohio.com/robots.txt> (referer: None)
DEBUG: Crawled (404) <GET https://www.roguetreesolutions.com/robots.txt> (referer: None)
DEBUG: Rule at line 1 without any user agent to enforce it on.
DEBUG: Rule at line 2 without any user agent to enforce it on.
DEBUG: Rule at line 14 without any user agent to enforce it on.
DEBUG: Rule at line 15 without any user agent to enforce it on.
DEBUG: Rule at line 23 without any user agent to enforce it on.
DEBUG: Rule at line 28 without any user agent to enforce it on.
DEBUG: Rule at line 29 without any user agent to enforce it on.
DEBUG: Rule at line 32 without any user agent to enforce it on.
DEBUG: Rule at line 33 without any user agent to enforce it on.
DEBUG: Rule at line 45 without any user agent to enforce it on.
DEBUG: Rule at line 46 without any user agent to enforce it on.
DEBUG: Rule at line 47 without any user agent to enforce it on.
DEBUG: Rule at line 48 without any user agent to enforce it on.
DEBUG: Rule at line 49 without any user agent to enforce it on.
DEBUG: Rule at line 50 without any user agent to enforce it on.
DEBUG: Rule at line 51 without any user agent to enforce it on.
DEBUG: Rule at line 52 without any user agent to enforce it on.
DEBUG: Rule at line 53 without any user agent to enforce it on.
DEBUG: Rule at line 54 without any user agent to enforce it on.
DEBUG: Rule at line 55 without any user agent to enforce it on.
DEBUG: Rule at line 57 without any user agent to enforce it on.
DEBUG: Rule at line 58 without any user agent to enforce it on.
DEBUG: Rule at line 59 without any user agent to enforce it on.
DEBUG: Rule at line 61 without any user agent to enforce it on.
DEBUG: Rule at line 62 without any user agent to enforce it on.
DEBUG: Rule at line 63 without any user agent to enforce it on.
DEBUG: Rule at line 64 without any user agent to enforce it on.
DEBUG: Rule at line 65 without any user agent to enforce it on.
DEBUG: Rule at line 66 without any user agent to enforce it on.
DEBUG: Rule at line 67 without any user agent to enforce it on.
DEBUG: Rule at line 70 without any user agent to enforce it on.
DEBUG: Rule at line 72 without any user agent to enforce it on.
DEBUG: Rule at line 78 without any user agent to enforce it on.
DEBUG: Rule at line 79 without any user agent to enforce it on.
DEBUG: Rule at line 80 without any user agent to enforce it on.
DEBUG: Rule at line 81 without any user agent to enforce it on.
DEBUG: Rule at line 87 without any user agent to enforce it on.
DEBUG: Rule at line 88 without any user agent to enforce it on.
DEBUG: Rule at line 92 without any user agent to enforce it on.
DEBUG: Rule at line 94 without any user agent to enforce it on.
DEBUG: Rule at line 96 without any user agent to enforce it on.
DEBUG: Rule at line 98 without any user agent to enforce it on.
DEBUG: Rule at line 100 without any user agent to enforce it on.
DEBUG: Rule at line 101 without any user agent to enforce it on.
DEBUG: Rule at line 104 without any user agent to enforce it on.
DEBUG: Rule at line 106 without any user agent to enforce it on.
DEBUG: Rule at line 107 without any user agent to enforce it on.
DEBUG: Rule at line 108 without any user agent to enforce it on.
DEBUG: Rule at line 109 without any user agent to enforce it on.
DEBUG: Rule at line 110 without any user agent to enforce it on.
DEBUG: Rule at line 111 without any user agent to enforce it on.
DEBUG: Rule at line 112 without any user agent to enforce it on.
DEBUG: Rule at line 119 without any user agent to enforce it on.
DEBUG: Rule at line 120 without any user agent to enforce it on.
DEBUG: Rule at line 121 without any user agent to enforce it on.
DEBUG: Rule at line 122 without any user agent to enforce it on.
DEBUG: Rule at line 123 without any user agent to enforce it on.
DEBUG: Rule at line 128 without any user agent to enforce it on.
DEBUG: Rule at line 145 without any user agent to enforce it on.
DEBUG: Rule at line 155 without any user agent to enforce it on.
DEBUG: Rule at line 165 without any user agent to enforce it on.
DEBUG: Redirecting (301) to <GET https://www.roguetreesolutions.com/> from <GET http://www.roguetreesolutions.com/>
DEBUG: Crawled (200) <GET https://extremetreeohio.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.roguetreesolutions.com/> (referer: None)
DEBUG: Crawled (200) <GET https://hoffmantreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://www.toddstreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://urbanloggersohio.com/> from <GET http://urbanloggersohio.com/>
DEBUG: Redirecting (301) to <GET https://hoffmantreeservice.com/> from <GET http://hoffmantreeservice.com/>
DEBUG: Redirecting (301) to <GET http://www.toddstreeservice.com/> from <GET http://toddstreeservice.com/>
DEBUG: Crawled (200) <GET http://rstreeservicellc.com/robots.txt> (referer: None)
DEBUG: Rule at line 1 without any user agent to enforce it on.
DEBUG: Rule at line 5 without any user agent to enforce it on.
DEBUG: Rule at line 9 without any user agent to enforce it on.
DEBUG: Rule at line 14 without any user agent to enforce it on.
DEBUG: Rule at line 19 without any user agent to enforce it on.
DEBUG: Rule at line 21 without any user agent to enforce it on.
DEBUG: Rule at line 22 without any user agent to enforce it on.
DEBUG: Rule at line 23 without any user agent to enforce it on.
DEBUG: Rule at line 26 without any user agent to enforce it on.
DEBUG: Rule at line 28 without any user agent to enforce it on.
DEBUG: Rule at line 29 without any user agent to enforce it on.
DEBUG: Rule at line 31 without any user agent to enforce it on.
DEBUG: Rule at line 32 without any user agent to enforce it on.
DEBUG: Rule at line 33 without any user agent to enforce it on.
DEBUG: Rule at line 34 without any user agent to enforce it on.
DEBUG: Rule at line 35 without any user agent to enforce it on.
DEBUG: Rule at line 36 without any user agent to enforce it on.
DEBUG: Rule at line 37 without any user agent to enforce it on.
DEBUG: Rule at line 39 without any user agent to enforce it on.
DEBUG: Rule at line 40 without any user agent to enforce it on.
DEBUG: Rule at line 43 without any user agent to enforce it on.
DEBUG: Rule at line 44 without any user agent to enforce it on.
DEBUG: Rule at line 46 without any user agent to enforce it on.
DEBUG: Rule at line 48 without any user agent to enforce it on.
DEBUG: Rule at line 51 without any user agent to enforce it on.
DEBUG: Rule at line 53 without any user agent to enforce it on.
DEBUG: Rule at line 54 without any user agent to enforce it on.
DEBUG: Rule at line 55 without any user agent to enforce it on.
DEBUG: Rule at line 56 without any user agent to enforce it on.
DEBUG: Rule at line 57 without any user agent to enforce it on.
DEBUG: Rule at line 58 without any user agent to enforce it on.
DEBUG: Rule at line 60 without any user agent to enforce it on.
DEBUG: Rule at line 61 without any user agent to enforce it on.
DEBUG: Rule at line 62 without any user agent to enforce it on.
DEBUG: Rule at line 63 without any user agent to enforce it on.
DEBUG: Rule at line 64 without any user agent to enforce it on.
DEBUG: Rule at line 65 without any user agent to enforce it on.
DEBUG: Rule at line 66 without any user agent to enforce it on.
DEBUG: Rule at line 67 without any user agent to enforce it on.
DEBUG: Rule at line 68 without any user agent to enforce it on.
DEBUG: Rule at line 69 without any user agent to enforce it on.
DEBUG: Rule at line 70 without any user agent to enforce it on.
DEBUG: Rule at line 71 without any user agent to enforce it on.
DEBUG: Rule at line 72 without any user agent to enforce it on.
DEBUG: Rule at line 73 without any user agent to enforce it on.
DEBUG: Rule at line 74 without any user agent to enforce it on.
DEBUG: Rule at line 75 without any user agent to enforce it on.
DEBUG: Rule at line 76 without any user agent to enforce it on.
DEBUG: Rule at line 77 without any user agent to enforce it on.
DEBUG: Rule at line 78 without any user agent to enforce it on.
DEBUG: Rule at line 79 without any user agent to enforce it on.
DEBUG: Rule at line 80 without any user agent to enforce it on.
DEBUG: Rule at line 81 without any user agent to enforce it on.
DEBUG: Rule at line 83 without any user agent to enforce it on.
DEBUG: Rule at line 85 without any user agent to enforce it on.
DEBUG: Rule at line 86 without any user agent to enforce it on.
DEBUG: Rule at line 87 without any user agent to enforce it on.
DEBUG: Rule at line 88 without any user agent to enforce it on.
DEBUG: Rule at line 89 without any user agent to enforce it on.
DEBUG: Rule at line 90 without any user agent to enforce it on.
DEBUG: Rule at line 91 without any user agent to enforce it on.
DEBUG: Rule at line 92 without any user agent to enforce it on.
DEBUG: Rule at line 93 without any user agent to enforce it on.
DEBUG: Rule at line 94 without any user agent to enforce it on.
DEBUG: Rule at line 95 without any user agent to enforce it on.
DEBUG: Rule at line 96 without any user agent to enforce it on.
DEBUG: Rule at line 98 without any user agent to enforce it on.
DEBUG: Rule at line 99 without any user agent to enforce it on.
DEBUG: Rule at line 100 without any user agent to enforce it on.
DEBUG: Rule at line 101 without any user agent to enforce it on.
DEBUG: Rule at line 102 without any user agent to enforce it on.
DEBUG: Rule at line 103 without any user agent to enforce it on.
DEBUG: Rule at line 104 without any user agent to enforce it on.
DEBUG: Rule at line 106 without any user agent to enforce it on.
DEBUG: Rule at line 107 without any user agent to enforce it on.
DEBUG: Rule at line 108 without any user agent to enforce it on.
DEBUG: Rule at line 110 without any user agent to enforce it on.
DEBUG: Rule at line 112 without any user agent to enforce it on.
DEBUG: Rule at line 113 without any user agent to enforce it on.
DEBUG: Rule at line 114 without any user agent to enforce it on.
DEBUG: Rule at line 115 without any user agent to enforce it on.
DEBUG: Rule at line 116 without any user agent to enforce it on.
DEBUG: Rule at line 117 without any user agent to enforce it on.
DEBUG: Rule at line 121 without any user agent to enforce it on.
DEBUG: Rule at line 122 without any user agent to enforce it on.
DEBUG: Crawled (404) <GET http://www.boonestreeservice.com/robots.txt> (referer: None)
DEBUG: Rule at line 2 without any user agent to enforce it on.
DEBUG: Rule at line 8 without any user agent to enforce it on.
DEBUG: Rule at line 9 without any user agent to enforce it on.
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://hoffmantreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET http://www.treeservicedelawareoh.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://www.toddstreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.davey.com/about/contact-us/?type=residential> (referer: https://www.davey.com/residential-tree-services/local-offices/columbus-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Columbus)
DEBUG: Redirecting (302) to <GET https://www.greatdanetreeexperts.com/404.html> from <GET https://www.greatdanetreeexperts.com/robots.txt>
DEBUG: Scraped from <200 https://www.davey.com/about/contact-us/?type=residential>
{'emails': [],
 'facebook': 'https://www.facebook.com/DaveyTree',
 'instagram': 'https://instagram.com/DaveyTree/',
 'linkedin': 'http://www.linkedin.com/company/the-davey-tree-expert-company',
 'twitter': 'https://twitter.com/DaveyTree'}
DEBUG: Crawled (403) <GET http://www.toddstreeservice.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://barbertontree.com/robots.txt> from <GET https://www.barbertontree.com/robots.txt>
DEBUG: Crawled (200) <GET http://www.boonestreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET http://rstreeservicellc.com/?utm_source=gmb&utm_medium=referral> (referer: None)
DEBUG: Crawled (200) <GET http://www.treeservicedelawareoh.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.greatdanetreeexperts.com/404.html> (referer: None)
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Rule at line 11 without any user agent to enforce it on.
DEBUG: Rule at line 12 without any user agent to enforce it on.
DEBUG: Rule at line 15 without any user agent to enforce it on.
DEBUG: Rule at line 17 without any user agent to enforce it on.
DEBUG: Rule at line 28 without any user agent to enforce it on.
DEBUG: Rule at line 43 without any user agent to enforce it on.
DEBUG: Rule at line 44 without any user agent to enforce it on.
DEBUG: Rule at line 45 without any user agent to enforce it on.
DEBUG: Rule at line 46 without any user agent to enforce it on.
DEBUG: Rule at line 47 without any user agent to enforce it on.
DEBUG: Rule at line 48 without any user agent to enforce it on.
DEBUG: Rule at line 49 without any user agent to enforce it on.
DEBUG: Rule at line 50 without any user agent to enforce it on.
DEBUG: Rule at line 54 without any user agent to enforce it on.
DEBUG: Rule at line 57 without any user agent to enforce it on.
DEBUG: Rule at line 62 without any user agent to enforce it on.
DEBUG: Rule at line 63 without any user agent to enforce it on.
DEBUG: Rule at line 67 without any user agent to enforce it on.
DEBUG: Rule at line 88 without any user agent to enforce it on.
DEBUG: Rule at line 136 without any user agent to enforce it on.
DEBUG: Rule at line 141 without any user agent to enforce it on.
DEBUG: Rule at line 144 without any user agent to enforce it on.
DEBUG: Rule at line 147 without any user agent to enforce it on.
DEBUG: Rule at line 156 without any user agent to enforce it on.
DEBUG: Rule at line 157 without any user agent to enforce it on.
DEBUG: Rule at line 191 without any user agent to enforce it on.
INFO: Ignoring response <403 http://www.toddstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://brushbandittree.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.treeservicenow.net/robots.txt> from <GET http://www.treeservicenow.net/robots.txt>
DEBUG: Redirecting (301) to <GET https://lamannatreeservice.com/robots.txt> from <GET http://www.lamannatreeservice.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.greatdanetreeexperts.com/> (referer: None)
DEBUG: Crawled (200) <GET https://barbertontree.com/robots.txt> (referer: None)
DEBUG: Rule at line 3 without any user agent to enforce it on.
DEBUG: Redirecting (301) to <GET https://lamannatreeservice.com/?robots=1> from <GET https://lamannatreeservice.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://barbertontree.com/> from <GET https://www.barbertontree.com/>
DEBUG: Crawled (200) <GET https://lamannatreeservice.com/?robots=1> (referer: None)
DEBUG: Redirecting (301) to <GET https://lamannatreeservice.com/> from <GET http://www.lamannatreeservice.com/>
DEBUG: Crawled (200) <GET https://premiertreesllc.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://urbanloggersohio.com/> (referer: None)
DEBUG: Crawled (200) <GET http://www.treeservicedelawareoh.com/contact-us/> (referer: http://www.treeservicedelawareoh.com/)
DEBUG: Redirecting (301) to <GET https://lamannatreeservice.com/?robots=1> from <GET https://lamannatreeservice.com/robots.txt>
DEBUG: Retrying <GET https://barbertontree.com/robots.txt> (failed 1 times): 429 Unknown Status
DEBUG: Crawled (200) <GET https://lamannatreeservice.com/?robots=1> (referer: None)
DEBUG: Redirecting (301) to <GET https://treeservicenow.net/robots.txt> from <GET https://www.treeservicenow.net/robots.txt>
DEBUG: Crawled (200) <GET https://larochetree.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://tree-works-ohio.com/robots.txt> from <GET http://tree-works-ohio.com/robots.txt>
DEBUG: Crawled (200) <GET https://lamannatreeservice.com/> (referer: None)
DEBUG: Scraped from <200 http://www.treeservicedelawareoh.com/contact-us/>
{'emails': [],
 'facebook': 'https://www.facebook.com/pages/biz/43015/James-Tree-Service/194987980849287/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Retrying <GET https://premiertreesllc.com/> (failed 1 times): 429 Unknown Status
DEBUG: Crawled (200) <GET https://www.davey.com/about/contact-us/?type=nonresidential> (referer: https://www.davey.com/residential-tree-services/local-offices/columbus-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Columbus)
DEBUG: Redirecting (301) to <GET https://www.larochetree.com/> from <GET https://larochetree.com/>
DEBUG: Retrying <GET https://barbertontree.com/robots.txt> (failed 2 times): 429 Unknown Status
DEBUG: Crawled (200) <GET https://www.delmartreeservices.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://brushbandittree.com/> (referer: None)
DEBUG: Scraped from <200 https://www.davey.com/about/contact-us/?type=nonresidential>
{'emails': [],
 'facebook': 'https://www.facebook.com/DaveyTree',
 'instagram': 'https://instagram.com/DaveyTree/',
 'linkedin': 'http://www.linkedin.com/company/the-davey-tree-expert-company',
 'twitter': 'https://twitter.com/DaveyTree'}
DEBUG: Crawled (403) <GET http://tomcotreecare.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://lamannatreeservice.com/contact/> (referer: https://lamannatreeservice.com/)
ERROR: Gave up retrying <GET https://barbertontree.com/robots.txt> (failed 3 times): 429 Unknown Status
DEBUG: Crawled (429) <GET https://barbertontree.com/robots.txt> (referer: None)
DEBUG: Rule at line 12 without any user agent to enforce it on.
DEBUG: Rule at line 14 without any user agent to enforce it on.
DEBUG: Rule at line 17 without any user agent to enforce it on.
DEBUG: Rule at line 20 without any user agent to enforce it on.
DEBUG: Rule at line 21 without any user agent to enforce it on.
DEBUG: Rule at line 22 without any user agent to enforce it on.
DEBUG: Rule at line 23 without any user agent to enforce it on.
DEBUG: Rule at line 26 without any user agent to enforce it on.
DEBUG: Rule at line 29 without any user agent to enforce it on.
DEBUG: Rule at line 32 without any user agent to enforce it on.
DEBUG: Rule at line 33 without any user agent to enforce it on.
DEBUG: Rule at line 34 without any user agent to enforce it on.
DEBUG: Rule at line 37 without any user agent to enforce it on.
DEBUG: Rule at line 38 without any user agent to enforce it on.
DEBUG: Rule at line 39 without any user agent to enforce it on.
DEBUG: Rule at line 42 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://www.delmartreeservices.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.davey.com/residential-tree-services/local-offices/akron-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Akron> (referer: None)
DEBUG: Crawled (200) <GET https://urbanloggersohio.com/contact-urban-loggers/> (referer: https://urbanloggersohio.com/)
DEBUG: Crawled (200) <GET https://premiertreesllc.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.larochetree.com/robots.txt> (referer: None)
ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\engine.py", line 152, in _next_request
    request = next(self.slot.start_requests)
  File "C:\Users\moffi\PycharmProjects\scraping_web\scraper\scraper\spiders\website_spider.py", line 40, in start_requests
    yield scrapy.Request(url=url, callback=self.parse_website)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\http\request\__init__.py", line 60, in __init__
    self._set_url(url)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\http\request\__init__.py", line 108, in _set_url
    raise ValueError(f'Missing scheme in request url: {self._url}')
ValueError: Missing scheme in request url: 
DEBUG: Crawled (403) <GET http://www.dolcestreeservice.com/robots.txt> (referer: None)
DEBUG: Scraped from <200 https://lamannatreeservice.com/contact/>
{'emails': [],
 'facebook': 'https://www.facebook.com/lamannatreeservice/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://treeservicenow.net/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://tomcotreecare.com/> (referer: None)
DEBUG: Scraped from <200 https://urbanloggersohio.com/contact-urban-loggers/>
{'emails': ['Info@urbanloggersohio.com'],
 'facebook': '',
 'instagram': 'https://www.instagram.com/urbanloggersllc/',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (403) <GET http://www.dolcestreeservice.com/> (referer: None)
DEBUG: Retrying <GET https://barbertontree.com/> (failed 1 times): 429 Unknown Status
INFO: Ignoring response <403 http://tomcotreecare.com/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET https://www.treeservicenow.net/> from <GET http://www.treeservicenow.net/>
INFO: Ignoring response <403 http://www.dolcestreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Retrying <GET https://barbertontree.com/> (failed 2 times): 429 Unknown Status
DEBUG: Crawled (200) <GET https://www.larochetree.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.davey.com/about/contact-us/?type=nonservicerequest> (referer: https://www.davey.com/residential-tree-services/local-offices/columbus-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Columbus)
DEBUG: Retrying <GET https://premiertreesllc.com/contact-us/> (failed 1 times): 429 Unknown Status
DEBUG: Crawled (404) <GET https://www.napierandson.com/robots.txt> (referer: None)
ERROR: Gave up retrying <GET https://barbertontree.com/> (failed 3 times): 429 Unknown Status
DEBUG: Crawled (429) <GET https://barbertontree.com/> (referer: None)
DEBUG: Crawled (200) <GET https://brushbandittree.com/contact-us/> (referer: https://brushbandittree.com/)
DEBUG: Scraped from <200 https://www.davey.com/about/contact-us/?type=nonservicerequest>
{'emails': [],
 'facebook': 'https://www.facebook.com/DaveyTree',
 'instagram': 'https://instagram.com/DaveyTree/',
 'linkedin': 'http://www.linkedin.com/company/the-davey-tree-expert-company',
 'twitter': 'https://twitter.com/DaveyTree'}
DEBUG: Crawled (200) <GET https://tree-works-ohio.com/robots.txt> (referer: None)
DEBUG: Retrying <GET https://premiertreesllc.com/contact-us/> (failed 2 times): 429 Unknown Status
INFO: Ignoring response <429 https://barbertontree.com/>: HTTP status code is not handled or not allowed
DEBUG: Scraped from <200 https://brushbandittree.com/contact-us/>
{'emails': [],
 'facebook': 'https://www.facebook.com/brushbandittree/',
 'instagram': 'https://www.instagram.com/brushband1t/',
 'linkedin': '',
 'twitter': 'https://twitter.com/brushband1t%20'}
DEBUG: Crawled (200) <GET https://www.larochetree.com/contact> (referer: https://www.larochetree.com/)
DEBUG: Crawled (200) <GET https://www.napierandson.com/> (referer: None)
ERROR: Gave up retrying <GET https://premiertreesllc.com/contact-us/> (failed 3 times): 429 Unknown Status
DEBUG: Crawled (429) <GET https://premiertreesllc.com/contact-us/> (referer: https://premiertreesllc.com/)
DEBUG: Crawled (200) <GET https://treeservicecolumbusohio.net/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://treeservicenow.net/> from <GET https://www.treeservicenow.net/>
DEBUG: Scraped from <200 https://www.larochetree.com/contact>
{'emails': ['831126cb46b74583bf6f72c5061cba9d@sentry-viewer.wixpress.com',
            'ContactUs@larochetree.com',
            'dd0a55ccb8124b9c9d938e3acf41f8aa@sentry.wixpress.com',
            'contactus@larochetree.com',
            '605a7baede844d278b89dc95ae0a9123@sentry-next.wixpress.com'],
 'facebook': 'https://www.facebook.com/LaRocheTree',
 'instagram': 'https://www.instagram.com/LaRochetree/',
 'linkedin': 'https://www.linkedin.com/company/laroche-tree-service-inc',
 'twitter': 'https://twitter.com/larochetree'}
DEBUG: Redirecting (301) to <GET https://tree-works-ohio.com/> from <GET http://tree-works-ohio.com/>
INFO: Ignoring response <429 https://premiertreesllc.com/contact-us/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://treeservicenow.net/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://treeservicecolumbusohio.net/> (referer: None)
DEBUG: Crawled (200) <GET https://treeservicenow.net/> (referer: None)
DEBUG: Crawled (200) <GET https://treeservicenow.net/contact_us_1> (referer: https://treeservicenow.net/)
DEBUG: Crawled (200) <GET https://treeservicecolumbusohio.net/contact-us/> (referer: https://treeservicecolumbusohio.net/)
DEBUG: Crawled (200) <GET https://tree-works-ohio.com/> (referer: None)
DEBUG: Scraped from <200 https://treeservicenow.net/contact_us_1>
{'emails': [], 'facebook': '', 'instagram': '', 'linkedin': '', 'twitter': ''}
DEBUG: Scraped from <200 https://treeservicecolumbusohio.net/contact-us/>
{'emails': [], 'facebook': '', 'instagram': '', 'linkedin': '', 'twitter': ''}
DEBUG: Crawled (200) <GET https://tree-works-ohio.com/contact-tree-works-for-all-your-emergency-services-tree-removal-stump-grinding-etc/> (referer: https://tree-works-ohio.com/)
DEBUG: Scraped from <200 https://tree-works-ohio.com/contact-tree-works-for-all-your-emergency-services-tree-removal-stump-grinding-etc/>
{'emails': [], 'facebook': '', 'instagram': '', 'linkedin': '', 'twitter': ''}
INFO: Closing spider (finished)
INFO: Stored json feed (26 items) in: website_spider_output.json
INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 64229,
 'downloader/request_count': 245,
 'downloader/request_method_count/GET': 245,
 'downloader/response_bytes': 3504822,
 'downloader/response_count': 245,
 'downloader/response_status_count/200': 145,
 'downloader/response_status_count/301': 55,
 'downloader/response_status_count/302': 1,
 'downloader/response_status_count/403': 29,
 'downloader/response_status_count/404': 5,
 'downloader/response_status_count/429': 10,
 'dupefilter/filtered': 59,
 'elapsed_time_seconds': 9.725817,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 9, 12, 24, 1, 917649),
 'httpcompression/response_bytes': 14897610,
 'httpcompression/response_count': 156,
 'httperror/response_ignored_count': 16,
 'httperror/response_ignored_status_count/403': 14,
 'httperror/response_ignored_status_count/429': 2,
 'item_scraped_count': 26,
 'log_count/DEBUG': 491,
 'log_count/ERROR': 4,
 'log_count/INFO': 27,
 'request_depth_max': 1,
 'response_received_count': 182,
 'retry/count': 7,
 'retry/max_reached': 3,
 'retry/reason_count/429 Unknown Status': 7,
 'robotstxt/request_count': 84,
 'robotstxt/response_count': 84,
 'robotstxt/response_status_count/200': 63,
 'robotstxt/response_status_count/403': 15,
 'robotstxt/response_status_count/404': 5,
 'robotstxt/response_status_count/429': 1,
 'scheduler/dequeued': 137,
 'scheduler/dequeued/memory': 137,
 'scheduler/enqueued': 137,
 'scheduler/enqueued/memory': 137,
 'start_time': datetime.datetime(2022, 12, 9, 12, 23, 52, 191832)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: 2a2ffdff3734a16c
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
DEBUG: Redirecting (301) to <GET https://www.fandftrees.com/robots.txt> from <GET http://www.fandftrees.com/robots.txt>
DEBUG: Attempting to acquire lock 3223339012672 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 3223339012672 acquired on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Attempting to release lock 3223339012672 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 3223339012672 released on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Crawled (200) <GET http://www.columbustreeservicesllc.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET https://trapperstreeservice.com/robots.txt> (referer: None)
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Rule at line 14 without any user agent to enforce it on.
DEBUG: Rule at line 16 without any user agent to enforce it on.
DEBUG: Rule at line 35 without any user agent to enforce it on.
DEBUG: Rule at line 42 without any user agent to enforce it on.
DEBUG: Rule at line 43 without any user agent to enforce it on.
DEBUG: Rule at line 44 without any user agent to enforce it on.
DEBUG: Rule at line 45 without any user agent to enforce it on.
DEBUG: Rule at line 46 without any user agent to enforce it on.
DEBUG: Rule at line 47 without any user agent to enforce it on.
DEBUG: Rule at line 69 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://www.treetechohio.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET https://www.midohiotree.org/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://www.columbustreeservicesllc.com/> (referer: None)
DEBUG: Crawled (403) <GET https://trapperstreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.fandftrees.com/robots.txt> (referer: None)
DEBUG: Filtered duplicate request: <GET http://hardwicktreecare.com/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG: Crawled (200) <GET https://www.treetechohio.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.fandftrees.com/> from <GET http://www.fandftrees.com/>
DEBUG: Crawled (403) <GET https://www.midohiotree.org/> (referer: None)
DEBUG: Crawled (200) <GET http://www.charteroakscompany.com/robots.txt> (referer: None)
INFO: Ignoring response <403 https://trapperstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://www.fandftrees.com/> (referer: None)
DEBUG: Crawled (200) <GET http://hardwicktreecare.com/robots.txt> (referer: None)
INFO: Ignoring response <403 https://www.midohiotree.org/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET https://starwoodtree.com/robots.txt> from <GET http://starwoodtree.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://www.hardwicktreecare.com/> from <GET http://hardwicktreecare.com/>
DEBUG: Crawled (404) <GET http://ohiotreeandexcavating.net/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.hardwicktreecare.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.treetechohio.com/contact> (referer: https://www.treetechohio.com/)
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.hardwicktreecare.com/> (referer: None)
DEBUG: Crawled (403) <GET https://www.tackettstreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://www.treesaremybusiness.com/robots.txt> (referer: None)
DEBUG: Scraped from <200 https://www.treetechohio.com/contact>
{'emails': ['605a7baede844d278b89dc95ae0a9123@sentry-next.wixpress.com',
            'info@mysite.com',
            '8eb368c655b84e029ed79ad7a5c1718e@sentry.wixpress.com',
            '831126cb46b74583bf6f72c5061cba9d@sentry-viewer.wixpress.com'],
 'facebook': 'http://www.facebook.com/wix',
 'instagram': '',
 'linkedin': '',
 'twitter': 'http://www.twitter.com/wix'}
DEBUG: Crawled (200) <GET https://www.russelltreeexperts.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://ohiotreeandexcavating.com/> from <GET http://ohiotreeandexcavating.net/>
DEBUG: Crawled (403) <GET https://www.tackettstreeservice.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.treesaremybusiness.com/> from <GET http://www.treesaremybusiness.com/>
DEBUG: Crawled (403) <GET http://www.hackettstreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://ohiotreeandexcavating.com/robots.txt> from <GET http://ohiotreeandexcavating.com/robots.txt>
INFO: Ignoring response <403 https://www.tackettstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (403) <GET http://www.hackettstreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/akron/?utm_source=GMB&utm_medium=organic&utm_campaign=doylestown> (referer: None)
DEBUG: Crawled (200) <GET https://www.hardwicktreecare.com/contact-hardwick-tree-care> (referer: https://www.hardwicktreecare.com/)
DEBUG: Crawled (200) <GET https://ohiotreeandexcavating.com/robots.txt> (referer: None)
INFO: Ignoring response <403 http://www.hackettstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://www.russelltreeexperts.com/> (referer: None)
DEBUG: Crawled (404) <GET http://ohiotreecare.com/robots.txt> (referer: None)
DEBUG: Rule at line 1 without any user agent to enforce it on.
DEBUG: Rule at line 2 without any user agent to enforce it on.
DEBUG: Rule at line 8 without any user agent to enforce it on.
DEBUG: Rule at line 9 without any user agent to enforce it on.
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Rule at line 11 without any user agent to enforce it on.
DEBUG: Rule at line 12 without any user agent to enforce it on.
DEBUG: Redirecting (301) to <GET https://ohiotreeandexcavating.com/> from <GET http://ohiotreeandexcavating.com/>
DEBUG: Scraped from <200 https://www.hardwicktreecare.com/contact-hardwick-tree-care>
{'emails': ['605a7baede844d278b89dc95ae0a9123@sentry-next.wixpress.com',
            '8eb368c655b84e029ed79ad7a5c1718e@sentry.wixpress.com',
            'f1ffc0b5efe04e9eb9762cd808722520@sentry.wixpress.com',
            'info@HardwickTreeCare.com',
            '831126cb46b74583bf6f72c5061cba9d@sentry-viewer.wixpress.com'],
 'facebook': 'https://www.facebook.com/hardwicktreecarellc',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Redirecting (301) to <GET https://www.charteroakscompany.com/> from <GET http://www.charteroakscompany.com/>
DEBUG: Crawled (403) <GET http://deeprootedtreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://specialtytreeohio.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://www.timberlandtreeohio.com/robots.txt> from <GET http://timberlandtreeohio.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.independenttree.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://deeprootedtreeservice.com/> (referer: None)
INFO: Ignoring response <403 http://deeprootedtreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET https://treesaremybusiness.com/> from <GET https://www.treesaremybusiness.com/>
DEBUG: Crawled (200) <GET https://ohiotreeandexcavating.com/> (referer: None)
DEBUG: Crawled (200) <GET http://www.timberlandtreeohio.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://herculestree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://specialtytreeohio.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.whymonster.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.charteroakscompany.com/> (referer: None)
DEBUG: Redirecting (301) to <GET http://www.timberlandtreeohio.com/> from <GET http://timberlandtreeohio.com/>
DEBUG: Redirecting (301) to <GET https://www.monstertreeservice.com/akron/contact-us/> from <GET https://www.whymonster.com/akron/contact-us/>
DEBUG: Crawled (200) <GET https://www.independenttree.com/?utm_source=gmb&utm_medium=local&utm_campaign=website> (referer: None)
DEBUG: Crawled (200) <GET https://starwoodtree.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://jstreeservicesllc.com/robots.txt> from <GET http://www.jstreeservicesllc.com/robots.txt>
DEBUG: Crawled (403) <GET http://cottstrees.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/akron/contact-us/> (referer: https://www.monstertreeservice.com/akron/?utm_source=GMB&utm_medium=organic&utm_campaign=doylestown)
DEBUG: Redirecting (301) to <GET https://starwoodtree.com/> from <GET http://starwoodtree.com/>
DEBUG: Crawled (200) <GET http://www.timberlandtreeohio.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://cottstrees.com/> (referer: None)
DEBUG: Scraped from <200 https://www.monstertreeservice.com/akron/contact-us/>
{'emails': [],
 'facebook': 'https://www.facebook.com/MonsterTreeServiceofAkron/',
 'instagram': 'https://www.instagram.com/monstertreeservices/',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://treesaremybusiness.com/robots.txt> (referer: None)
INFO: Ignoring response <403 http://cottstrees.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://www.ewsmithtree.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://roquetree.com/robots.txt> from <GET http://roquetree.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.independenttree.com/contact-us/> (referer: https://www.independenttree.com/?utm_source=gmb&utm_medium=local&utm_campaign=website)
DEBUG: Redirecting (301) to <GET https://www.basictreecare.com/robots.txt> from <GET http://www.basictreecare.com/robots.txt>
DEBUG: Crawled (200) <GET http://jstreeservicesllc.com/robots.txt> (referer: None)
DEBUG: Scraped from <200 https://www.independenttree.com/contact-us/>
{'emails': ['info@independenttree.com'],
 'facebook': 'https://www.facebook.com/IndependentTreeOH/',
 'instagram': 'https://www.instagram.com/independenttreeservice/',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://treesaremybusiness.com/> (referer: None)
DEBUG: Crawled (200) <GET http://www.kiddertreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://haneytreeservice.com/robots.txt> from <GET http://www.haneytreeservice.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.ewsmithtree.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.kiddertreemov.com/> from <GET http://www.kiddertreeservice.com/>
DEBUG: Crawled (200) <GET http://ohiotreecare.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/cleveland/?utm_source=GMB&utm_medium=organic&utm_campaign=AvonLake> (referer: None)
DEBUG: Crawled (200) <GET https://www.basictreecare.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://jstreeservicesllc.com/> from <GET http://www.jstreeservicesllc.com/>
DEBUG: Crawled (200) <GET https://treesaremybusiness.com/contact-us/> (referer: https://treesaremybusiness.com/)
DEBUG: Crawled (200) <GET http://haneytreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.basictreecare.com/> from <GET http://www.basictreecare.com/>
DEBUG: Scraped from <200 https://treesaremybusiness.com/contact-us/>
{'emails': ['millcraft@treesaremybusiness.com',
            'office@treesaremybusiness.com'],
 'facebook': 'https://www.facebook.com/treesaremybusinessohio/',
 'instagram': 'https://www.instagram.com/trees_are_my_business/',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://roquetree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.kiddertreemov.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://haneytreeservice.com/> from <GET http://www.haneytreeservice.com/>
DEBUG: Crawled (200) <GET https://www.ewsmithtree.com/contact-us/> (referer: https://www.ewsmithtree.com/)
DEBUG: Redirecting (301) to <GET https://roquetree.com/> from <GET http://roquetree.com/>
DEBUG: Crawled (200) <GET http://www.timberlandtreeohio.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.basictreecare.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.kiddertreemov.com/> (referer: None)
DEBUG: Scraped from <200 https://www.ewsmithtree.com/contact-us/>
{'emails': [], 'facebook': '', 'instagram': '', 'linkedin': '', 'twitter': ''}
DEBUG: Crawled (200) <GET http://haneytreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.jttreeservicellc.com/robots.txt> from <GET http://www.jttreeservicellc.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://www.roquetree.com/> from <GET https://roquetree.com/>
DEBUG: Redirecting (301) to <GET http://haneytreeservice.com/index.html> from <GET http://haneytreeservice.com/>
DEBUG: Crawled (403) <GET https://www.kandatreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET https://www.kandatreeservice.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.ripleytreeservice.com/robots.txt> from <GET http://www.ripleytreeservice.com/robots.txt>
INFO: Ignoring response <403 https://www.kandatreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (403) <GET http://www.jjsfamilytreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://jstreeservicesllc.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://www.jjsfamilytreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.davey.com/robots.txt> (referer: None)
INFO: Ignoring response <403 http://www.jjsfamilytreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET http://haneytreeservice.com/index.html> (referer: None)
DEBUG: Crawled (200) <GET https://www.roquetree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.jttreeservicellc.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://extremetreeservicestoledo.com/robots.txt> from <GET https://www.extremetreeservicestoledo.com/robots.txt>
DEBUG: Crawled (200) <GET https://parkstree.net/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://herculestree.com/> from <GET http://herculestree.com/>
DEBUG: Crawled (200) <GET https://www.davey.com/residential-tree-services/local-offices/columbus-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Columbus> (referer: None)
DEBUG: Crawled (200) <GET https://starwoodtree.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.ripleytreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://parkstree.net/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.jttreeservicellc.com/> from <GET http://www.jttreeservicellc.com/>
DEBUG: Crawled (200) <GET https://www.roquetree.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.rawtreeserv.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.savatree.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.ripleytreeservice.com/> from <GET http://www.ripleytreeservice.com/>
DEBUG: Crawled (200) <GET https://oak-tree-services-tree-service.business.site/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://parkstree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://jstreeservicesllc.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://rawtreeserv.com/> from <GET https://www.rawtreeserv.com/>
DEBUG: Crawled (200) <GET https://www.jttreeservicellc.com/> (referer: None)
DEBUG: Crawled (200) <GET https://parkstree.com/> (referer: None)
DEBUG: Crawled (200) <GET https://extremetreeservicestoledo.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.savatree.com/dayton-ohio-tree-service-lawn-care?utm_source=GMB&utm_medium=organic&utm_campaign=dayton> (referer: None)
DEBUG: Crawled (200) <GET https://www.ripleytreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://oak-tree-services-tree-service.business.site/?utm_source=gmb&utm_medium=referral> (referer: None)
DEBUG: Redirecting (301) to <GET https://extremetreeservicestoledo.com/> from <GET https://www.extremetreeservicestoledo.com/>
DEBUG: Crawled (200) <GET https://www.davey.com/residential-tree-services/local-offices/cincinnati-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Cincinnati> (referer: None)
DEBUG: Crawled (200) <GET https://extremetreeservicestoledo.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://extremetreeservicestoledo.com/> (referer: None)
DEBUG: Crawled (200) <GET https://rawtreeserv.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://herculestree.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.stevestoledotree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://blackstreemov.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://jstreeservicesllc.com/contact-us/> from <GET http://www.jstreeservicesllc.com/contact-us>
DEBUG: Redirecting (301) to <GET https://www.blackstreemov.com/> from <GET http://blackstreemov.com/>
DEBUG: Crawled (200) <GET https://www.stevestoledotree.com/?utm_source=googlelocal&utm_medium=organic> (referer: None)
DEBUG: Crawled (403) <GET http://www.haymakertreeandlawn.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://www.haymakertreeandlawn.com/> (referer: None)
DEBUG: Crawled (200) <GET https://jstreeservicesllc.com/contact-us/> (referer: https://jstreeservicesllc.com/)
DEBUG: Crawled (200) <GET https://www.davey.com/about/contact-us/?type=nonservicerequest> (referer: https://www.davey.com/residential-tree-services/local-offices/columbus-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Columbus)
DEBUG: Crawled (200) <GET https://rawtreeserv.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.blackstreemov.com/robots.txt> (referer: None)
INFO: Ignoring response <403 http://www.haymakertreeandlawn.com/>: HTTP status code is not handled or not allowed
DEBUG: Scraped from <200 https://jstreeservicesllc.com/contact-us/>
{'emails': ['j_s.tree@yahoo.com'],
 'facebook': 'http://www.facebook.com/jandstreeservices',
 'instagram': 'http://www.instagram.com/jandstreeservices',
 'linkedin': '',
 'twitter': ''}
DEBUG: Scraped from <200 https://www.davey.com/about/contact-us/?type=nonservicerequest>
{'emails': [],
 'facebook': 'https://www.facebook.com/DaveyTree',
 'instagram': 'https://instagram.com/DaveyTree/',
 'linkedin': 'http://www.linkedin.com/company/the-davey-tree-expert-company',
 'twitter': 'https://twitter.com/DaveyTree'}
DEBUG: Crawled (200) <GET https://herculestree.com/contact> (referer: https://herculestree.com/)
DEBUG: Crawled (200) <GET https://extremetreeservicestoledo.com/contact/> (referer: https://extremetreeservicestoledo.com/)
DEBUG: Crawled (200) <GET https://www.blackstreemov.com/> (referer: None)
DEBUG: Crawled (200) <GET http://jstreeservicesllc.com/contact-us/> (referer: None)
DEBUG: Scraped from <200 https://herculestree.com/contact>
{'emails': ['Herculestree@gmail.com', 'HerculesTree@gmail.com'],
 'facebook': 'https://www.facebook.com/HerculesTreeLLC/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Scraped from <200 https://extremetreeservicestoledo.com/contact/>
{'emails': ['extremetreeswanton@gmail.com'],
 'facebook': 'https://www.facebook.com/ToledosExtremeTree/',
 'instagram': '',
 'linkedin': '',
 'twitter': 'https://twitter.com/SuperClimber101'}
DEBUG: Crawled (200) <GET https://affordabletreeservicesofohiollc.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://starwoodtree.com/contact-us/> (referer: https://starwoodtree.com/)
DEBUG: Scraped from <200 http://jstreeservicesllc.com/contact-us/>
{'emails': ['j_s.tree@yahoo.com'],
 'facebook': 'http://www.facebook.com/jandstreeservices',
 'instagram': 'http://www.instagram.com/jandstreeservices',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://affordabletreeservicesofohiollc.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.blackstreemov.com/contact> (referer: https://www.blackstreemov.com/)
DEBUG: Crawled (403) <GET http://www.cstreemulch.com/robots.txt> (referer: None)
DEBUG: Scraped from <200 https://starwoodtree.com/contact-us/>
{'emails': ['support@starwoodtree.com'],
 'facebook': 'https://www.facebook.com/Starwoodtreeservice/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (403) <GET http://www.cstreemulch.com/> (referer: None)
DEBUG: Scraped from <200 https://www.blackstreemov.com/contact>
{'emails': ['605a7baede844d278b89dc95ae0a9123@sentry-next.wixpress.com',
            'BlacksTreeService140@gmail.com',
            'blackstreeservice140@gmail.com',
            '831126cb46b74583bf6f72c5061cba9d@sentry-viewer.wixpress.com'],
 'facebook': 'https://www.facebook.com/pages/Blacks-Tree-Service/205784306214398',
 'instagram': 'http://instagram.com/wix',
 'linkedin': '',
 'twitter': 'https://twitter.com/BlacksTreeMOV'}
INFO: Ignoring response <403 http://www.cstreemulch.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://treesrus-treeservice.business.site/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://www.ashtreeservicepro.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://www.ashtreeservicepro.com/> (referer: None)
DEBUG: Crawled (200) <GET https://challengerstreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.davey.com/about/contact-us/?type=residential> (referer: https://www.davey.com/residential-tree-services/local-offices/columbus-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Columbus)
DEBUG: Crawled (200) <GET https://herculestree.com/contact/request-quote> (referer: https://herculestree.com/)
INFO: Ignoring response <403 http://www.ashtreeservicepro.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://herculestree.com/contact/> (referer: https://herculestree.com/)
DEBUG: Crawled (200) <GET https://herculestree.com/contact/request-quote/> (referer: https://herculestree.com/)
DEBUG: Crawled (200) <GET https://treesrus-treeservice.business.site/?utm_source=gmb&utm_medium=referral> (referer: None)
DEBUG: Scraped from <200 https://www.davey.com/about/contact-us/?type=residential>
{'emails': [],
 'facebook': 'https://www.facebook.com/DaveyTree',
 'instagram': 'https://instagram.com/DaveyTree/',
 'linkedin': 'http://www.linkedin.com/company/the-davey-tree-expert-company',
 'twitter': 'https://twitter.com/DaveyTree'}
DEBUG: Redirecting (301) to <GET http://www.toddstreeservice.com/robots.txt> from <GET http://toddstreeservice.com/robots.txt>
DEBUG: Scraped from <200 https://herculestree.com/contact/request-quote>
{'emails': ['HerculesTree@gmail.com', 'info@herculesTrees.com'],
 'facebook': 'https://www.facebook.com/HerculesTreeLLC/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Scraped from <200 https://herculestree.com/contact/>
{'emails': ['Herculestree@gmail.com', 'HerculesTree@gmail.com'],
 'facebook': 'https://www.facebook.com/HerculesTreeLLC/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Scraped from <200 https://herculestree.com/contact/request-quote/>
{'emails': ['HerculesTree@gmail.com', 'info@herculesTrees.com'],
 'facebook': 'https://www.facebook.com/HerculesTreeLLC/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://extremetreeohio.com/robots.txt> (referer: None)
DEBUG: Retrying <GET https://affordabletreese/robots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: affordabletreese.
DEBUG: Redirecting (301) to <GET https://hoffmantreeservice.com/robots.txt> from <GET http://hoffmantreeservice.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://www.roguetreesolutions.com/robots.txt> from <GET http://www.roguetreesolutions.com/robots.txt>
DEBUG: Crawled (200) <GET http://urbanloggersohio.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://extremetreeohio.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://urbanloggersohio.com/> from <GET http://urbanloggersohio.com/>
DEBUG: Crawled (200) <GET https://www.davey.com/residential-tree-services/local-offices/northeast-cleveland-tree-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Northeast%20Cleveland> (referer: None)
DEBUG: Crawled (403) <GET http://www.toddstreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://www.toddstreeservice.com/> from <GET http://toddstreeservice.com/>
DEBUG: Crawled (200) <GET https://hoffmantreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://rstreeservicellc.com/robots.txt> (referer: None)
DEBUG: Rule at line 1 without any user agent to enforce it on.
DEBUG: Rule at line 5 without any user agent to enforce it on.
DEBUG: Rule at line 9 without any user agent to enforce it on.
DEBUG: Rule at line 14 without any user agent to enforce it on.
DEBUG: Rule at line 19 without any user agent to enforce it on.
DEBUG: Rule at line 21 without any user agent to enforce it on.
DEBUG: Rule at line 22 without any user agent to enforce it on.
DEBUG: Rule at line 23 without any user agent to enforce it on.
DEBUG: Rule at line 26 without any user agent to enforce it on.
DEBUG: Rule at line 28 without any user agent to enforce it on.
DEBUG: Rule at line 29 without any user agent to enforce it on.
DEBUG: Rule at line 31 without any user agent to enforce it on.
DEBUG: Rule at line 32 without any user agent to enforce it on.
DEBUG: Rule at line 33 without any user agent to enforce it on.
DEBUG: Rule at line 34 without any user agent to enforce it on.
DEBUG: Rule at line 35 without any user agent to enforce it on.
DEBUG: Rule at line 36 without any user agent to enforce it on.
DEBUG: Rule at line 37 without any user agent to enforce it on.
DEBUG: Rule at line 39 without any user agent to enforce it on.
DEBUG: Rule at line 40 without any user agent to enforce it on.
DEBUG: Rule at line 43 without any user agent to enforce it on.
DEBUG: Rule at line 44 without any user agent to enforce it on.
DEBUG: Rule at line 46 without any user agent to enforce it on.
DEBUG: Rule at line 48 without any user agent to enforce it on.
DEBUG: Rule at line 51 without any user agent to enforce it on.
DEBUG: Rule at line 53 without any user agent to enforce it on.
DEBUG: Rule at line 54 without any user agent to enforce it on.
DEBUG: Rule at line 55 without any user agent to enforce it on.
DEBUG: Rule at line 56 without any user agent to enforce it on.
DEBUG: Rule at line 57 without any user agent to enforce it on.
DEBUG: Rule at line 58 without any user agent to enforce it on.
DEBUG: Rule at line 60 without any user agent to enforce it on.
DEBUG: Rule at line 61 without any user agent to enforce it on.
DEBUG: Rule at line 62 without any user agent to enforce it on.
DEBUG: Rule at line 63 without any user agent to enforce it on.
DEBUG: Rule at line 64 without any user agent to enforce it on.
DEBUG: Rule at line 65 without any user agent to enforce it on.
DEBUG: Rule at line 66 without any user agent to enforce it on.
DEBUG: Rule at line 67 without any user agent to enforce it on.
DEBUG: Rule at line 68 without any user agent to enforce it on.
DEBUG: Rule at line 69 without any user agent to enforce it on.
DEBUG: Rule at line 70 without any user agent to enforce it on.
DEBUG: Rule at line 71 without any user agent to enforce it on.
DEBUG: Rule at line 72 without any user agent to enforce it on.
DEBUG: Rule at line 73 without any user agent to enforce it on.
DEBUG: Rule at line 74 without any user agent to enforce it on.
DEBUG: Rule at line 75 without any user agent to enforce it on.
DEBUG: Rule at line 76 without any user agent to enforce it on.
DEBUG: Rule at line 77 without any user agent to enforce it on.
DEBUG: Rule at line 78 without any user agent to enforce it on.
DEBUG: Rule at line 79 without any user agent to enforce it on.
DEBUG: Rule at line 80 without any user agent to enforce it on.
DEBUG: Rule at line 81 without any user agent to enforce it on.
DEBUG: Rule at line 83 without any user agent to enforce it on.
DEBUG: Rule at line 85 without any user agent to enforce it on.
DEBUG: Rule at line 86 without any user agent to enforce it on.
DEBUG: Rule at line 87 without any user agent to enforce it on.
DEBUG: Rule at line 88 without any user agent to enforce it on.
DEBUG: Rule at line 89 without any user agent to enforce it on.
DEBUG: Rule at line 90 without any user agent to enforce it on.
DEBUG: Rule at line 91 without any user agent to enforce it on.
DEBUG: Rule at line 92 without any user agent to enforce it on.
DEBUG: Rule at line 93 without any user agent to enforce it on.
DEBUG: Rule at line 94 without any user agent to enforce it on.
DEBUG: Rule at line 95 without any user agent to enforce it on.
DEBUG: Rule at line 96 without any user agent to enforce it on.
DEBUG: Rule at line 98 without any user agent to enforce it on.
DEBUG: Rule at line 99 without any user agent to enforce it on.
DEBUG: Rule at line 100 without any user agent to enforce it on.
DEBUG: Rule at line 101 without any user agent to enforce it on.
DEBUG: Rule at line 102 without any user agent to enforce it on.
DEBUG: Rule at line 103 without any user agent to enforce it on.
DEBUG: Rule at line 104 without any user agent to enforce it on.
DEBUG: Rule at line 106 without any user agent to enforce it on.
DEBUG: Rule at line 107 without any user agent to enforce it on.
DEBUG: Rule at line 108 without any user agent to enforce it on.
DEBUG: Rule at line 110 without any user agent to enforce it on.
DEBUG: Rule at line 112 without any user agent to enforce it on.
DEBUG: Rule at line 113 without any user agent to enforce it on.
DEBUG: Rule at line 114 without any user agent to enforce it on.
DEBUG: Rule at line 115 without any user agent to enforce it on.
DEBUG: Rule at line 116 without any user agent to enforce it on.
DEBUG: Rule at line 117 without any user agent to enforce it on.
DEBUG: Rule at line 121 without any user agent to enforce it on.
DEBUG: Rule at line 122 without any user agent to enforce it on.
DEBUG: Crawled (404) <GET http://www.boonestreeservice.com/robots.txt> (referer: None)
DEBUG: Rule at line 2 without any user agent to enforce it on.
DEBUG: Rule at line 8 without any user agent to enforce it on.
DEBUG: Rule at line 9 without any user agent to enforce it on.
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Crawled (404) <GET https://www.roguetreesolutions.com/robots.txt> (referer: None)
DEBUG: Rule at line 1 without any user agent to enforce it on.
DEBUG: Rule at line 2 without any user agent to enforce it on.
DEBUG: Rule at line 14 without any user agent to enforce it on.
DEBUG: Rule at line 15 without any user agent to enforce it on.
DEBUG: Rule at line 23 without any user agent to enforce it on.
DEBUG: Rule at line 28 without any user agent to enforce it on.
DEBUG: Rule at line 29 without any user agent to enforce it on.
DEBUG: Rule at line 32 without any user agent to enforce it on.
DEBUG: Rule at line 33 without any user agent to enforce it on.
DEBUG: Rule at line 45 without any user agent to enforce it on.
DEBUG: Rule at line 46 without any user agent to enforce it on.
DEBUG: Rule at line 47 without any user agent to enforce it on.
DEBUG: Rule at line 48 without any user agent to enforce it on.
DEBUG: Rule at line 49 without any user agent to enforce it on.
DEBUG: Rule at line 50 without any user agent to enforce it on.
DEBUG: Rule at line 51 without any user agent to enforce it on.
DEBUG: Rule at line 52 without any user agent to enforce it on.
DEBUG: Rule at line 53 without any user agent to enforce it on.
DEBUG: Rule at line 54 without any user agent to enforce it on.
DEBUG: Rule at line 55 without any user agent to enforce it on.
DEBUG: Rule at line 57 without any user agent to enforce it on.
DEBUG: Rule at line 58 without any user agent to enforce it on.
DEBUG: Rule at line 59 without any user agent to enforce it on.
DEBUG: Rule at line 61 without any user agent to enforce it on.
DEBUG: Rule at line 62 without any user agent to enforce it on.
DEBUG: Rule at line 63 without any user agent to enforce it on.
DEBUG: Rule at line 64 without any user agent to enforce it on.
DEBUG: Rule at line 65 without any user agent to enforce it on.
DEBUG: Rule at line 66 without any user agent to enforce it on.
DEBUG: Rule at line 67 without any user agent to enforce it on.
DEBUG: Rule at line 70 without any user agent to enforce it on.
DEBUG: Rule at line 72 without any user agent to enforce it on.
DEBUG: Rule at line 78 without any user agent to enforce it on.
DEBUG: Rule at line 79 without any user agent to enforce it on.
DEBUG: Rule at line 80 without any user agent to enforce it on.
DEBUG: Rule at line 81 without any user agent to enforce it on.
DEBUG: Rule at line 87 without any user agent to enforce it on.
DEBUG: Rule at line 88 without any user agent to enforce it on.
DEBUG: Rule at line 92 without any user agent to enforce it on.
DEBUG: Rule at line 94 without any user agent to enforce it on.
DEBUG: Rule at line 96 without any user agent to enforce it on.
DEBUG: Rule at line 98 without any user agent to enforce it on.
DEBUG: Rule at line 100 without any user agent to enforce it on.
DEBUG: Rule at line 101 without any user agent to enforce it on.
DEBUG: Rule at line 104 without any user agent to enforce it on.
DEBUG: Rule at line 106 without any user agent to enforce it on.
DEBUG: Rule at line 107 without any user agent to enforce it on.
DEBUG: Rule at line 108 without any user agent to enforce it on.
DEBUG: Rule at line 109 without any user agent to enforce it on.
DEBUG: Rule at line 110 without any user agent to enforce it on.
DEBUG: Rule at line 111 without any user agent to enforce it on.
DEBUG: Rule at line 112 without any user agent to enforce it on.
DEBUG: Rule at line 119 without any user agent to enforce it on.
DEBUG: Rule at line 120 without any user agent to enforce it on.
DEBUG: Rule at line 121 without any user agent to enforce it on.
DEBUG: Rule at line 122 without any user agent to enforce it on.
DEBUG: Rule at line 123 without any user agent to enforce it on.
DEBUG: Rule at line 128 without any user agent to enforce it on.
DEBUG: Rule at line 145 without any user agent to enforce it on.
DEBUG: Rule at line 155 without any user agent to enforce it on.
DEBUG: Rule at line 165 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://www.davey.com/about/contact-us/?type=nonresidential> (referer: https://www.davey.com/residential-tree-services/local-offices/columbus-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Columbus)
DEBUG: Crawled (403) <GET http://www.toddstreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.roguetreesolutions.com/> from <GET http://www.roguetreesolutions.com/>
DEBUG: Redirecting (301) to <GET https://hoffmantreeservice.com/> from <GET http://hoffmantreeservice.com/>
DEBUG: Crawled (200) <GET https://www.roguetreesolutions.com/> (referer: None)
DEBUG: Scraped from <200 https://www.davey.com/about/contact-us/?type=nonresidential>
{'emails': [],
 'facebook': 'https://www.facebook.com/DaveyTree',
 'instagram': 'https://instagram.com/DaveyTree/',
 'linkedin': 'http://www.linkedin.com/company/the-davey-tree-expert-company',
 'twitter': 'https://twitter.com/DaveyTree'}
DEBUG: Crawled (200) <GET http://www.boonestreeservice.com/> (referer: None)
DEBUG: Crawled (403) <GET http://www.toddstreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET http://rstreeservicellc.com/?utm_source=gmb&utm_medium=referral> (referer: None)
DEBUG: Crawled (200) <GET https://hoffmantreeservice.com/> (referer: None)
INFO: Ignoring response <403 http://www.toddstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Retrying <GET https://affordabletreese/robots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: affordabletreese.
DEBUG: Redirecting (301) to <GET https://tree-works-ohio.com/robots.txt> from <GET http://tree-works-ohio.com/robots.txt>
DEBUG: Redirecting (302) to <GET https://www.greatdanetreeexperts.com/404.html> from <GET https://www.greatdanetreeexperts.com/robots.txt>
DEBUG: Crawled (200) <GET https://urbanloggersohio.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.treeservicenow.net/robots.txt> from <GET http://www.treeservicenow.net/robots.txt>
DEBUG: Crawled (200) <GET https://www.greatdanetreeexperts.com/404.html> (referer: None)
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Rule at line 11 without any user agent to enforce it on.
DEBUG: Rule at line 12 without any user agent to enforce it on.
DEBUG: Rule at line 15 without any user agent to enforce it on.
DEBUG: Rule at line 17 without any user agent to enforce it on.
DEBUG: Rule at line 28 without any user agent to enforce it on.
DEBUG: Rule at line 43 without any user agent to enforce it on.
DEBUG: Rule at line 44 without any user agent to enforce it on.
DEBUG: Rule at line 45 without any user agent to enforce it on.
DEBUG: Rule at line 46 without any user agent to enforce it on.
DEBUG: Rule at line 47 without any user agent to enforce it on.
DEBUG: Rule at line 48 without any user agent to enforce it on.
DEBUG: Rule at line 49 without any user agent to enforce it on.
DEBUG: Rule at line 50 without any user agent to enforce it on.
DEBUG: Rule at line 54 without any user agent to enforce it on.
DEBUG: Rule at line 57 without any user agent to enforce it on.
DEBUG: Rule at line 62 without any user agent to enforce it on.
DEBUG: Rule at line 63 without any user agent to enforce it on.
DEBUG: Rule at line 67 without any user agent to enforce it on.
DEBUG: Rule at line 88 without any user agent to enforce it on.
DEBUG: Rule at line 136 without any user agent to enforce it on.
DEBUG: Rule at line 141 without any user agent to enforce it on.
DEBUG: Rule at line 144 without any user agent to enforce it on.
DEBUG: Rule at line 147 without any user agent to enforce it on.
DEBUG: Rule at line 156 without any user agent to enforce it on.
DEBUG: Rule at line 157 without any user agent to enforce it on.
DEBUG: Rule at line 191 without any user agent to enforce it on.
DEBUG: Redirecting (301) to <GET https://barbertontree.com/robots.txt> from <GET https://www.barbertontree.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.davey.com/residential-tree-services/local-offices/akron-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Akron> (referer: None)
DEBUG: Crawled (200) <GET https://urbanloggersohio.com/contact-urban-loggers/> (referer: https://urbanloggersohio.com/)
DEBUG: Crawled (200) <GET https://barbertontree.com/robots.txt> (referer: None)
DEBUG: Rule at line 3 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://brushbandittree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.greatdanetreeexperts.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://barbertontree.com/> from <GET https://www.barbertontree.com/>
DEBUG: Crawled (200) <GET https://premiertreesllc.com/robots.txt> (referer: None)
DEBUG: Scraped from <200 https://urbanloggersohio.com/contact-urban-loggers/>
{'emails': ['Info@urbanloggersohio.com'],
 'facebook': '',
 'instagram': 'https://www.instagram.com/urbanloggersllc/',
 'linkedin': '',
 'twitter': ''}
DEBUG: Redirecting (301) to <GET https://lamannatreeservice.com/robots.txt> from <GET http://www.lamannatreeservice.com/robots.txt>
DEBUG: Retrying <GET https://premiertreesllc.com/> (failed 1 times): 429 Unknown Status
DEBUG: Retrying <GET https://barbertontree.com/robots.txt> (failed 1 times): 429 Unknown Status
DEBUG: Redirecting (301) to <GET https://lamannatreeservice.com/?robots=1> from <GET https://lamannatreeservice.com/robots.txt>
DEBUG: Crawled (200) <GET https://lamannatreeservice.com/?robots=1> (referer: None)
DEBUG: Crawled (200) <GET https://larochetree.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://lamannatreeservice.com/> from <GET http://www.lamannatreeservice.com/>
DEBUG: Crawled (200) <GET https://tree-works-ohio.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.larochetree.com/> from <GET https://larochetree.com/>
DEBUG: Retrying <GET https://barbertontree.com/robots.txt> (failed 2 times): 429 Unknown Status
DEBUG: Redirecting (301) to <GET https://lamannatreeservice.com/?robots=1> from <GET https://lamannatreeservice.com/robots.txt>
DEBUG: Crawled (200) <GET https://lamannatreeservice.com/?robots=1> (referer: None)
DEBUG: Crawled (200) <GET https://lamannatreeservice.com/> (referer: None)
ERROR: Gave up retrying <GET https://barbertontree.com/robots.txt> (failed 3 times): 429 Unknown Status
DEBUG: Crawled (429) <GET https://barbertontree.com/robots.txt> (referer: None)
DEBUG: Rule at line 12 without any user agent to enforce it on.
DEBUG: Rule at line 14 without any user agent to enforce it on.
DEBUG: Rule at line 17 without any user agent to enforce it on.
DEBUG: Rule at line 20 without any user agent to enforce it on.
DEBUG: Rule at line 21 without any user agent to enforce it on.
DEBUG: Rule at line 22 without any user agent to enforce it on.
DEBUG: Rule at line 23 without any user agent to enforce it on.
DEBUG: Rule at line 26 without any user agent to enforce it on.
DEBUG: Rule at line 29 without any user agent to enforce it on.
DEBUG: Rule at line 32 without any user agent to enforce it on.
DEBUG: Rule at line 33 without any user agent to enforce it on.
DEBUG: Rule at line 34 without any user agent to enforce it on.
DEBUG: Rule at line 37 without any user agent to enforce it on.
DEBUG: Rule at line 38 without any user agent to enforce it on.
DEBUG: Rule at line 39 without any user agent to enforce it on.
DEBUG: Rule at line 42 without any user agent to enforce it on.
DEBUG: Crawled (403) <GET http://tomcotreecare.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://premiertreesllc.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://tree-works-ohio.com/> from <GET http://tree-works-ohio.com/>
ERROR: Gave up retrying <GET https://affordabletreese/robots.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: affordabletreese.
ERROR: Error downloading <GET https://affordabletreese/robots.txt>: DNS lookup failed: no results for hostname lookup: affordabletreese.
Traceback (most recent call last):
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 49, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: affordabletreese.
DEBUG: Crawled (200) <GET https://lamannatreeservice.com/contact/> (referer: https://lamannatreeservice.com/)
DEBUG: Retrying <GET https://barbertontree.com/> (failed 1 times): 429 Unknown Status
DEBUG: Crawled (200) <GET http://www.treeservicedelawareoh.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.delmartreeservices.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://tomcotreecare.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.larochetree.com/robots.txt> (referer: None)
DEBUG: Scraped from <200 https://lamannatreeservice.com/contact/>
{'emails': [],
 'facebook': 'https://www.facebook.com/lamannatreeservice/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://www.delmartreeservices.com/> (referer: None)
DEBUG: Retrying <GET https://barbertontree.com/> (failed 2 times): 429 Unknown Status
INFO: Ignoring response <403 http://tomcotreecare.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (403) <GET http://www.dolcestreeservice.com/robots.txt> (referer: None)
ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\engine.py", line 152, in _next_request
    request = next(self.slot.start_requests)
  File "C:\Users\moffi\PycharmProjects\scraping_web\scraper\scraper\spiders\website_spider.py", line 40, in start_requests
    yield scrapy.Request(url=url, callback=self.parse_website)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\http\request\__init__.py", line 60, in __init__
    self._set_url(url)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\http\request\__init__.py", line 108, in _set_url
    raise ValueError(f'Missing scheme in request url: {self._url}')
ValueError: Missing scheme in request url: 
DEBUG: Retrying <GET https://premiertreesllc.com/contact-us/> (failed 1 times): 429 Unknown Status
DEBUG: Crawled (200) <GET http://www.treeservicedelawareoh.com/> (referer: None)
DEBUG: Crawled (403) <GET http://www.dolcestreeservice.com/> (referer: None)
ERROR: Gave up retrying <GET https://barbertontree.com/> (failed 3 times): 429 Unknown Status
DEBUG: Crawled (429) <GET https://barbertontree.com/> (referer: None)
INFO: Ignoring response <403 http://www.dolcestreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://www.larochetree.com/> (referer: None)
DEBUG: Retrying <GET https://premiertreesllc.com/contact-us/> (failed 2 times): 429 Unknown Status
DEBUG: Crawled (200) <GET https://tree-works-ohio.com/> (referer: None)
DEBUG: Crawled (200) <GET https://brushbandittree.com/> (referer: None)
INFO: Ignoring response <429 https://barbertontree.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (404) <GET https://www.napierandson.com/robots.txt> (referer: None)
ERROR: Gave up retrying <GET https://premiertreesllc.com/contact-us/> (failed 3 times): 429 Unknown Status
DEBUG: Crawled (429) <GET https://premiertreesllc.com/contact-us/> (referer: https://premiertreesllc.com/)
DEBUG: Crawled (200) <GET http://www.treeservicedelawareoh.com/contact-us/> (referer: http://www.treeservicedelawareoh.com/)
INFO: Ignoring response <429 https://premiertreesllc.com/contact-us/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET https://treeservicenow.net/robots.txt> from <GET https://www.treeservicenow.net/robots.txt>
DEBUG: Scraped from <200 http://www.treeservicedelawareoh.com/contact-us/>
{'emails': [],
 'facebook': 'https://www.facebook.com/pages/biz/43015/James-Tree-Service/194987980849287/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://www.napierandson.com/> (referer: None)
DEBUG: Crawled (200) <GET https://treeservicecolumbusohio.net/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://tree-works-ohio.com/contact-tree-works-for-all-your-emergency-services-tree-removal-stump-grinding-etc/> (referer: https://tree-works-ohio.com/)
DEBUG: Crawled (200) <GET https://brushbandittree.com/contact-us/> (referer: https://brushbandittree.com/)
DEBUG: Crawled (200) <GET https://treeservicenow.net/robots.txt> (referer: None)
DEBUG: Retrying <GET https://affordabletreese> (failed 1 times): DNS lookup failed: no results for hostname lookup: affordabletreese.
DEBUG: Scraped from <200 https://tree-works-ohio.com/contact-tree-works-for-all-your-emergency-services-tree-removal-stump-grinding-etc/>
{'emails': [], 'facebook': '', 'instagram': '', 'linkedin': '', 'twitter': ''}
DEBUG: Scraped from <200 https://brushbandittree.com/contact-us/>
{'emails': [],
 'facebook': 'https://www.facebook.com/brushbandittree/',
 'instagram': 'https://www.instagram.com/brushband1t/',
 'linkedin': '',
 'twitter': 'https://twitter.com/brushband1t%20'}
DEBUG: Crawled (200) <GET https://treeservicecolumbusohio.net/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.treeservicenow.net/> from <GET http://www.treeservicenow.net/>
DEBUG: Crawled (200) <GET https://www.larochetree.com/contact> (referer: https://www.larochetree.com/)
DEBUG: Redirecting (301) to <GET https://treeservicenow.net/> from <GET https://www.treeservicenow.net/>
DEBUG: Crawled (200) <GET https://treeservicecolumbusohio.net/contact-us/> (referer: https://treeservicecolumbusohio.net/)
DEBUG: Scraped from <200 https://www.larochetree.com/contact>
{'emails': ['605a7baede844d278b89dc95ae0a9123@sentry-next.wixpress.com',
            'dd0a55ccb8124b9c9d938e3acf41f8aa@sentry.wixpress.com',
            'contactus@larochetree.com',
            'ContactUs@larochetree.com',
            '831126cb46b74583bf6f72c5061cba9d@sentry-viewer.wixpress.com'],
 'facebook': 'https://www.facebook.com/LaRocheTree',
 'instagram': 'https://www.instagram.com/LaRochetree/',
 'linkedin': 'https://www.linkedin.com/company/laroche-tree-service-inc',
 'twitter': 'https://twitter.com/larochetree'}
DEBUG: Scraped from <200 https://treeservicecolumbusohio.net/contact-us/>
{'emails': [], 'facebook': '', 'instagram': '', 'linkedin': '', 'twitter': ''}
DEBUG: Crawled (200) <GET https://treeservicenow.net/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://treeservicenow.net/> (referer: None)
DEBUG: Crawled (200) <GET https://treeservicenow.net/contact_us_1> (referer: https://treeservicenow.net/)
DEBUG: Scraped from <200 https://treeservicenow.net/contact_us_1>
{'emails': [], 'facebook': '', 'instagram': '', 'linkedin': '', 'twitter': ''}
DEBUG: Retrying <GET https://affordabletreese> (failed 2 times): DNS lookup failed: no results for hostname lookup: affordabletreese.
DEBUG: Crawled (200) <GET https://challengerstreeservice.com/> (referer: None)
ERROR: Gave up retrying <GET https://affordabletreese> (failed 3 times): DNS lookup failed: no results for hostname lookup: affordabletreese.
ERROR: Error downloading <GET https://affordabletreese>
Traceback (most recent call last):
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\twisted\internet\defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\twisted\python\failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 49, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\twisted\internet\defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\twisted\internet\endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: affordabletreese.
INFO: Closing spider (finished)
INFO: Stored json feed (26 items) in: website_spider_output.json
INFO: Dumping Scrapy stats:
{'downloader/exception_count': 6,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 6,
 'downloader/request_bytes': 66008,
 'downloader/request_count': 251,
 'downloader/request_method_count/GET': 251,
 'downloader/response_bytes': 3505479,
 'downloader/response_count': 245,
 'downloader/response_status_count/200': 145,
 'downloader/response_status_count/301': 55,
 'downloader/response_status_count/302': 1,
 'downloader/response_status_count/403': 29,
 'downloader/response_status_count/404': 5,
 'downloader/response_status_count/429': 10,
 'dupefilter/filtered': 61,
 'elapsed_time_seconds': 12.250735,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 9, 12, 28, 17, 11523),
 'httpcompression/response_bytes': 14909707,
 'httpcompression/response_count': 156,
 'httperror/response_ignored_count': 16,
 'httperror/response_ignored_status_count/403': 14,
 'httperror/response_ignored_status_count/429': 2,
 'item_scraped_count': 26,
 'log_count/DEBUG': 495,
 'log_count/ERROR': 8,
 'log_count/INFO': 27,
 'request_depth_max': 1,
 'response_received_count': 182,
 'retry/count': 11,
 'retry/max_reached': 5,
 'retry/reason_count/429 Unknown Status': 7,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 4,
 "robotstxt/exception_count/<class 'twisted.internet.error.DNSLookupError'>": 1,
 'robotstxt/request_count': 85,
 'robotstxt/response_count': 84,
 'robotstxt/response_status_count/200': 63,
 'robotstxt/response_status_count/403': 15,
 'robotstxt/response_status_count/404': 5,
 'robotstxt/response_status_count/429': 1,
 'scheduler/dequeued': 140,
 'scheduler/dequeued/memory': 140,
 'scheduler/enqueued': 140,
 'scheduler/enqueued/memory': 140,
 'start_time': datetime.datetime(2022, 12, 9, 12, 28, 4, 760788)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: 38c9d0b51920e232
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
DEBUG: Attempting to acquire lock 1799154118816 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 1799154118816 acquired on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Attempting to release lock 1799154118816 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 1799154118816 released on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Crawled (200) <GET http://www.columbustreeservicesllc.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.fandftrees.com/robots.txt> from <GET http://www.fandftrees.com/robots.txt>
DEBUG: Crawled (403) <GET https://trapperstreeservice.com/robots.txt> (referer: None)
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Rule at line 14 without any user agent to enforce it on.
DEBUG: Rule at line 16 without any user agent to enforce it on.
DEBUG: Rule at line 35 without any user agent to enforce it on.
DEBUG: Rule at line 42 without any user agent to enforce it on.
DEBUG: Rule at line 43 without any user agent to enforce it on.
DEBUG: Rule at line 44 without any user agent to enforce it on.
DEBUG: Rule at line 45 without any user agent to enforce it on.
DEBUG: Rule at line 46 without any user agent to enforce it on.
DEBUG: Rule at line 47 without any user agent to enforce it on.
DEBUG: Rule at line 69 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET http://www.columbustreeservicesllc.com/> (referer: None)
DEBUG: Crawled (403) <GET https://www.midohiotree.org/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET https://trapperstreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.fandftrees.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.treetechohio.com/robots.txt> (referer: None)
DEBUG: Filtered duplicate request: <GET http://hardwicktreecare.com/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG: Crawled (200) <GET http://www.charteroakscompany.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.fandftrees.com/> from <GET http://www.fandftrees.com/>
INFO: Ignoring response <403 https://trapperstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (403) <GET https://www.midohiotree.org/> (referer: None)
DEBUG: Crawled (404) <GET http://ohiotreeandexcavating.net/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.fandftrees.com/> (referer: None)
DEBUG: Crawled (200) <GET http://hardwicktreecare.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.treetechohio.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.hardwicktreecare.com/> from <GET http://hardwicktreecare.com/>
INFO: Ignoring response <403 https://www.midohiotree.org/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET https://starwoodtree.com/robots.txt> from <GET http://starwoodtree.com/robots.txt>
DEBUG: Redirecting (301) to <GET http://ohiotreeandexcavating.com/> from <GET http://ohiotreeandexcavating.net/>
DEBUG: Redirecting (301) to <GET https://ohiotreeandexcavating.com/robots.txt> from <GET http://ohiotreeandexcavating.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.russelltreeexperts.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://www.treesaremybusiness.com/robots.txt> (referer: None)
DEBUG: Crawled (404) <GET http://ohiotreecare.com/robots.txt> (referer: None)
DEBUG: Rule at line 1 without any user agent to enforce it on.
DEBUG: Rule at line 2 without any user agent to enforce it on.
DEBUG: Rule at line 8 without any user agent to enforce it on.
DEBUG: Rule at line 9 without any user agent to enforce it on.
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Rule at line 11 without any user agent to enforce it on.
DEBUG: Rule at line 12 without any user agent to enforce it on.
DEBUG: Crawled (403) <GET https://www.tackettstreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://ohiotreeandexcavating.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.treetechohio.com/contact> (referer: https://www.treetechohio.com/)
DEBUG: Redirecting (301) to <GET https://ohiotreeandexcavating.com/> from <GET http://ohiotreeandexcavating.com/>
DEBUG: Crawled (200) <GET https://www.hardwicktreecare.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET https://www.tackettstreeservice.com/> (referer: None)
DEBUG: Scraped from <200 https://www.treetechohio.com/contact>
{'emails': ['605a7baede844d278b89dc95ae0a9123@sentry-next.wixpress.com',
            '8eb368c655b84e029ed79ad7a5c1718e@sentry.wixpress.com',
            'info@mysite.com',
            '831126cb46b74583bf6f72c5061cba9d@sentry-viewer.wixpress.com'],
 'facebook': 'http://www.facebook.com/wix',
 'instagram': '',
 'linkedin': '',
 'twitter': 'http://www.twitter.com/wix'}
DEBUG: Crawled (403) <GET http://www.hackettstreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.treesaremybusiness.com/> from <GET http://www.treesaremybusiness.com/>
DEBUG: Crawled (200) <GET https://www.hardwicktreecare.com/> (referer: None)
INFO: Ignoring response <403 https://www.tackettstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (403) <GET http://www.hackettstreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/akron/?utm_source=GMB&utm_medium=organic&utm_campaign=doylestown> (referer: None)
DEBUG: Crawled (200) <GET https://www.russelltreeexperts.com/> (referer: None)
INFO: Ignoring response <403 http://www.hackettstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (403) <GET http://deeprootedtreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.charteroakscompany.com/> from <GET http://www.charteroakscompany.com/>
DEBUG: Crawled (403) <GET http://deeprootedtreeservice.com/> (referer: None)
DEBUG: Redirecting (301) to <GET http://www.timberlandtreeohio.com/robots.txt> from <GET http://timberlandtreeohio.com/robots.txt>
INFO: Ignoring response <403 http://deeprootedtreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://specialtytreeohio.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://ohiotreeandexcavating.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.hardwicktreecare.com/contact-hardwick-tree-care> (referer: https://www.hardwicktreecare.com/)
DEBUG: Redirecting (301) to <GET https://treesaremybusiness.com/> from <GET https://www.treesaremybusiness.com/>
DEBUG: Scraped from <200 https://www.hardwicktreecare.com/contact-hardwick-tree-care>
{'emails': ['605a7baede844d278b89dc95ae0a9123@sentry-next.wixpress.com',
            '8eb368c655b84e029ed79ad7a5c1718e@sentry.wixpress.com',
            'f1ffc0b5efe04e9eb9762cd808722520@sentry.wixpress.com',
            'info@HardwickTreeCare.com',
            '831126cb46b74583bf6f72c5061cba9d@sentry-viewer.wixpress.com'],
 'facebook': 'https://www.facebook.com/hardwicktreecarellc',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://www.independenttree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://www.timberlandtreeohio.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://herculestree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://starwoodtree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.whymonster.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://specialtytreeohio.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.charteroakscompany.com/> (referer: None)
DEBUG: Redirecting (301) to <GET http://www.timberlandtreeohio.com/> from <GET http://timberlandtreeohio.com/>
DEBUG: Redirecting (301) to <GET https://www.monstertreeservice.com/akron/contact-us/> from <GET https://www.whymonster.com/akron/contact-us/>
DEBUG: Redirecting (301) to <GET https://starwoodtree.com/> from <GET http://starwoodtree.com/>
DEBUG: Crawled (403) <GET http://cottstrees.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/akron/contact-us/> (referer: https://www.monstertreeservice.com/akron/?utm_source=GMB&utm_medium=organic&utm_campaign=doylestown)
DEBUG: Crawled (200) <GET http://www.timberlandtreeohio.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://jstreeservicesllc.com/robots.txt> from <GET http://www.jstreeservicesllc.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://herculestree.com/> from <GET http://herculestree.com/>
DEBUG: Crawled (403) <GET http://cottstrees.com/> (referer: None)
DEBUG: Crawled (200) <GET https://treesaremybusiness.com/robots.txt> (referer: None)
DEBUG: Scraped from <200 https://www.monstertreeservice.com/akron/contact-us/>
{'emails': [],
 'facebook': 'https://www.facebook.com/MonsterTreeServiceofAkron/',
 'instagram': 'https://www.instagram.com/monstertreeservices/',
 'linkedin': '',
 'twitter': ''}
INFO: Ignoring response <403 http://cottstrees.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://www.independenttree.com/?utm_source=gmb&utm_medium=local&utm_campaign=website> (referer: None)
DEBUG: Crawled (200) <GET https://www.ewsmithtree.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://roquetree.com/robots.txt> from <GET http://roquetree.com/robots.txt>
DEBUG: Crawled (200) <GET http://ohiotreecare.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.basictreecare.com/robots.txt> from <GET http://www.basictreecare.com/robots.txt>
DEBUG: Crawled (200) <GET https://treesaremybusiness.com/> (referer: None)
DEBUG: Redirecting (301) to <GET http://haneytreeservice.com/robots.txt> from <GET http://www.haneytreeservice.com/robots.txt>
DEBUG: Crawled (200) <GET http://jstreeservicesllc.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.independenttree.com/contact-us/> (referer: https://www.independenttree.com/?utm_source=gmb&utm_medium=local&utm_campaign=website)
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/cleveland/?utm_source=GMB&utm_medium=organic&utm_campaign=AvonLake> (referer: None)
DEBUG: Crawled (200) <GET https://www.ewsmithtree.com/> (referer: None)
DEBUG: Crawled (200) <GET http://www.kiddertreeservice.com/robots.txt> (referer: None)
DEBUG: Scraped from <200 https://www.independenttree.com/contact-us/>
{'emails': ['info@independenttree.com'],
 'facebook': 'https://www.facebook.com/IndependentTreeOH/',
 'instagram': 'https://www.instagram.com/independenttreeservice/',
 'linkedin': '',
 'twitter': ''}
DEBUG: Redirecting (301) to <GET https://www.kiddertreemov.com/> from <GET http://www.kiddertreeservice.com/>
DEBUG: Crawled (200) <GET https://treesaremybusiness.com/contact-us/> (referer: https://treesaremybusiness.com/)
DEBUG: Crawled (200) <GET https://www.basictreecare.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://haneytreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://herculestree.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://jstreeservicesllc.com/> from <GET http://www.jstreeservicesllc.com/>
DEBUG: Crawled (200) <GET https://roquetree.com/robots.txt> (referer: None)
DEBUG: Scraped from <200 https://treesaremybusiness.com/contact-us/>
{'emails': ['millcraft@treesaremybusiness.com',
            'office@treesaremybusiness.com'],
 'facebook': 'https://www.facebook.com/treesaremybusinessohio/',
 'instagram': 'https://www.instagram.com/trees_are_my_business/',
 'linkedin': '',
 'twitter': ''}
DEBUG: Redirecting (301) to <GET https://www.basictreecare.com/> from <GET http://www.basictreecare.com/>
DEBUG: Crawled (200) <GET http://www.timberlandtreeohio.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.kiddertreemov.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://haneytreeservice.com/> from <GET http://www.haneytreeservice.com/>
DEBUG: Redirecting (301) to <GET https://www.jttreeservicellc.com/robots.txt> from <GET http://www.jttreeservicellc.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://roquetree.com/> from <GET http://roquetree.com/>
DEBUG: Crawled (200) <GET http://haneytreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://herculestree.com/contact> (referer: https://herculestree.com/)
DEBUG: Crawled (200) <GET https://www.kiddertreemov.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.ewsmithtree.com/contact-us/> (referer: https://www.ewsmithtree.com/)
DEBUG: Crawled (200) <GET https://www.basictreecare.com/> (referer: None)
DEBUG: Scraped from <200 https://herculestree.com/contact>
{'emails': ['Herculestree@gmail.com', 'HerculesTree@gmail.com'],
 'facebook': 'https://www.facebook.com/HerculesTreeLLC/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Scraped from <200 https://www.ewsmithtree.com/contact-us/>
{'emails': [], 'facebook': '', 'instagram': '', 'linkedin': '', 'twitter': ''}
DEBUG: Redirecting (301) to <GET http://haneytreeservice.com/index.html> from <GET http://haneytreeservice.com/>
DEBUG: Crawled (403) <GET https://www.kandatreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://challengerstreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.roquetree.com/> from <GET https://roquetree.com/>
DEBUG: Crawled (403) <GET https://www.kandatreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://herculestree.com/contact/request-quote/> (referer: https://herculestree.com/)
INFO: Ignoring response <403 https://www.kandatreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Scraped from <200 https://herculestree.com/contact/request-quote/>
{'emails': ['info@herculesTrees.com', 'HerculesTree@gmail.com'],
 'facebook': 'https://www.facebook.com/HerculesTreeLLC/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://www.davey.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://www.jjsfamilytreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.ripleytreeservice.com/robots.txt> from <GET http://www.ripleytreeservice.com/robots.txt>
DEBUG: Crawled (403) <GET http://www.jjsfamilytreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET http://haneytreeservice.com/index.html> (referer: None)
DEBUG: Crawled (200) <GET https://jstreeservicesllc.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://starwoodtree.com/> (referer: None)
INFO: Ignoring response <403 http://www.jjsfamilytreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://www.jttreeservicellc.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://herculestree.com/contact/request-quote> (referer: https://herculestree.com/)
DEBUG: Scraped from <200 https://herculestree.com/contact/request-quote>
{'emails': ['info@herculesTrees.com', 'HerculesTree@gmail.com'],
 'facebook': 'https://www.facebook.com/HerculesTreeLLC/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://parkstree.net/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.jttreeservicellc.com/> from <GET http://www.jttreeservicellc.com/>
DEBUG: Crawled (200) <GET https://www.roquetree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://parkstree.net/> (referer: None)
DEBUG: Redirecting (301) to <GET https://extremetreeservicestoledo.com/robots.txt> from <GET https://www.extremetreeservicestoledo.com/robots.txt>
DEBUG: Crawled (200) <GET https://herculestree.com/contact/> (referer: https://herculestree.com/)
DEBUG: Crawled (200) <GET https://www.rawtreeserv.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.davey.com/residential-tree-services/local-offices/columbus-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Columbus> (referer: None)
DEBUG: Crawled (200) <GET https://oak-tree-services-tree-service.business.site/robots.txt> (referer: None)
DEBUG: Scraped from <200 https://herculestree.com/contact/>
{'emails': ['Herculestree@gmail.com', 'HerculesTree@gmail.com'],
 'facebook': 'https://www.facebook.com/HerculesTreeLLC/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://www.savatree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.jttreeservicellc.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.roquetree.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.ripleytreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://rawtreeserv.com/> from <GET https://www.rawtreeserv.com/>
DEBUG: Crawled (200) <GET https://jstreeservicesllc.com/> (referer: None)
DEBUG: Crawled (200) <GET https://parkstree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://oak-tree-services-tree-service.business.site/?utm_source=gmb&utm_medium=referral> (referer: None)
DEBUG: Crawled (200) <GET https://parkstree.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.ripleytreeservice.com/> from <GET http://www.ripleytreeservice.com/>
DEBUG: Crawled (200) <GET https://www.savatree.com/dayton-ohio-tree-service-lawn-care?utm_source=GMB&utm_medium=organic&utm_campaign=dayton> (referer: None)
DEBUG: Crawled (200) <GET https://extremetreeservicestoledo.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://extremetreeservicestoledo.com/> from <GET https://www.extremetreeservicestoledo.com/>
DEBUG: Crawled (200) <GET https://www.ripleytreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.davey.com/residential-tree-services/local-offices/cincinnati-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Cincinnati> (referer: None)
DEBUG: Crawled (200) <GET https://extremetreeservicestoledo.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://rawtreeserv.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.stevestoledotree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://blackstreemov.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://extremetreeservicestoledo.com/> (referer: None)
DEBUG: Crawled (403) <GET http://www.haymakertreeandlawn.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.blackstreemov.com/> from <GET http://blackstreemov.com/>
DEBUG: Crawled (200) <GET https://rawtreeserv.com/> (referer: None)
DEBUG: Crawled (403) <GET http://www.haymakertreeandlawn.com/> (referer: None)
DEBUG: Crawled (200) <GET https://challengerstreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://affordabletreeservicesofohiollc.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://starwoodtree.com/contact-us/> (referer: https://starwoodtree.com/)
DEBUG: Redirecting (301) to <GET http://jstreeservicesllc.com/contact-us/> from <GET http://www.jstreeservicesllc.com/contact-us>
DEBUG: Crawled (403) <GET http://www.cstreemulch.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.blackstreemov.com/robots.txt> (referer: None)
INFO: Ignoring response <403 http://www.haymakertreeandlawn.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://affordabletreeservicesofohiollc.com/> (referer: None)
DEBUG: Crawled (403) <GET http://www.cstreemulch.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.stevestoledotree.com/?utm_source=googlelocal&utm_medium=organic> (referer: None)
DEBUG: Crawled (200) <GET https://jstreeservicesllc.com/contact-us/> (referer: https://jstreeservicesllc.com/)
DEBUG: Scraped from <200 https://starwoodtree.com/contact-us/>
{'emails': ['support@starwoodtree.com'],
 'facebook': 'https://www.facebook.com/Starwoodtreeservice/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://www.blackstreemov.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.davey.com/about/contact-us/?type=residential> (referer: https://www.davey.com/residential-tree-services/local-offices/columbus-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Columbus)
INFO: Ignoring response <403 http://www.cstreemulch.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://extremetreeservicestoledo.com/contact/> (referer: https://extremetreeservicestoledo.com/)
DEBUG: Scraped from <200 https://jstreeservicesllc.com/contact-us/>
{'emails': ['j_s.tree@yahoo.com'],
 'facebook': 'http://www.facebook.com/jandstreeservices',
 'instagram': 'http://www.instagram.com/jandstreeservices',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (403) <GET http://www.ashtreeservicepro.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://treesrus-treeservice.business.site/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://www.ashtreeservicepro.com/> (referer: None)
DEBUG: Scraped from <200 https://www.davey.com/about/contact-us/?type=residential>
{'emails': [],
 'facebook': 'https://www.facebook.com/DaveyTree',
 'instagram': 'https://instagram.com/DaveyTree/',
 'linkedin': 'http://www.linkedin.com/company/the-davey-tree-expert-company',
 'twitter': 'https://twitter.com/DaveyTree'}
DEBUG: Crawled (200) <GET https://www.davey.com/residential-tree-services/local-offices/northeast-cleveland-tree-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Northeast%20Cleveland> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.roguetreesolutions.com/robots.txt> from <GET http://www.roguetreesolutions.com/robots.txt>
INFO: Ignoring response <403 http://www.ashtreeservicepro.com/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET http://www.toddstreeservice.com/robots.txt> from <GET http://toddstreeservice.com/robots.txt>
DEBUG: Scraped from <200 https://extremetreeservicestoledo.com/contact/>
{'emails': ['extremetreeswanton@gmail.com'],
 'facebook': 'https://www.facebook.com/ToledosExtremeTree/',
 'instagram': '',
 'linkedin': '',
 'twitter': 'https://twitter.com/SuperClimber101'}
DEBUG: Crawled (200) <GET https://treesrus-treeservice.business.site/?utm_source=gmb&utm_medium=referral> (referer: None)
DEBUG: Crawled (200) <GET https://extremetreeohio.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://jstreeservicesllc.com/contact-us/> (referer: None)
DEBUG: Redirecting (301) to <GET https://hoffmantreeservice.com/robots.txt> from <GET http://hoffmantreeservice.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.blackstreemov.com/contact> (referer: https://www.blackstreemov.com/)
DEBUG: Crawled (200) <GET http://urbanloggersohio.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://extremetreeohio.com/> (referer: None)
DEBUG: Scraped from <200 http://jstreeservicesllc.com/contact-us/>
{'emails': ['j_s.tree@yahoo.com'],
 'facebook': 'http://www.facebook.com/jandstreeservices',
 'instagram': 'http://www.instagram.com/jandstreeservices',
 'linkedin': '',
 'twitter': ''}
DEBUG: Scraped from <200 https://www.blackstreemov.com/contact>
{'emails': ['605a7baede844d278b89dc95ae0a9123@sentry-next.wixpress.com',
            '831126cb46b74583bf6f72c5061cba9d@sentry-viewer.wixpress.com',
            'BlacksTreeService140@gmail.com',
            'blackstreeservice140@gmail.com'],
 'facebook': 'https://www.facebook.com/pages/Blacks-Tree-Service/205784306214398',
 'instagram': 'http://instagram.com/wix',
 'linkedin': '',
 'twitter': 'https://twitter.com/BlacksTreeMOV'}
DEBUG: Redirecting (301) to <GET https://urbanloggersohio.com/> from <GET http://urbanloggersohio.com/>
DEBUG: Crawled (403) <GET http://www.toddstreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (404) <GET https://www.roguetreesolutions.com/robots.txt> (referer: None)
DEBUG: Rule at line 1 without any user agent to enforce it on.
DEBUG: Rule at line 2 without any user agent to enforce it on.
DEBUG: Rule at line 14 without any user agent to enforce it on.
DEBUG: Rule at line 15 without any user agent to enforce it on.
DEBUG: Rule at line 23 without any user agent to enforce it on.
DEBUG: Rule at line 28 without any user agent to enforce it on.
DEBUG: Rule at line 29 without any user agent to enforce it on.
DEBUG: Rule at line 32 without any user agent to enforce it on.
DEBUG: Rule at line 33 without any user agent to enforce it on.
DEBUG: Rule at line 45 without any user agent to enforce it on.
DEBUG: Rule at line 46 without any user agent to enforce it on.
DEBUG: Rule at line 47 without any user agent to enforce it on.
DEBUG: Rule at line 48 without any user agent to enforce it on.
DEBUG: Rule at line 49 without any user agent to enforce it on.
DEBUG: Rule at line 50 without any user agent to enforce it on.
DEBUG: Rule at line 51 without any user agent to enforce it on.
DEBUG: Rule at line 52 without any user agent to enforce it on.
DEBUG: Rule at line 53 without any user agent to enforce it on.
DEBUG: Rule at line 54 without any user agent to enforce it on.
DEBUG: Rule at line 55 without any user agent to enforce it on.
DEBUG: Rule at line 57 without any user agent to enforce it on.
DEBUG: Rule at line 58 without any user agent to enforce it on.
DEBUG: Rule at line 59 without any user agent to enforce it on.
DEBUG: Rule at line 61 without any user agent to enforce it on.
DEBUG: Rule at line 62 without any user agent to enforce it on.
DEBUG: Rule at line 63 without any user agent to enforce it on.
DEBUG: Rule at line 64 without any user agent to enforce it on.
DEBUG: Rule at line 65 without any user agent to enforce it on.
DEBUG: Rule at line 66 without any user agent to enforce it on.
DEBUG: Rule at line 67 without any user agent to enforce it on.
DEBUG: Rule at line 70 without any user agent to enforce it on.
DEBUG: Rule at line 72 without any user agent to enforce it on.
DEBUG: Rule at line 78 without any user agent to enforce it on.
DEBUG: Rule at line 79 without any user agent to enforce it on.
DEBUG: Rule at line 80 without any user agent to enforce it on.
DEBUG: Rule at line 81 without any user agent to enforce it on.
DEBUG: Rule at line 87 without any user agent to enforce it on.
DEBUG: Rule at line 88 without any user agent to enforce it on.
DEBUG: Rule at line 92 without any user agent to enforce it on.
DEBUG: Rule at line 94 without any user agent to enforce it on.
DEBUG: Rule at line 96 without any user agent to enforce it on.
DEBUG: Rule at line 98 without any user agent to enforce it on.
DEBUG: Rule at line 100 without any user agent to enforce it on.
DEBUG: Rule at line 101 without any user agent to enforce it on.
DEBUG: Rule at line 104 without any user agent to enforce it on.
DEBUG: Rule at line 106 without any user agent to enforce it on.
DEBUG: Rule at line 107 without any user agent to enforce it on.
DEBUG: Rule at line 108 without any user agent to enforce it on.
DEBUG: Rule at line 109 without any user agent to enforce it on.
DEBUG: Rule at line 110 without any user agent to enforce it on.
DEBUG: Rule at line 111 without any user agent to enforce it on.
DEBUG: Rule at line 112 without any user agent to enforce it on.
DEBUG: Rule at line 119 without any user agent to enforce it on.
DEBUG: Rule at line 120 without any user agent to enforce it on.
DEBUG: Rule at line 121 without any user agent to enforce it on.
DEBUG: Rule at line 122 without any user agent to enforce it on.
DEBUG: Rule at line 123 without any user agent to enforce it on.
DEBUG: Rule at line 128 without any user agent to enforce it on.
DEBUG: Rule at line 145 without any user agent to enforce it on.
DEBUG: Rule at line 155 without any user agent to enforce it on.
DEBUG: Rule at line 165 without any user agent to enforce it on.
DEBUG: Redirecting (301) to <GET https://www.roguetreesolutions.com/> from <GET http://www.roguetreesolutions.com/>
DEBUG: Crawled (200) <GET http://rstreeservicellc.com/robots.txt> (referer: None)
DEBUG: Rule at line 1 without any user agent to enforce it on.
DEBUG: Rule at line 5 without any user agent to enforce it on.
DEBUG: Rule at line 9 without any user agent to enforce it on.
DEBUG: Rule at line 14 without any user agent to enforce it on.
DEBUG: Rule at line 19 without any user agent to enforce it on.
DEBUG: Rule at line 21 without any user agent to enforce it on.
DEBUG: Rule at line 22 without any user agent to enforce it on.
DEBUG: Rule at line 23 without any user agent to enforce it on.
DEBUG: Rule at line 26 without any user agent to enforce it on.
DEBUG: Rule at line 28 without any user agent to enforce it on.
DEBUG: Rule at line 29 without any user agent to enforce it on.
DEBUG: Rule at line 31 without any user agent to enforce it on.
DEBUG: Rule at line 32 without any user agent to enforce it on.
DEBUG: Rule at line 33 without any user agent to enforce it on.
DEBUG: Rule at line 34 without any user agent to enforce it on.
DEBUG: Rule at line 35 without any user agent to enforce it on.
DEBUG: Rule at line 36 without any user agent to enforce it on.
DEBUG: Rule at line 37 without any user agent to enforce it on.
DEBUG: Rule at line 39 without any user agent to enforce it on.
DEBUG: Rule at line 40 without any user agent to enforce it on.
DEBUG: Rule at line 43 without any user agent to enforce it on.
DEBUG: Rule at line 44 without any user agent to enforce it on.
DEBUG: Rule at line 46 without any user agent to enforce it on.
DEBUG: Rule at line 48 without any user agent to enforce it on.
DEBUG: Rule at line 51 without any user agent to enforce it on.
DEBUG: Rule at line 53 without any user agent to enforce it on.
DEBUG: Rule at line 54 without any user agent to enforce it on.
DEBUG: Rule at line 55 without any user agent to enforce it on.
DEBUG: Rule at line 56 without any user agent to enforce it on.
DEBUG: Rule at line 57 without any user agent to enforce it on.
DEBUG: Rule at line 58 without any user agent to enforce it on.
DEBUG: Rule at line 60 without any user agent to enforce it on.
DEBUG: Rule at line 61 without any user agent to enforce it on.
DEBUG: Rule at line 62 without any user agent to enforce it on.
DEBUG: Rule at line 63 without any user agent to enforce it on.
DEBUG: Rule at line 64 without any user agent to enforce it on.
DEBUG: Rule at line 65 without any user agent to enforce it on.
DEBUG: Rule at line 66 without any user agent to enforce it on.
DEBUG: Rule at line 67 without any user agent to enforce it on.
DEBUG: Rule at line 68 without any user agent to enforce it on.
DEBUG: Rule at line 69 without any user agent to enforce it on.
DEBUG: Rule at line 70 without any user agent to enforce it on.
DEBUG: Rule at line 71 without any user agent to enforce it on.
DEBUG: Rule at line 72 without any user agent to enforce it on.
DEBUG: Rule at line 73 without any user agent to enforce it on.
DEBUG: Rule at line 74 without any user agent to enforce it on.
DEBUG: Rule at line 75 without any user agent to enforce it on.
DEBUG: Rule at line 76 without any user agent to enforce it on.
DEBUG: Rule at line 77 without any user agent to enforce it on.
DEBUG: Rule at line 78 without any user agent to enforce it on.
DEBUG: Rule at line 79 without any user agent to enforce it on.
DEBUG: Rule at line 80 without any user agent to enforce it on.
DEBUG: Rule at line 81 without any user agent to enforce it on.
DEBUG: Rule at line 83 without any user agent to enforce it on.
DEBUG: Rule at line 85 without any user agent to enforce it on.
DEBUG: Rule at line 86 without any user agent to enforce it on.
DEBUG: Rule at line 87 without any user agent to enforce it on.
DEBUG: Rule at line 88 without any user agent to enforce it on.
DEBUG: Rule at line 89 without any user agent to enforce it on.
DEBUG: Rule at line 90 without any user agent to enforce it on.
DEBUG: Rule at line 91 without any user agent to enforce it on.
DEBUG: Rule at line 92 without any user agent to enforce it on.
DEBUG: Rule at line 93 without any user agent to enforce it on.
DEBUG: Rule at line 94 without any user agent to enforce it on.
DEBUG: Rule at line 95 without any user agent to enforce it on.
DEBUG: Rule at line 96 without any user agent to enforce it on.
DEBUG: Rule at line 98 without any user agent to enforce it on.
DEBUG: Rule at line 99 without any user agent to enforce it on.
DEBUG: Rule at line 100 without any user agent to enforce it on.
DEBUG: Rule at line 101 without any user agent to enforce it on.
DEBUG: Rule at line 102 without any user agent to enforce it on.
DEBUG: Rule at line 103 without any user agent to enforce it on.
DEBUG: Rule at line 104 without any user agent to enforce it on.
DEBUG: Rule at line 106 without any user agent to enforce it on.
DEBUG: Rule at line 107 without any user agent to enforce it on.
DEBUG: Rule at line 108 without any user agent to enforce it on.
DEBUG: Rule at line 110 without any user agent to enforce it on.
DEBUG: Rule at line 112 without any user agent to enforce it on.
DEBUG: Rule at line 113 without any user agent to enforce it on.
DEBUG: Rule at line 114 without any user agent to enforce it on.
DEBUG: Rule at line 115 without any user agent to enforce it on.
DEBUG: Rule at line 116 without any user agent to enforce it on.
DEBUG: Rule at line 117 without any user agent to enforce it on.
DEBUG: Rule at line 121 without any user agent to enforce it on.
DEBUG: Rule at line 122 without any user agent to enforce it on.
DEBUG: Redirecting (301) to <GET http://www.toddstreeservice.com/> from <GET http://toddstreeservice.com/>
DEBUG: Crawled (200) <GET https://hoffmantreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (404) <GET http://www.boonestreeservice.com/robots.txt> (referer: None)
DEBUG: Rule at line 2 without any user agent to enforce it on.
DEBUG: Rule at line 8 without any user agent to enforce it on.
DEBUG: Rule at line 9 without any user agent to enforce it on.
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://www.roguetreesolutions.com/> (referer: None)
DEBUG: Crawled (403) <GET http://www.toddstreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://hoffmantreeservice.com/> from <GET http://hoffmantreeservice.com/>
DEBUG: Crawled (200) <GET http://rstreeservicellc.com/?utm_source=gmb&utm_medium=referral> (referer: None)
DEBUG: Crawled (200) <GET http://www.treeservicedelawareoh.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://www.boonestreeservice.com/> (referer: None)
DEBUG: Crawled (403) <GET http://www.toddstreeservice.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://barbertontree.com/robots.txt> from <GET https://www.barbertontree.com/robots.txt>
DEBUG: Redirecting (302) to <GET https://www.greatdanetreeexperts.com/404.html> from <GET https://www.greatdanetreeexperts.com/robots.txt>
DEBUG: Crawled (200) <GET https://hoffmantreeservice.com/> (referer: None)
INFO: Ignoring response <403 http://www.toddstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET http://www.treeservicedelawareoh.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.davey.com/about/contact-us/?type=nonresidential> (referer: https://www.davey.com/residential-tree-services/local-offices/columbus-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Columbus)
DEBUG: Crawled (200) <GET https://www.greatdanetreeexperts.com/404.html> (referer: None)
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Rule at line 11 without any user agent to enforce it on.
DEBUG: Rule at line 12 without any user agent to enforce it on.
DEBUG: Rule at line 15 without any user agent to enforce it on.
DEBUG: Rule at line 17 without any user agent to enforce it on.
DEBUG: Rule at line 28 without any user agent to enforce it on.
DEBUG: Rule at line 43 without any user agent to enforce it on.
DEBUG: Rule at line 44 without any user agent to enforce it on.
DEBUG: Rule at line 45 without any user agent to enforce it on.
DEBUG: Rule at line 46 without any user agent to enforce it on.
DEBUG: Rule at line 47 without any user agent to enforce it on.
DEBUG: Rule at line 48 without any user agent to enforce it on.
DEBUG: Rule at line 49 without any user agent to enforce it on.
DEBUG: Rule at line 50 without any user agent to enforce it on.
DEBUG: Rule at line 54 without any user agent to enforce it on.
DEBUG: Rule at line 57 without any user agent to enforce it on.
DEBUG: Rule at line 62 without any user agent to enforce it on.
DEBUG: Rule at line 63 without any user agent to enforce it on.
DEBUG: Rule at line 67 without any user agent to enforce it on.
DEBUG: Rule at line 88 without any user agent to enforce it on.
DEBUG: Rule at line 136 without any user agent to enforce it on.
DEBUG: Rule at line 141 without any user agent to enforce it on.
DEBUG: Rule at line 144 without any user agent to enforce it on.
DEBUG: Rule at line 147 without any user agent to enforce it on.
DEBUG: Rule at line 156 without any user agent to enforce it on.
DEBUG: Rule at line 157 without any user agent to enforce it on.
DEBUG: Rule at line 191 without any user agent to enforce it on.
DEBUG: Redirecting (301) to <GET https://lamannatreeservice.com/robots.txt> from <GET http://www.lamannatreeservice.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://www.treeservicenow.net/robots.txt> from <GET http://www.treeservicenow.net/robots.txt>
DEBUG: Redirecting (301) to <GET https://tree-works-ohio.com/robots.txt> from <GET http://tree-works-ohio.com/robots.txt>
DEBUG: Crawled (200) <GET https://brushbandittree.com/robots.txt> (referer: None)
DEBUG: Scraped from <200 https://www.davey.com/about/contact-us/?type=nonresidential>
{'emails': [],
 'facebook': 'https://www.facebook.com/DaveyTree',
 'instagram': 'https://instagram.com/DaveyTree/',
 'linkedin': 'http://www.linkedin.com/company/the-davey-tree-expert-company',
 'twitter': 'https://twitter.com/DaveyTree'}
DEBUG: Crawled (200) <GET https://premiertreesllc.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://lamannatreeservice.com/?robots=1> from <GET https://lamannatreeservice.com/robots.txt>
DEBUG: Crawled (200) <GET https://lamannatreeservice.com/?robots=1> (referer: None)
DEBUG: Crawled (200) <GET https://urbanloggersohio.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://lamannatreeservice.com/> from <GET http://www.lamannatreeservice.com/>
DEBUG: Redirecting (301) to <GET https://lamannatreeservice.com/?robots=1> from <GET https://lamannatreeservice.com/robots.txt>
DEBUG: Crawled (200) <GET http://www.treeservicedelawareoh.com/contact-us/> (referer: http://www.treeservicedelawareoh.com/)
DEBUG: Crawled (200) <GET https://lamannatreeservice.com/?robots=1> (referer: None)
DEBUG: Retrying <GET https://premiertreesllc.com/> (failed 1 times): 429 Unknown Status
DEBUG: Crawled (200) <GET https://www.davey.com/residential-tree-services/local-offices/akron-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Akron> (referer: None)
DEBUG: Crawled (200) <GET https://lamannatreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://larochetree.com/robots.txt> (referer: None)
DEBUG: Scraped from <200 http://www.treeservicedelawareoh.com/contact-us/>
{'emails': [],
 'facebook': 'https://www.facebook.com/pages/biz/43015/James-Tree-Service/194987980849287/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Retrying <GET https://premiertreesllc.com/> (failed 2 times): 429 Unknown Status
DEBUG: Redirecting (301) to <GET https://www.larochetree.com/> from <GET https://larochetree.com/>
DEBUG: Crawled (200) <GET https://www.delmartreeservices.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://treeservicenow.net/robots.txt> from <GET https://www.treeservicenow.net/robots.txt>
DEBUG: Crawled (200) <GET https://www.delmartreeservices.com/> (referer: None)
DEBUG: Crawled (200) <GET https://brushbandittree.com/> (referer: None)
DEBUG: Crawled (403) <GET http://tomcotreecare.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://barbertontree.com/robots.txt> (referer: None)
DEBUG: Rule at line 3 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://urbanloggersohio.com/contact-urban-loggers/> (referer: https://urbanloggersohio.com/)
DEBUG: Crawled (200) <GET https://lamannatreeservice.com/contact/> (referer: https://lamannatreeservice.com/)
DEBUG: Redirecting (301) to <GET https://barbertontree.com/> from <GET https://www.barbertontree.com/>
DEBUG: Crawled (200) <GET https://www.davey.com/about/contact-us/?type=nonservicerequest> (referer: https://www.davey.com/residential-tree-services/local-offices/columbus-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Columbus)
DEBUG: Crawled (200) <GET https://www.greatdanetreeexperts.com/> (referer: None)
DEBUG: Crawled (200) <GET https://tree-works-ohio.com/robots.txt> (referer: None)
ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\engine.py", line 152, in _next_request
    request = next(self.slot.start_requests)
  File "C:\Users\moffi\PycharmProjects\scraping_web\scraper\scraper\spiders\website_spider.py", line 40, in start_requests
    yield scrapy.Request(url=url, callback=self.parse_website)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\http\request\__init__.py", line 60, in __init__
    self._set_url(url)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\http\request\__init__.py", line 108, in _set_url
    raise ValueError(f'Missing scheme in request url: {self._url}')
ValueError: Missing scheme in request url: 
DEBUG: Crawled (200) <GET https://www.larochetree.com/robots.txt> (referer: None)
DEBUG: Scraped from <200 https://urbanloggersohio.com/contact-urban-loggers/>
{'emails': ['Info@urbanloggersohio.com'],
 'facebook': '',
 'instagram': 'https://www.instagram.com/urbanloggersllc/',
 'linkedin': '',
 'twitter': ''}
DEBUG: Scraped from <200 https://lamannatreeservice.com/contact/>
{'emails': [],
 'facebook': 'https://www.facebook.com/lamannatreeservice/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (403) <GET http://tomcotreecare.com/> (referer: None)
DEBUG: Crawled (403) <GET http://www.dolcestreeservice.com/robots.txt> (referer: None)
DEBUG: Scraped from <200 https://www.davey.com/about/contact-us/?type=nonservicerequest>
{'emails': [],
 'facebook': 'https://www.facebook.com/DaveyTree',
 'instagram': 'https://instagram.com/DaveyTree/',
 'linkedin': 'http://www.linkedin.com/company/the-davey-tree-expert-company',
 'twitter': 'https://twitter.com/DaveyTree'}
DEBUG: Crawled (403) <GET http://www.dolcestreeservice.com/> (referer: None)
DEBUG: Retrying <GET https://barbertontree.com/robots.txt> (failed 1 times): 429 Unknown Status
DEBUG: Crawled (200) <GET https://premiertreesllc.com/> (referer: None)
INFO: Ignoring response <403 http://tomcotreecare.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://www.larochetree.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://tree-works-ohio.com/> from <GET http://tree-works-ohio.com/>
INFO: Ignoring response <403 http://www.dolcestreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://barbertontree.com/robots.txt> (referer: None)
DEBUG: Rule at line 3 without any user agent to enforce it on.
DEBUG: Crawled (404) <GET https://www.napierandson.com/robots.txt> (referer: None)
DEBUG: Retrying <GET https://premiertreesllc.com/contact-us/> (failed 1 times): 429 Unknown Status
DEBUG: Crawled (200) <GET https://treeservicenow.net/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://brushbandittree.com/contact-us/> (referer: https://brushbandittree.com/)
DEBUG: Retrying <GET https://barbertontree.com/> (failed 1 times): 429 Unknown Status
DEBUG: Crawled (200) <GET https://www.larochetree.com/contact> (referer: https://www.larochetree.com/)
DEBUG: Crawled (200) <GET https://www.napierandson.com/> (referer: None)
DEBUG: Scraped from <200 https://brushbandittree.com/contact-us/>
{'emails': [],
 'facebook': 'https://www.facebook.com/brushbandittree/',
 'instagram': 'https://www.instagram.com/brushband1t/',
 'linkedin': '',
 'twitter': 'https://twitter.com/brushband1t%20'}
DEBUG: Retrying <GET https://premiertreesllc.com/contact-us/> (failed 2 times): 429 Unknown Status
DEBUG: Retrying <GET https://barbertontree.com/> (failed 2 times): 429 Unknown Status
DEBUG: Crawled (200) <GET https://treeservicecolumbusohio.net/robots.txt> (referer: None)
DEBUG: Scraped from <200 https://www.larochetree.com/contact>
{'emails': ['605a7baede844d278b89dc95ae0a9123@sentry-next.wixpress.com',
            'contactus@larochetree.com',
            'dd0a55ccb8124b9c9d938e3acf41f8aa@sentry.wixpress.com',
            'ContactUs@larochetree.com',
            '831126cb46b74583bf6f72c5061cba9d@sentry-viewer.wixpress.com'],
 'facebook': 'https://www.facebook.com/LaRocheTree',
 'instagram': 'https://www.instagram.com/LaRochetree/',
 'linkedin': 'https://www.linkedin.com/company/laroche-tree-service-inc',
 'twitter': 'https://twitter.com/larochetree'}
DEBUG: Redirecting (301) to <GET https://www.treeservicenow.net/> from <GET http://www.treeservicenow.net/>
ERROR: Gave up retrying <GET https://barbertontree.com/> (failed 3 times): 429 Unknown Status
DEBUG: Crawled (429) <GET https://barbertontree.com/> (referer: None)
ERROR: Gave up retrying <GET https://premiertreesllc.com/contact-us/> (failed 3 times): 429 Unknown Status
DEBUG: Crawled (429) <GET https://premiertreesllc.com/contact-us/> (referer: https://premiertreesllc.com/)
DEBUG: Crawled (200) <GET https://tree-works-ohio.com/> (referer: None)
INFO: Ignoring response <429 https://barbertontree.com/>: HTTP status code is not handled or not allowed
INFO: Ignoring response <429 https://premiertreesllc.com/contact-us/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://treeservicecolumbusohio.net/> (referer: None)
DEBUG: Crawled (200) <GET https://treeservicecolumbusohio.net/contact-us/> (referer: https://treeservicecolumbusohio.net/)
DEBUG: Crawled (200) <GET https://tree-works-ohio.com/contact-tree-works-for-all-your-emergency-services-tree-removal-stump-grinding-etc/> (referer: https://tree-works-ohio.com/)
DEBUG: Scraped from <200 https://treeservicecolumbusohio.net/contact-us/>
{'emails': [], 'facebook': '', 'instagram': '', 'linkedin': '', 'twitter': ''}
DEBUG: Scraped from <200 https://tree-works-ohio.com/contact-tree-works-for-all-your-emergency-services-tree-removal-stump-grinding-etc/>
{'emails': [], 'facebook': '', 'instagram': '', 'linkedin': '', 'twitter': ''}
DEBUG: Redirecting (301) to <GET https://treeservicenow.net/> from <GET https://www.treeservicenow.net/>
DEBUG: Crawled (200) <GET https://treeservicenow.net/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://treeservicenow.net/> (referer: None)
DEBUG: Crawled (200) <GET https://treeservicenow.net/contact_us_1> (referer: https://treeservicenow.net/)
DEBUG: Scraped from <200 https://treeservicenow.net/contact_us_1>
{'emails': [], 'facebook': '', 'instagram': '', 'linkedin': '', 'twitter': ''}
INFO: Closing spider (finished)
INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 64162,
 'downloader/request_count': 245,
 'downloader/request_method_count/GET': 245,
 'downloader/response_bytes': 3503572,
 'downloader/response_count': 245,
 'downloader/response_status_count/200': 146,
 'downloader/response_status_count/301': 55,
 'downloader/response_status_count/302': 1,
 'downloader/response_status_count/403': 29,
 'downloader/response_status_count/404': 5,
 'downloader/response_status_count/429': 9,
 'dupefilter/filtered': 59,
 'elapsed_time_seconds': 10.510258,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 9, 12, 28, 54, 765747),
 'httpcompression/response_bytes': 14897292,
 'httpcompression/response_count': 157,
 'httperror/response_ignored_count': 16,
 'httperror/response_ignored_status_count/403': 14,
 'httperror/response_ignored_status_count/429': 2,
 'item_scraped_count': 26,
 'log_count/DEBUG': 476,
 'log_count/ERROR': 3,
 'log_count/INFO': 26,
 'request_depth_max': 1,
 'response_received_count': 182,
 'retry/count': 7,
 'retry/max_reached': 2,
 'retry/reason_count/429 Unknown Status': 7,
 'robotstxt/request_count': 84,
 'robotstxt/response_count': 84,
 'robotstxt/response_status_count/200': 64,
 'robotstxt/response_status_count/403': 15,
 'robotstxt/response_status_count/404': 5,
 'scheduler/dequeued': 138,
 'scheduler/dequeued/memory': 138,
 'scheduler/enqueued': 138,
 'scheduler/enqueued/memory': 138,
 'start_time': datetime.datetime(2022, 12, 9, 12, 28, 44, 255489)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: 21cbfd6211fd5399
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
DEBUG: Attempting to acquire lock 2162878827296 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2162878827296 acquired on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Attempting to release lock 2162878827296 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2162878827296 released on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Crawled (200) <GET http://www.columbustreeservicesllc.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.fandftrees.com/robots.txt> from <GET http://www.fandftrees.com/robots.txt>
DEBUG: Crawled (403) <GET https://trapperstreeservice.com/robots.txt> (referer: None)
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Rule at line 14 without any user agent to enforce it on.
DEBUG: Rule at line 16 without any user agent to enforce it on.
DEBUG: Rule at line 35 without any user agent to enforce it on.
DEBUG: Rule at line 42 without any user agent to enforce it on.
DEBUG: Rule at line 43 without any user agent to enforce it on.
DEBUG: Rule at line 44 without any user agent to enforce it on.
DEBUG: Rule at line 45 without any user agent to enforce it on.
DEBUG: Rule at line 46 without any user agent to enforce it on.
DEBUG: Rule at line 47 without any user agent to enforce it on.
DEBUG: Rule at line 69 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://www.treetechohio.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET https://www.midohiotree.org/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://www.columbustreeservicesllc.com/> (referer: None)
DEBUG: Crawled (403) <GET https://trapperstreeservice.com/> (referer: None)
DEBUG: Crawled (403) <GET https://www.midohiotree.org/> (referer: None)
DEBUG: Filtered duplicate request: <GET http://hardwicktreecare.com/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG: Crawled (404) <GET http://ohiotreeandexcavating.net/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.fandftrees.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.treetechohio.com/> (referer: None)
DEBUG: Crawled (200) <GET http://www.charteroakscompany.com/robots.txt> (referer: None)
INFO: Ignoring response <403 https://trapperstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET https://www.fandftrees.com/> from <GET http://www.fandftrees.com/>
INFO: Ignoring response <403 https://www.midohiotree.org/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET http://hardwicktreecare.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.fandftrees.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.hardwicktreecare.com/> from <GET http://hardwicktreecare.com/>
DEBUG: Redirecting (301) to <GET https://starwoodtree.com/robots.txt> from <GET http://starwoodtree.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.treetechohio.com/contact> (referer: https://www.treetechohio.com/)
DEBUG: Redirecting (301) to <GET http://ohiotreeandexcavating.com/> from <GET http://ohiotreeandexcavating.net/>
DEBUG: Crawled (200) <GET https://www.hardwicktreecare.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://ohiotreeandexcavating.com/robots.txt> from <GET http://ohiotreeandexcavating.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.russelltreeexperts.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.hardwicktreecare.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://www.treesaremybusiness.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://ohiotreeandexcavating.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://ohiotreeandexcavating.com/> from <GET http://ohiotreeandexcavating.com/>
DEBUG: Crawled (403) <GET http://www.hackettstreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://www.hackettstreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.russelltreeexperts.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.treesaremybusiness.com/> from <GET http://www.treesaremybusiness.com/>
DEBUG: Crawled (403) <GET https://www.tackettstreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/akron/?utm_source=GMB&utm_medium=organic&utm_campaign=doylestown> (referer: None)
DEBUG: Crawled (200) <GET https://www.hardwicktreecare.com/contact-hardwick-tree-care> (referer: https://www.hardwicktreecare.com/)
INFO: Ignoring response <403 http://www.hackettstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (404) <GET http://ohiotreecare.com/robots.txt> (referer: None)
DEBUG: Rule at line 1 without any user agent to enforce it on.
DEBUG: Rule at line 2 without any user agent to enforce it on.
DEBUG: Rule at line 8 without any user agent to enforce it on.
DEBUG: Rule at line 9 without any user agent to enforce it on.
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Rule at line 11 without any user agent to enforce it on.
DEBUG: Rule at line 12 without any user agent to enforce it on.
DEBUG: Crawled (403) <GET https://www.tackettstreeservice.com/> (referer: None)
DEBUG: Crawled (403) <GET http://deeprootedtreeservice.com/robots.txt> (referer: None)
INFO: Ignoring response <403 https://www.tackettstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://www.independenttree.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.charteroakscompany.com/> from <GET http://www.charteroakscompany.com/>
DEBUG: Crawled (403) <GET http://deeprootedtreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://specialtytreeohio.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://www.timberlandtreeohio.com/robots.txt> from <GET http://timberlandtreeohio.com/robots.txt>
INFO: Ignoring response <403 http://deeprootedtreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://ohiotreeandexcavating.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://treesaremybusiness.com/> from <GET https://www.treesaremybusiness.com/>
DEBUG: Crawled (200) <GET http://herculestree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://www.timberlandtreeohio.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://specialtytreeohio.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.independenttree.com/?utm_source=gmb&utm_medium=local&utm_campaign=website> (referer: None)
DEBUG: Crawled (200) <GET https://www.whymonster.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://jstreeservicesllc.com/robots.txt> from <GET http://www.jstreeservicesllc.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.charteroakscompany.com/> (referer: None)
DEBUG: Redirecting (301) to <GET http://www.timberlandtreeohio.com/> from <GET http://timberlandtreeohio.com/>
DEBUG: Crawled (200) <GET https://starwoodtree.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.monstertreeservice.com/akron/contact-us/> from <GET https://www.whymonster.com/akron/contact-us/>
DEBUG: Crawled (403) <GET http://cottstrees.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://herculestree.com/> from <GET http://herculestree.com/>
DEBUG: Crawled (200) <GET http://www.timberlandtreeohio.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://starwoodtree.com/> from <GET http://starwoodtree.com/>
DEBUG: Crawled (200) <GET https://www.ewsmithtree.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://cottstrees.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/akron/contact-us/> (referer: https://www.monstertreeservice.com/akron/?utm_source=GMB&utm_medium=organic&utm_campaign=doylestown)
DEBUG: Crawled (200) <GET https://treesaremybusiness.com/robots.txt> (referer: None)
INFO: Ignoring response <403 http://cottstrees.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://www.independenttree.com/contact-us/> (referer: https://www.independenttree.com/?utm_source=gmb&utm_medium=local&utm_campaign=website)
DEBUG: Redirecting (301) to <GET https://www.basictreecare.com/robots.txt> from <GET http://www.basictreecare.com/robots.txt>
DEBUG: Crawled (200) <GET http://jstreeservicesllc.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://roquetree.com/robots.txt> from <GET http://roquetree.com/robots.txt>
DEBUG: Crawled (200) <GET http://ohiotreecare.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.ewsmithtree.com/> (referer: None)
DEBUG: Crawled (200) <GET http://www.kiddertreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://haneytreeservice.com/robots.txt> from <GET http://www.haneytreeservice.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://www.kiddertreemov.com/> from <GET http://www.kiddertreeservice.com/>
DEBUG: Crawled (200) <GET https://treesaremybusiness.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/cleveland/?utm_source=GMB&utm_medium=organic&utm_campaign=AvonLake> (referer: None)
DEBUG: Redirecting (301) to <GET https://jstreeservicesllc.com/> from <GET http://www.jstreeservicesllc.com/>
DEBUG: Crawled (200) <GET https://www.basictreecare.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.kiddertreemov.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://herculestree.com/> (referer: None)
DEBUG: Crawled (200) <GET http://haneytreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.basictreecare.com/> from <GET http://www.basictreecare.com/>
DEBUG: Redirecting (301) to <GET https://www.jttreeservicellc.com/robots.txt> from <GET http://www.jttreeservicellc.com/robots.txt>
DEBUG: Redirecting (301) to <GET http://haneytreeservice.com/> from <GET http://www.haneytreeservice.com/>
DEBUG: Crawled (200) <GET https://treesaremybusiness.com/contact-us/> (referer: https://treesaremybusiness.com/)
DEBUG: Crawled (200) <GET https://roquetree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://www.timberlandtreeohio.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.kiddertreemov.com/> (referer: None)
DEBUG: Crawled (200) <GET http://haneytreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://roquetree.com/> from <GET http://roquetree.com/>
DEBUG: Crawled (200) <GET https://www.ewsmithtree.com/contact-us/> (referer: https://www.ewsmithtree.com/)
DEBUG: Crawled (200) <GET https://www.basictreecare.com/> (referer: None)
DEBUG: Redirecting (301) to <GET http://haneytreeservice.com/index.html> from <GET http://haneytreeservice.com/>
DEBUG: Crawled (200) <GET https://herculestree.com/contact> (referer: https://herculestree.com/)
DEBUG: Redirecting (301) to <GET https://www.roquetree.com/> from <GET https://roquetree.com/>
DEBUG: Crawled (200) <GET https://jstreeservicesllc.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.davey.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET https://www.kandatreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET https://www.kandatreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET http://haneytreeservice.com/index.html> (referer: None)
DEBUG: Crawled (200) <GET https://www.jttreeservicellc.com/robots.txt> (referer: None)
INFO: Ignoring response <403 https://www.kandatreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (403) <GET http://www.jjsfamilytreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.ripleytreeservice.com/robots.txt> from <GET http://www.ripleytreeservice.com/robots.txt>
DEBUG: Crawled (403) <GET http://www.jjsfamilytreeservice.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.jttreeservicellc.com/> from <GET http://www.jttreeservicellc.com/>
INFO: Ignoring response <403 http://www.jjsfamilytreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://challengerstreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://herculestree.com/contact/request-quote/> (referer: https://herculestree.com/)
DEBUG: Crawled (200) <GET https://www.roquetree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://herculestree.com/contact/request-quote> (referer: https://herculestree.com/)
DEBUG: Crawled (200) <GET https://www.davey.com/residential-tree-services/local-offices/columbus-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Columbus> (referer: None)
DEBUG: Crawled (200) <GET https://starwoodtree.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.jttreeservicellc.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://extremetreeservicestoledo.com/robots.txt> from <GET https://www.extremetreeservicestoledo.com/robots.txt>
DEBUG: Crawled (200) <GET https://herculestree.com/contact/> (referer: https://herculestree.com/)
DEBUG: Crawled (200) <GET https://parkstree.net/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://jstreeservicesllc.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.roquetree.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.savatree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://parkstree.net/> (referer: None)
DEBUG: Crawled (200) <GET https://oak-tree-services-tree-service.business.site/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://parkstree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.rawtreeserv.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://parkstree.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.ripleytreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://oak-tree-services-tree-service.business.site/?utm_source=gmb&utm_medium=referral> (referer: None)
DEBUG: Redirecting (301) to <GET https://rawtreeserv.com/> from <GET https://www.rawtreeserv.com/>
DEBUG: Crawled (200) <GET https://www.savatree.com/dayton-ohio-tree-service-lawn-care?utm_source=GMB&utm_medium=organic&utm_campaign=dayton> (referer: None)
DEBUG: Crawled (200) <GET https://extremetreeservicestoledo.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.ripleytreeservice.com/> from <GET http://www.ripleytreeservice.com/>
DEBUG: Redirecting (301) to <GET https://extremetreeservicestoledo.com/> from <GET https://www.extremetreeservicestoledo.com/>
DEBUG: Crawled (200) <GET https://extremetreeservicestoledo.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.ripleytreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.stevestoledotree.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://jstreeservicesllc.com/contact-us/> from <GET http://www.jstreeservicesllc.com/contact-us>
DEBUG: Crawled (200) <GET https://extremetreeservicestoledo.com/> (referer: None)
DEBUG: Crawled (200) <GET https://jstreeservicesllc.com/contact-us/> (referer: https://jstreeservicesllc.com/)
DEBUG: Crawled (200) <GET https://rawtreeserv.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://blackstreemov.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.stevestoledotree.com/?utm_source=googlelocal&utm_medium=organic> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.blackstreemov.com/> from <GET http://blackstreemov.com/>
DEBUG: Crawled (403) <GET http://www.haymakertreeandlawn.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://www.haymakertreeandlawn.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.davey.com/about/contact-us/?type=nonservicerequest> (referer: https://www.davey.com/residential-tree-services/local-offices/columbus-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Columbus)
DEBUG: Crawled (200) <GET https://affordabletreeservicesofohiollc.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.blackstreemov.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://affordabletreeservicesofohiollc.com/> (referer: None)
INFO: Ignoring response <403 http://www.haymakertreeandlawn.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://extremetreeservicestoledo.com/contact/> (referer: https://extremetreeservicestoledo.com/)
DEBUG: Crawled (200) <GET https://rawtreeserv.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.blackstreemov.com/> (referer: None)
DEBUG: Crawled (403) <GET http://www.cstreemulch.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://treesrus-treeservice.business.site/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://jstreeservicesllc.com/contact-us/> (referer: None)
DEBUG: Crawled (403) <GET http://www.cstreemulch.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.davey.com/residential-tree-services/local-offices/cincinnati-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Cincinnati> (referer: None)
DEBUG: Crawled (403) <GET http://www.ashtreeservicepro.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://www.ashtreeservicepro.com/> (referer: None)
INFO: Ignoring response <403 http://www.cstreemulch.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://starwoodtree.com/contact-us/> (referer: https://starwoodtree.com/)
DEBUG: Redirecting (301) to <GET https://www.roguetreesolutions.com/robots.txt> from <GET http://www.roguetreesolutions.com/robots.txt>
DEBUG: Crawled (200) <GET https://treesrus-treeservice.business.site/?utm_source=gmb&utm_medium=referral> (referer: None)
INFO: Ignoring response <403 http://www.ashtreeservicepro.com/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET http://www.toddstreeservice.com/robots.txt> from <GET http://toddstreeservice.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.blackstreemov.com/contact> (referer: https://www.blackstreemov.com/)
DEBUG: Redirecting (301) to <GET https://hoffmantreeservice.com/robots.txt> from <GET http://hoffmantreeservice.com/robots.txt>
DEBUG: Crawled (200) <GET http://urbanloggersohio.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://extremetreeohio.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://extremetreeohio.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://urbanloggersohio.com/> from <GET http://urbanloggersohio.com/>
DEBUG: Crawled (403) <GET http://www.toddstreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://rstreeservicellc.com/robots.txt> (referer: None)
DEBUG: Rule at line 1 without any user agent to enforce it on.
DEBUG: Rule at line 5 without any user agent to enforce it on.
DEBUG: Rule at line 9 without any user agent to enforce it on.
DEBUG: Rule at line 14 without any user agent to enforce it on.
DEBUG: Rule at line 19 without any user agent to enforce it on.
DEBUG: Rule at line 21 without any user agent to enforce it on.
DEBUG: Rule at line 22 without any user agent to enforce it on.
DEBUG: Rule at line 23 without any user agent to enforce it on.
DEBUG: Rule at line 26 without any user agent to enforce it on.
DEBUG: Rule at line 28 without any user agent to enforce it on.
DEBUG: Rule at line 29 without any user agent to enforce it on.
DEBUG: Rule at line 31 without any user agent to enforce it on.
DEBUG: Rule at line 32 without any user agent to enforce it on.
DEBUG: Rule at line 33 without any user agent to enforce it on.
DEBUG: Rule at line 34 without any user agent to enforce it on.
DEBUG: Rule at line 35 without any user agent to enforce it on.
DEBUG: Rule at line 36 without any user agent to enforce it on.
DEBUG: Rule at line 37 without any user agent to enforce it on.
DEBUG: Rule at line 39 without any user agent to enforce it on.
DEBUG: Rule at line 40 without any user agent to enforce it on.
DEBUG: Rule at line 43 without any user agent to enforce it on.
DEBUG: Rule at line 44 without any user agent to enforce it on.
DEBUG: Rule at line 46 without any user agent to enforce it on.
DEBUG: Rule at line 48 without any user agent to enforce it on.
DEBUG: Rule at line 51 without any user agent to enforce it on.
DEBUG: Rule at line 53 without any user agent to enforce it on.
DEBUG: Rule at line 54 without any user agent to enforce it on.
DEBUG: Rule at line 55 without any user agent to enforce it on.
DEBUG: Rule at line 56 without any user agent to enforce it on.
DEBUG: Rule at line 57 without any user agent to enforce it on.
DEBUG: Rule at line 58 without any user agent to enforce it on.
DEBUG: Rule at line 60 without any user agent to enforce it on.
DEBUG: Rule at line 61 without any user agent to enforce it on.
DEBUG: Rule at line 62 without any user agent to enforce it on.
DEBUG: Rule at line 63 without any user agent to enforce it on.
DEBUG: Rule at line 64 without any user agent to enforce it on.
DEBUG: Rule at line 65 without any user agent to enforce it on.
DEBUG: Rule at line 66 without any user agent to enforce it on.
DEBUG: Rule at line 67 without any user agent to enforce it on.
DEBUG: Rule at line 68 without any user agent to enforce it on.
DEBUG: Rule at line 69 without any user agent to enforce it on.
DEBUG: Rule at line 70 without any user agent to enforce it on.
DEBUG: Rule at line 71 without any user agent to enforce it on.
DEBUG: Rule at line 72 without any user agent to enforce it on.
DEBUG: Rule at line 73 without any user agent to enforce it on.
DEBUG: Rule at line 74 without any user agent to enforce it on.
DEBUG: Rule at line 75 without any user agent to enforce it on.
DEBUG: Rule at line 76 without any user agent to enforce it on.
DEBUG: Rule at line 77 without any user agent to enforce it on.
DEBUG: Rule at line 78 without any user agent to enforce it on.
DEBUG: Rule at line 79 without any user agent to enforce it on.
DEBUG: Rule at line 80 without any user agent to enforce it on.
DEBUG: Rule at line 81 without any user agent to enforce it on.
DEBUG: Rule at line 83 without any user agent to enforce it on.
DEBUG: Rule at line 85 without any user agent to enforce it on.
DEBUG: Rule at line 86 without any user agent to enforce it on.
DEBUG: Rule at line 87 without any user agent to enforce it on.
DEBUG: Rule at line 88 without any user agent to enforce it on.
DEBUG: Rule at line 89 without any user agent to enforce it on.
DEBUG: Rule at line 90 without any user agent to enforce it on.
DEBUG: Rule at line 91 without any user agent to enforce it on.
DEBUG: Rule at line 92 without any user agent to enforce it on.
DEBUG: Rule at line 93 without any user agent to enforce it on.
DEBUG: Rule at line 94 without any user agent to enforce it on.
DEBUG: Rule at line 95 without any user agent to enforce it on.
DEBUG: Rule at line 96 without any user agent to enforce it on.
DEBUG: Rule at line 98 without any user agent to enforce it on.
DEBUG: Rule at line 99 without any user agent to enforce it on.
DEBUG: Rule at line 100 without any user agent to enforce it on.
DEBUG: Rule at line 101 without any user agent to enforce it on.
DEBUG: Rule at line 102 without any user agent to enforce it on.
DEBUG: Rule at line 103 without any user agent to enforce it on.
DEBUG: Rule at line 104 without any user agent to enforce it on.
DEBUG: Rule at line 106 without any user agent to enforce it on.
DEBUG: Rule at line 107 without any user agent to enforce it on.
DEBUG: Rule at line 108 without any user agent to enforce it on.
DEBUG: Rule at line 110 without any user agent to enforce it on.
DEBUG: Rule at line 112 without any user agent to enforce it on.
DEBUG: Rule at line 113 without any user agent to enforce it on.
DEBUG: Rule at line 114 without any user agent to enforce it on.
DEBUG: Rule at line 115 without any user agent to enforce it on.
DEBUG: Rule at line 116 without any user agent to enforce it on.
DEBUG: Rule at line 117 without any user agent to enforce it on.
DEBUG: Rule at line 121 without any user agent to enforce it on.
DEBUG: Rule at line 122 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://hoffmantreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://www.toddstreeservice.com/> from <GET http://toddstreeservice.com/>
DEBUG: Crawled (200) <GET http://www.treeservicedelawareoh.com/robots.txt> (referer: None)
DEBUG: Crawled (404) <GET http://www.boonestreeservice.com/robots.txt> (referer: None)
DEBUG: Rule at line 2 without any user agent to enforce it on.
DEBUG: Rule at line 8 without any user agent to enforce it on.
DEBUG: Rule at line 9 without any user agent to enforce it on.
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Crawled (404) <GET https://www.roguetreesolutions.com/robots.txt> (referer: None)
DEBUG: Rule at line 1 without any user agent to enforce it on.
DEBUG: Rule at line 2 without any user agent to enforce it on.
DEBUG: Rule at line 14 without any user agent to enforce it on.
DEBUG: Rule at line 15 without any user agent to enforce it on.
DEBUG: Rule at line 23 without any user agent to enforce it on.
DEBUG: Rule at line 28 without any user agent to enforce it on.
DEBUG: Rule at line 29 without any user agent to enforce it on.
DEBUG: Rule at line 32 without any user agent to enforce it on.
DEBUG: Rule at line 33 without any user agent to enforce it on.
DEBUG: Rule at line 45 without any user agent to enforce it on.
DEBUG: Rule at line 46 without any user agent to enforce it on.
DEBUG: Rule at line 47 without any user agent to enforce it on.
DEBUG: Rule at line 48 without any user agent to enforce it on.
DEBUG: Rule at line 49 without any user agent to enforce it on.
DEBUG: Rule at line 50 without any user agent to enforce it on.
DEBUG: Rule at line 51 without any user agent to enforce it on.
DEBUG: Rule at line 52 without any user agent to enforce it on.
DEBUG: Rule at line 53 without any user agent to enforce it on.
DEBUG: Rule at line 54 without any user agent to enforce it on.
DEBUG: Rule at line 55 without any user agent to enforce it on.
DEBUG: Rule at line 57 without any user agent to enforce it on.
DEBUG: Rule at line 58 without any user agent to enforce it on.
DEBUG: Rule at line 59 without any user agent to enforce it on.
DEBUG: Rule at line 61 without any user agent to enforce it on.
DEBUG: Rule at line 62 without any user agent to enforce it on.
DEBUG: Rule at line 63 without any user agent to enforce it on.
DEBUG: Rule at line 64 without any user agent to enforce it on.
DEBUG: Rule at line 65 without any user agent to enforce it on.
DEBUG: Rule at line 66 without any user agent to enforce it on.
DEBUG: Rule at line 67 without any user agent to enforce it on.
DEBUG: Rule at line 70 without any user agent to enforce it on.
DEBUG: Rule at line 72 without any user agent to enforce it on.
DEBUG: Rule at line 78 without any user agent to enforce it on.
DEBUG: Rule at line 79 without any user agent to enforce it on.
DEBUG: Rule at line 80 without any user agent to enforce it on.
DEBUG: Rule at line 81 without any user agent to enforce it on.
DEBUG: Rule at line 87 without any user agent to enforce it on.
DEBUG: Rule at line 88 without any user agent to enforce it on.
DEBUG: Rule at line 92 without any user agent to enforce it on.
DEBUG: Rule at line 94 without any user agent to enforce it on.
DEBUG: Rule at line 96 without any user agent to enforce it on.
DEBUG: Rule at line 98 without any user agent to enforce it on.
DEBUG: Rule at line 100 without any user agent to enforce it on.
DEBUG: Rule at line 101 without any user agent to enforce it on.
DEBUG: Rule at line 104 without any user agent to enforce it on.
DEBUG: Rule at line 106 without any user agent to enforce it on.
DEBUG: Rule at line 107 without any user agent to enforce it on.
DEBUG: Rule at line 108 without any user agent to enforce it on.
DEBUG: Rule at line 109 without any user agent to enforce it on.
DEBUG: Rule at line 110 without any user agent to enforce it on.
DEBUG: Rule at line 111 without any user agent to enforce it on.
DEBUG: Rule at line 112 without any user agent to enforce it on.
DEBUG: Rule at line 119 without any user agent to enforce it on.
DEBUG: Rule at line 120 without any user agent to enforce it on.
DEBUG: Rule at line 121 without any user agent to enforce it on.
DEBUG: Rule at line 122 without any user agent to enforce it on.
DEBUG: Rule at line 123 without any user agent to enforce it on.
DEBUG: Rule at line 128 without any user agent to enforce it on.
DEBUG: Rule at line 145 without any user agent to enforce it on.
DEBUG: Rule at line 155 without any user agent to enforce it on.
DEBUG: Rule at line 165 without any user agent to enforce it on.
DEBUG: Redirecting (301) to <GET https://hoffmantreeservice.com/> from <GET http://hoffmantreeservice.com/>
DEBUG: Crawled (403) <GET http://www.toddstreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.roguetreesolutions.com/> from <GET http://www.roguetreesolutions.com/>
DEBUG: Crawled (200) <GET https://www.davey.com/about/contact-us/?type=nonresidential> (referer: https://www.davey.com/residential-tree-services/local-offices/columbus-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Columbus)
DEBUG: Crawled (200) <GET http://www.treeservicedelawareoh.com/> (referer: None)
DEBUG: Crawled (200) <GET http://rstreeservicellc.com/?utm_source=gmb&utm_medium=referral> (referer: None)
DEBUG: Crawled (200) <GET https://www.roguetreesolutions.com/> (referer: None)
DEBUG: Crawled (200) <GET http://www.boonestreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://hoffmantreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://challengerstreeservice.com/> (referer: None)
DEBUG: Crawled (403) <GET http://www.toddstreeservice.com/> (referer: None)
INFO: Ignoring response <403 http://www.toddstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://brushbandittree.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.treeservicenow.net/robots.txt> from <GET http://www.treeservicenow.net/robots.txt>
DEBUG: Redirecting (302) to <GET https://www.greatdanetreeexperts.com/404.html> from <GET https://www.greatdanetreeexperts.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://barbertontree.com/robots.txt> from <GET https://www.barbertontree.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.greatdanetreeexperts.com/404.html> (referer: None)
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Rule at line 11 without any user agent to enforce it on.
DEBUG: Rule at line 12 without any user agent to enforce it on.
DEBUG: Rule at line 15 without any user agent to enforce it on.
DEBUG: Rule at line 17 without any user agent to enforce it on.
DEBUG: Rule at line 28 without any user agent to enforce it on.
DEBUG: Rule at line 43 without any user agent to enforce it on.
DEBUG: Rule at line 44 without any user agent to enforce it on.
DEBUG: Rule at line 45 without any user agent to enforce it on.
DEBUG: Rule at line 46 without any user agent to enforce it on.
DEBUG: Rule at line 47 without any user agent to enforce it on.
DEBUG: Rule at line 48 without any user agent to enforce it on.
DEBUG: Rule at line 49 without any user agent to enforce it on.
DEBUG: Rule at line 50 without any user agent to enforce it on.
DEBUG: Rule at line 54 without any user agent to enforce it on.
DEBUG: Rule at line 57 without any user agent to enforce it on.
DEBUG: Rule at line 62 without any user agent to enforce it on.
DEBUG: Rule at line 63 without any user agent to enforce it on.
DEBUG: Rule at line 67 without any user agent to enforce it on.
DEBUG: Rule at line 88 without any user agent to enforce it on.
DEBUG: Rule at line 136 without any user agent to enforce it on.
DEBUG: Rule at line 141 without any user agent to enforce it on.
DEBUG: Rule at line 144 without any user agent to enforce it on.
DEBUG: Rule at line 147 without any user agent to enforce it on.
DEBUG: Rule at line 156 without any user agent to enforce it on.
DEBUG: Rule at line 157 without any user agent to enforce it on.
DEBUG: Rule at line 191 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://www.davey.com/residential-tree-services/local-offices/akron-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Akron> (referer: None)
DEBUG: Crawled (200) <GET http://www.treeservicedelawareoh.com/contact-us/> (referer: http://www.treeservicedelawareoh.com/)
DEBUG: Crawled (200) <GET https://www.davey.com/about/contact-us/?type=residential> (referer: https://www.davey.com/residential-tree-services/local-offices/columbus-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Columbus)
DEBUG: Crawled (200) <GET https://urbanloggersohio.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://lamannatreeservice.com/robots.txt> from <GET http://www.lamannatreeservice.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.davey.com/residential-tree-services/local-offices/northeast-cleveland-tree-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Northeast%20Cleveland> (referer: None)
DEBUG: Crawled (200) <GET https://premiertreesllc.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://tree-works-ohio.com/robots.txt> from <GET http://tree-works-ohio.com/robots.txt>
DEBUG: Crawled (200) <GET https://barbertontree.com/robots.txt> (referer: None)
DEBUG: Rule at line 3 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://www.greatdanetreeexperts.com/> (referer: None)
DEBUG: Crawled (200) <GET https://larochetree.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://lamannatreeservice.com/?robots=1> from <GET https://lamannatreeservice.com/robots.txt>
DEBUG: Retrying <GET https://premiertreesllc.com/> (failed 1 times): 429 Unknown Status
DEBUG: Redirecting (301) to <GET https://barbertontree.com/> from <GET https://www.barbertontree.com/>
DEBUG: Crawled (200) <GET https://lamannatreeservice.com/?robots=1> (referer: None)
DEBUG: Crawled (200) <GET https://www.delmartreeservices.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://brushbandittree.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://lamannatreeservice.com/> from <GET http://www.lamannatreeservice.com/>
DEBUG: Redirecting (301) to <GET https://www.larochetree.com/> from <GET https://larochetree.com/>
DEBUG: Crawled (403) <GET http://tomcotreecare.com/robots.txt> (referer: None)
DEBUG: Retrying <GET https://barbertontree.com/robots.txt> (failed 1 times): 429 Unknown Status
DEBUG: Crawled (200) <GET https://www.delmartreeservices.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://lamannatreeservice.com/?robots=1> from <GET https://lamannatreeservice.com/robots.txt>
DEBUG: Retrying <GET https://premiertreesllc.com/> (failed 2 times): 429 Unknown Status
DEBUG: Crawled (403) <GET http://www.dolcestreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://lamannatreeservice.com/?robots=1> (referer: None)
DEBUG: Crawled (200) <GET https://urbanloggersohio.com/contact-urban-loggers/> (referer: https://urbanloggersohio.com/)
DEBUG: Crawled (403) <GET http://tomcotreecare.com/> (referer: None)
DEBUG: Crawled (403) <GET http://www.dolcestreeservice.com/> (referer: None)
ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\engine.py", line 152, in _next_request
    request = next(self.slot.start_requests)
  File "C:\Users\moffi\PycharmProjects\scraping_web\scraper\scraper\spiders\website_spider.py", line 40, in start_requests
    yield scrapy.Request(url=url, callback=self.parse_website)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\http\request\__init__.py", line 60, in __init__
    self._set_url(url)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\http\request\__init__.py", line 108, in _set_url
    raise ValueError(f'Missing scheme in request url: {self._url}')
ValueError: Missing scheme in request url: 
DEBUG: Crawled (200) <GET https://lamannatreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.larochetree.com/robots.txt> (referer: None)
INFO: Ignoring response <403 http://tomcotreecare.com/>: HTTP status code is not handled or not allowed
INFO: Ignoring response <403 http://www.dolcestreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Retrying <GET https://barbertontree.com/robots.txt> (failed 2 times): 429 Unknown Status
DEBUG: Crawled (200) <GET https://lamannatreeservice.com/contact/> (referer: https://lamannatreeservice.com/)
DEBUG: Crawled (200) <GET https://premiertreesllc.com/> (referer: None)
ERROR: Gave up retrying <GET https://barbertontree.com/robots.txt> (failed 3 times): 429 Unknown Status
DEBUG: Crawled (429) <GET https://barbertontree.com/robots.txt> (referer: None)
DEBUG: Rule at line 12 without any user agent to enforce it on.
DEBUG: Rule at line 14 without any user agent to enforce it on.
DEBUG: Rule at line 17 without any user agent to enforce it on.
DEBUG: Rule at line 20 without any user agent to enforce it on.
DEBUG: Rule at line 21 without any user agent to enforce it on.
DEBUG: Rule at line 22 without any user agent to enforce it on.
DEBUG: Rule at line 23 without any user agent to enforce it on.
DEBUG: Rule at line 26 without any user agent to enforce it on.
DEBUG: Rule at line 29 without any user agent to enforce it on.
DEBUG: Rule at line 32 without any user agent to enforce it on.
DEBUG: Rule at line 33 without any user agent to enforce it on.
DEBUG: Rule at line 34 without any user agent to enforce it on.
DEBUG: Rule at line 37 without any user agent to enforce it on.
DEBUG: Rule at line 38 without any user agent to enforce it on.
DEBUG: Rule at line 39 without any user agent to enforce it on.
DEBUG: Rule at line 42 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://tree-works-ohio.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://treeservicenow.net/robots.txt> from <GET https://www.treeservicenow.net/robots.txt>
DEBUG: Crawled (200) <GET https://www.larochetree.com/> (referer: None)
DEBUG: Crawled (404) <GET https://www.napierandson.com/robots.txt> (referer: None)
DEBUG: Retrying <GET https://barbertontree.com/> (failed 1 times): 429 Unknown Status
DEBUG: Crawled (200) <GET https://brushbandittree.com/contact-us/> (referer: https://brushbandittree.com/)
DEBUG: Retrying <GET https://premiertreesllc.com/contact-us/> (failed 1 times): 429 Unknown Status
DEBUG: Crawled (200) <GET https://treeservicecolumbusohio.net/robots.txt> (referer: None)
DEBUG: Retrying <GET https://barbertontree.com/> (failed 2 times): 429 Unknown Status
DEBUG: Redirecting (301) to <GET https://tree-works-ohio.com/> from <GET http://tree-works-ohio.com/>
DEBUG: Retrying <GET https://premiertreesllc.com/contact-us/> (failed 2 times): 429 Unknown Status
DEBUG: Crawled (200) <GET https://www.napierandson.com/> (referer: None)
DEBUG: Crawled (200) <GET https://treeservicenow.net/robots.txt> (referer: None)
ERROR: Gave up retrying <GET https://premiertreesllc.com/contact-us/> (failed 3 times): 429 Unknown Status
DEBUG: Crawled (429) <GET https://premiertreesllc.com/contact-us/> (referer: https://premiertreesllc.com/)
DEBUG: Crawled (200) <GET https://www.larochetree.com/contact> (referer: https://www.larochetree.com/)
INFO: Ignoring response <429 https://premiertreesllc.com/contact-us/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://treeservicecolumbusohio.net/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.treeservicenow.net/> from <GET http://www.treeservicenow.net/>
DEBUG: Crawled (200) <GET https://tree-works-ohio.com/> (referer: None)
DEBUG: Crawled (200) <GET https://treeservicecolumbusohio.net/contact-us/> (referer: https://treeservicecolumbusohio.net/)
DEBUG: Redirecting (301) to <GET https://treeservicenow.net/> from <GET https://www.treeservicenow.net/>
DEBUG: Crawled (200) <GET https://tree-works-ohio.com/contact-tree-works-for-all-your-emergency-services-tree-removal-stump-grinding-etc/> (referer: https://tree-works-ohio.com/)
DEBUG: Crawled (200) <GET https://treeservicenow.net/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://treeservicenow.net/> (referer: None)
DEBUG: Crawled (200) <GET https://barbertontree.com/> (referer: None)
DEBUG: Crawled (200) <GET https://treeservicenow.net/contact_us_1> (referer: https://treeservicenow.net/)
DEBUG: Crawled (200) <GET https://barbertontree.com/contactus/> (referer: https://barbertontree.com/)
INFO: Closing spider (finished)
INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 65165,
 'downloader/request_count': 247,
 'downloader/request_method_count/GET': 247,
 'downloader/response_bytes': 3575326,
 'downloader/response_count': 247,
 'downloader/response_status_count/200': 147,
 'downloader/response_status_count/301': 55,
 'downloader/response_status_count/302': 1,
 'downloader/response_status_count/403': 29,
 'downloader/response_status_count/404': 5,
 'downloader/response_status_count/429': 10,
 'dupefilter/filtered': 61,
 'elapsed_time_seconds': 11.599519,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 9, 12, 30, 14, 195634),
 'httpcompression/response_bytes': 15521159,
 'httpcompression/response_count': 158,
 'httperror/response_ignored_count': 15,
 'httperror/response_ignored_status_count/403': 14,
 'httperror/response_ignored_status_count/429': 1,
 'log_count/DEBUG': 467,
 'log_count/ERROR': 3,
 'log_count/INFO': 25,
 'request_depth_max': 1,
 'response_received_count': 183,
 'retry/count': 8,
 'retry/max_reached': 2,
 'retry/reason_count/429 Unknown Status': 8,
 'robotstxt/request_count': 84,
 'robotstxt/response_count': 84,
 'robotstxt/response_status_count/200': 63,
 'robotstxt/response_status_count/403': 15,
 'robotstxt/response_status_count/404': 5,
 'robotstxt/response_status_count/429': 1,
 'scheduler/dequeued': 139,
 'scheduler/dequeued/memory': 139,
 'scheduler/enqueued': 139,
 'scheduler/enqueued/memory': 139,
 'start_time': datetime.datetime(2022, 12, 9, 12, 30, 2, 596115)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: d106c38301cd835f
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
DEBUG: Redirecting (301) to <GET https://www.fandftrees.com/robots.txt> from <GET http://www.fandftrees.com/robots.txt>
DEBUG: Attempting to acquire lock 2759908645168 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2759908645168 acquired on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Attempting to release lock 2759908645168 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2759908645168 released on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Crawled (200) <GET http://www.columbustreeservicesllc.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET https://trapperstreeservice.com/robots.txt> (referer: None)
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Rule at line 14 without any user agent to enforce it on.
DEBUG: Rule at line 16 without any user agent to enforce it on.
DEBUG: Rule at line 35 without any user agent to enforce it on.
DEBUG: Rule at line 42 without any user agent to enforce it on.
DEBUG: Rule at line 43 without any user agent to enforce it on.
DEBUG: Rule at line 44 without any user agent to enforce it on.
DEBUG: Rule at line 45 without any user agent to enforce it on.
DEBUG: Rule at line 46 without any user agent to enforce it on.
DEBUG: Rule at line 47 without any user agent to enforce it on.
DEBUG: Rule at line 69 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://www.treetechohio.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET https://www.midohiotree.org/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://www.columbustreeservicesllc.com/> (referer: None)
DEBUG: Crawled (403) <GET https://trapperstreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.fandftrees.com/robots.txt> (referer: None)
DEBUG: Filtered duplicate request: <GET http://hardwicktreecare.com/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG: Redirecting (301) to <GET https://www.fandftrees.com/> from <GET http://www.fandftrees.com/>
DEBUG: Crawled (200) <GET https://www.treetechohio.com/> (referer: None)
DEBUG: Crawled (404) <GET http://ohiotreeandexcavating.net/robots.txt> (referer: None)
INFO: Ignoring response <403 https://trapperstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET http://www.charteroakscompany.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET https://www.midohiotree.org/> (referer: None)
DEBUG: Crawled (200) <GET http://hardwicktreecare.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.fandftrees.com/> (referer: None)
INFO: Ignoring response <403 https://www.midohiotree.org/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET https://www.hardwicktreecare.com/> from <GET http://hardwicktreecare.com/>
DEBUG: Redirecting (301) to <GET https://starwoodtree.com/robots.txt> from <GET http://starwoodtree.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.russelltreeexperts.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://ohiotreeandexcavating.com/> from <GET http://ohiotreeandexcavating.net/>
DEBUG: Crawled (200) <GET https://www.hardwicktreecare.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://www.treesaremybusiness.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://ohiotreeandexcavating.com/robots.txt> from <GET http://ohiotreeandexcavating.com/robots.txt>
DEBUG: Crawled (403) <GET https://www.tackettstreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.hardwicktreecare.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.treetechohio.com/contact> (referer: https://www.treetechohio.com/)
DEBUG: Crawled (404) <GET http://ohiotreecare.com/robots.txt> (referer: None)
DEBUG: Rule at line 1 without any user agent to enforce it on.
DEBUG: Rule at line 2 without any user agent to enforce it on.
DEBUG: Rule at line 8 without any user agent to enforce it on.
DEBUG: Rule at line 9 without any user agent to enforce it on.
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Rule at line 11 without any user agent to enforce it on.
DEBUG: Rule at line 12 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://www.russelltreeexperts.com/> (referer: None)
DEBUG: Crawled (200) <GET https://ohiotreeandexcavating.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://ohiotreeandexcavating.com/> from <GET http://ohiotreeandexcavating.com/>
DEBUG: Crawled (403) <GET https://www.tackettstreeservice.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.treesaremybusiness.com/> from <GET http://www.treesaremybusiness.com/>
DEBUG: Crawled (200) <GET https://www.hardwicktreecare.com/contact-hardwick-tree-care> (referer: https://www.hardwicktreecare.com/)
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/akron/?utm_source=GMB&utm_medium=organic&utm_campaign=doylestown> (referer: None)
INFO: Ignoring response <403 https://www.tackettstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (403) <GET http://www.hackettstreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://www.hackettstreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://ohiotreeandexcavating.com/> (referer: None)
DEBUG: Crawled (403) <GET http://deeprootedtreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.independenttree.com/robots.txt> (referer: None)
INFO: Ignoring response <403 http://www.hackettstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET https://www.charteroakscompany.com/> from <GET http://www.charteroakscompany.com/>
DEBUG: Crawled (403) <GET http://deeprootedtreeservice.com/> (referer: None)
DEBUG: Redirecting (301) to <GET http://www.timberlandtreeohio.com/robots.txt> from <GET http://timberlandtreeohio.com/robots.txt>
INFO: Ignoring response <403 http://deeprootedtreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://specialtytreeohio.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://treesaremybusiness.com/> from <GET https://www.treesaremybusiness.com/>
DEBUG: Crawled (200) <GET http://herculestree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.independenttree.com/?utm_source=gmb&utm_medium=local&utm_campaign=website> (referer: None)
DEBUG: Crawled (200) <GET http://www.timberlandtreeohio.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://jstreeservicesllc.com/robots.txt> from <GET http://www.jstreeservicesllc.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.whymonster.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://starwoodtree.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://www.timberlandtreeohio.com/> from <GET http://timberlandtreeohio.com/>
DEBUG: Crawled (200) <GET https://specialtytreeohio.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.monstertreeservice.com/akron/contact-us/> from <GET https://www.whymonster.com/akron/contact-us/>
DEBUG: Crawled (200) <GET https://www.charteroakscompany.com/> (referer: None)
DEBUG: Crawled (403) <GET http://cottstrees.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://herculestree.com/> from <GET http://herculestree.com/>
DEBUG: Redirecting (301) to <GET https://starwoodtree.com/> from <GET http://starwoodtree.com/>
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/akron/contact-us/> (referer: https://www.monstertreeservice.com/akron/?utm_source=GMB&utm_medium=organic&utm_campaign=doylestown)
DEBUG: Crawled (200) <GET https://www.ewsmithtree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://www.timberlandtreeohio.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://cottstrees.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.independenttree.com/contact-us/> (referer: https://www.independenttree.com/?utm_source=gmb&utm_medium=local&utm_campaign=website)
DEBUG: Crawled (200) <GET https://treesaremybusiness.com/robots.txt> (referer: None)
INFO: Ignoring response <403 http://cottstrees.com/>: HTTP status code is not handled or not allowed
DEBUG: Scraped from <200 https://www.independenttree.com/contact-us/>
{'emails': ['info@independenttree.com'],
 'facebook': 'https://www.facebook.com/IndependentTreeOH/',
 'instagram': 'https://www.instagram.com/independenttreeservice/',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET http://ohiotreecare.com/> (referer: None)
DEBUG: Crawled (200) <GET http://jstreeservicesllc.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://roquetree.com/robots.txt> from <GET http://roquetree.com/robots.txt>
DEBUG: Crawled (200) <GET http://www.kiddertreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/cleveland/?utm_source=GMB&utm_medium=organic&utm_campaign=AvonLake> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.kiddertreemov.com/> from <GET http://www.kiddertreeservice.com/>
DEBUG: Redirecting (301) to <GET http://haneytreeservice.com/robots.txt> from <GET http://www.haneytreeservice.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://www.basictreecare.com/robots.txt> from <GET http://www.basictreecare.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.ewsmithtree.com/> (referer: None)
DEBUG: Crawled (200) <GET https://treesaremybusiness.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.kiddertreemov.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://jstreeservicesllc.com/> from <GET http://www.jstreeservicesllc.com/>
DEBUG: Crawled (200) <GET http://haneytreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://herculestree.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.jttreeservicellc.com/robots.txt> from <GET http://www.jttreeservicellc.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.kiddertreemov.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.basictreecare.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://haneytreeservice.com/> from <GET http://www.haneytreeservice.com/>
DEBUG: Crawled (200) <GET https://treesaremybusiness.com/contact-us/> (referer: https://treesaremybusiness.com/)
DEBUG: Crawled (200) <GET https://roquetree.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.basictreecare.com/> from <GET http://www.basictreecare.com/>
DEBUG: Crawled (200) <GET http://haneytreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://www.timberlandtreeohio.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.ewsmithtree.com/contact-us/> (referer: https://www.ewsmithtree.com/)
DEBUG: Redirecting (301) to <GET https://roquetree.com/> from <GET http://roquetree.com/>
DEBUG: Crawled (200) <GET https://herculestree.com/contact> (referer: https://herculestree.com/)
DEBUG: Redirecting (301) to <GET http://haneytreeservice.com/index.html> from <GET http://haneytreeservice.com/>
DEBUG: Crawled (200) <GET https://www.basictreecare.com/> (referer: None)
DEBUG: Crawled (200) <GET https://jstreeservicesllc.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.roquetree.com/> from <GET https://roquetree.com/>
DEBUG: Crawled (403) <GET https://www.kandatreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET https://www.kandatreeservice.com/> (referer: None)
INFO: Ignoring response <403 https://www.kandatreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET http://haneytreeservice.com/index.html> (referer: None)
DEBUG: Crawled (200) <GET https://www.davey.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.jttreeservicellc.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.ripleytreeservice.com/robots.txt> from <GET http://www.ripleytreeservice.com/robots.txt>
DEBUG: Crawled (403) <GET http://www.jjsfamilytreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://www.jjsfamilytreeservice.com/> (referer: None)
INFO: Ignoring response <403 http://www.jjsfamilytreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET https://www.jttreeservicellc.com/> from <GET http://www.jttreeservicellc.com/>
DEBUG: Crawled (200) <GET https://herculestree.com/contact/request-quote> (referer: https://herculestree.com/)
DEBUG: Crawled (200) <GET https://herculestree.com/contact/request-quote/> (referer: https://herculestree.com/)
DEBUG: Crawled (200) <GET https://www.roquetree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.jttreeservicellc.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://extremetreeservicestoledo.com/robots.txt> from <GET https://www.extremetreeservicestoledo.com/robots.txt>
DEBUG: Crawled (200) <GET https://jstreeservicesllc.com/> (referer: None)
DEBUG: Crawled (200) <GET https://herculestree.com/contact/> (referer: https://herculestree.com/)
DEBUG: Crawled (200) <GET https://www.davey.com/residential-tree-services/local-offices/columbus-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Columbus> (referer: None)
DEBUG: Crawled (200) <GET https://parkstree.net/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://oak-tree-services-tree-service.business.site/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://parkstree.net/> (referer: None)
DEBUG: Crawled (200) <GET https://www.roquetree.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.savatree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://starwoodtree.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.ripleytreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://parkstree.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.ripleytreeservice.com/> from <GET http://www.ripleytreeservice.com/>
DEBUG: Crawled (200) <GET https://oak-tree-services-tree-service.business.site/?utm_source=gmb&utm_medium=referral> (referer: None)
DEBUG: Crawled (200) <GET https://www.rawtreeserv.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://extremetreeservicestoledo.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://parkstree.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.savatree.com/dayton-ohio-tree-service-lawn-care?utm_source=GMB&utm_medium=organic&utm_campaign=dayton> (referer: None)
DEBUG: Redirecting (301) to <GET https://rawtreeserv.com/> from <GET https://www.rawtreeserv.com/>
DEBUG: Redirecting (301) to <GET https://extremetreeservicestoledo.com/> from <GET https://www.extremetreeservicestoledo.com/>
DEBUG: Crawled (200) <GET https://challengerstreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.ripleytreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.davey.com/residential-tree-services/local-offices/cincinnati-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Cincinnati> (referer: None)
ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\engine.py", line 152, in _next_request
    request = next(self.slot.start_requests)
  File "C:\Users\moffi\PycharmProjects\scraping_web\scraper\scraper\spiders\website_spider.py", line 40, in start_requests
    yield scrapy.Request(url=url, callback=self.parse_website)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\http\request\__init__.py", line 60, in __init__
    self._set_url(url)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\http\request\__init__.py", line 108, in _set_url
    raise ValueError(f'Missing scheme in request url: {self._url}')
ValueError: Missing scheme in request url: adres:%2050%20Hillside%20Dr,%20Delaware,%20OH%2043015,%20Stany%20Zjednoczone
DEBUG: Crawled (200) <GET https://extremetreeservicestoledo.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://jstreeservicesllc.com/contact-us/> from <GET http://www.jstreeservicesllc.com/contact-us>
DEBUG: Crawled (200) <GET https://www.stevestoledotree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://jstreeservicesllc.com/contact-us/> (referer: https://jstreeservicesllc.com/)
DEBUG: Crawled (200) <GET https://extremetreeservicestoledo.com/> (referer: None)
DEBUG: Crawled (200) <GET https://challengerstreeservice.com/> (referer: None)
DEBUG: Scraped from <200 https://jstreeservicesllc.com/contact-us/>
{'emails': ['j_s.tree@yahoo.com'],
 'facebook': 'http://www.facebook.com/jandstreeservices',
 'instagram': 'http://www.instagram.com/jandstreeservices',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://rawtreeserv.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.stevestoledotree.com/?utm_source=googlelocal&utm_medium=organic> (referer: None)
DEBUG: Crawled (200) <GET http://jstreeservicesllc.com/contact-us/> (referer: None)
DEBUG: Crawled (200) <GET https://www.davey.com/about/contact-us/?type=residential> (referer: https://www.davey.com/residential-tree-services/local-offices/columbus-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Columbus)
DEBUG: Crawled (200) <GET https://extremetreeservicestoledo.com/contact/> (referer: https://extremetreeservicestoledo.com/)
DEBUG: Scraped from <200 http://jstreeservicesllc.com/contact-us/>
{'emails': ['j_s.tree@yahoo.com'],
 'facebook': 'http://www.facebook.com/jandstreeservices',
 'instagram': 'http://www.instagram.com/jandstreeservices',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://rawtreeserv.com/> (referer: None)
DEBUG: Scraped from <200 https://extremetreeservicestoledo.com/contact/>
{'emails': ['extremetreeswanton@gmail.com'],
 'facebook': 'https://www.facebook.com/ToledosExtremeTree/',
 'instagram': '',
 'linkedin': '',
 'twitter': 'https://twitter.com/SuperClimber101'}
DEBUG: Crawled (200) <GET https://starwoodtree.com/contact-us/> (referer: https://starwoodtree.com/)
DEBUG: Scraped from <200 https://starwoodtree.com/contact-us/>
{'emails': ['support@starwoodtree.com'],
 'facebook': 'https://www.facebook.com/Starwoodtreeservice/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET http://www.treeservicedelawareoh.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://www.treeservicedelawareoh.com/> (referer: None)
DEBUG: Crawled (200) <GET http://www.treeservicedelawareoh.com/contact-us/> (referer: http://www.treeservicedelawareoh.com/)
DEBUG: Crawled (200) <GET https://www.davey.com/about/contact-us/?type=nonservicerequest> (referer: https://www.davey.com/residential-tree-services/local-offices/columbus-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Columbus)
DEBUG: Crawled (200) <GET https://www.davey.com/about/contact-us/?type=nonresidential> (referer: https://www.davey.com/residential-tree-services/local-offices/columbus-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Columbus)
INFO: Closing spider (finished)
INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 39466,
 'downloader/request_count': 147,
 'downloader/request_method_count/GET': 147,
 'downloader/response_bytes': 2116595,
 'downloader/response_count': 147,
 'downloader/response_status_count/200': 95,
 'downloader/response_status_count/301': 34,
 'downloader/response_status_count/403': 16,
 'downloader/response_status_count/404': 2,
 'dupefilter/filtered': 29,
 'elapsed_time_seconds': 7.155596,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 9, 12, 30, 49, 142098),
 'httpcompression/response_bytes': 9555494,
 'httpcompression/response_count': 102,
 'httperror/response_ignored_count': 8,
 'httperror/response_ignored_status_count/403': 8,
 'item_scraped_count': 5,
 'log_count/DEBUG': 178,
 'log_count/ERROR': 1,
 'log_count/INFO': 18,
 'request_depth_max': 1,
 'response_received_count': 113,
 'robotstxt/request_count': 52,
 'robotstxt/response_count': 52,
 'robotstxt/response_status_count/200': 42,
 'robotstxt/response_status_count/403': 8,
 'robotstxt/response_status_count/404': 2,
 'scheduler/dequeued': 84,
 'scheduler/dequeued/memory': 84,
 'scheduler/enqueued': 84,
 'scheduler/enqueued/memory': 84,
 'start_time': datetime.datetime(2022, 12, 9, 12, 30, 41, 986502)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: b29100bc1128a28d
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
DEBUG: Attempting to acquire lock 1772028846064 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 1772028846064 acquired on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Attempting to release lock 1772028846064 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 1772028846064 released on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Crawled (200) <GET http://www.columbustreeservicesllc.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET https://trapperstreeservice.com/robots.txt> (referer: None)
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Rule at line 14 without any user agent to enforce it on.
DEBUG: Rule at line 16 without any user agent to enforce it on.
DEBUG: Rule at line 35 without any user agent to enforce it on.
DEBUG: Rule at line 42 without any user agent to enforce it on.
DEBUG: Rule at line 43 without any user agent to enforce it on.
DEBUG: Rule at line 44 without any user agent to enforce it on.
DEBUG: Rule at line 45 without any user agent to enforce it on.
DEBUG: Rule at line 46 without any user agent to enforce it on.
DEBUG: Rule at line 47 without any user agent to enforce it on.
DEBUG: Rule at line 69 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://www.treetechohio.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://www.columbustreeservicesllc.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.fandftrees.com/robots.txt> from <GET http://www.fandftrees.com/robots.txt>
DEBUG: Crawled (403) <GET https://trapperstreeservice.com/> (referer: None)
DEBUG: Crawled (403) <GET https://www.midohiotree.org/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.treetechohio.com/> (referer: None)
DEBUG: Filtered duplicate request: <GET http://hardwicktreecare.com/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG: Crawled (403) <GET https://www.midohiotree.org/> (referer: None)
INFO: Ignoring response <403 https://trapperstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://www.fandftrees.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://hardwicktreecare.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.fandftrees.com/> from <GET http://www.fandftrees.com/>
DEBUG: Crawled (200) <GET http://www.charteroakscompany.com/robots.txt> (referer: None)
INFO: Ignoring response <403 https://www.midohiotree.org/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://www.fandftrees.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.hardwicktreecare.com/> from <GET http://hardwicktreecare.com/>
DEBUG: Redirecting (301) to <GET https://starwoodtree.com/robots.txt> from <GET http://starwoodtree.com/robots.txt>
DEBUG: Crawled (404) <GET http://ohiotreeandexcavating.net/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.hardwicktreecare.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.russelltreeexperts.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.treetechohio.com/contact> (referer: https://www.treetechohio.com/)
DEBUG: Crawled (200) <GET https://www.hardwicktreecare.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://ohiotreeandexcavating.com/> from <GET http://ohiotreeandexcavating.net/>
DEBUG: Crawled (403) <GET https://www.tackettstreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://www.treesaremybusiness.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.russelltreeexperts.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.hardwicktreecare.com/contact-hardwick-tree-care> (referer: https://www.hardwicktreecare.com/)
DEBUG: Redirecting (301) to <GET https://ohiotreeandexcavating.com/robots.txt> from <GET http://ohiotreeandexcavating.com/robots.txt>
DEBUG: Crawled (403) <GET https://www.tackettstreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://ohiotreeandexcavating.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/akron/?utm_source=GMB&utm_medium=organic&utm_campaign=doylestown> (referer: None)
DEBUG: Crawled (403) <GET http://www.hackettstreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.treesaremybusiness.com/> from <GET http://www.treesaremybusiness.com/>
DEBUG: Redirecting (301) to <GET https://ohiotreeandexcavating.com/> from <GET http://ohiotreeandexcavating.com/>
DEBUG: Crawled (404) <GET http://ohiotreecare.com/robots.txt> (referer: None)
DEBUG: Rule at line 1 without any user agent to enforce it on.
DEBUG: Rule at line 2 without any user agent to enforce it on.
DEBUG: Rule at line 8 without any user agent to enforce it on.
DEBUG: Rule at line 9 without any user agent to enforce it on.
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Rule at line 11 without any user agent to enforce it on.
DEBUG: Rule at line 12 without any user agent to enforce it on.
INFO: Ignoring response <403 https://www.tackettstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (403) <GET http://www.hackettstreeservice.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.charteroakscompany.com/> from <GET http://www.charteroakscompany.com/>
DEBUG: Crawled (403) <GET http://deeprootedtreeservice.com/robots.txt> (referer: None)
INFO: Ignoring response <403 http://www.hackettstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://specialtytreeohio.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://deeprootedtreeservice.com/> (referer: None)
DEBUG: Redirecting (301) to <GET http://www.timberlandtreeohio.com/robots.txt> from <GET http://timberlandtreeohio.com/robots.txt>
INFO: Ignoring response <403 http://deeprootedtreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://ohiotreeandexcavating.com/> (referer: None)
DEBUG: Crawled (200) <GET https://specialtytreeohio.com/> (referer: None)
DEBUG: Crawled (200) <GET http://herculestree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://starwoodtree.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://treesaremybusiness.com/> from <GET https://www.treesaremybusiness.com/>
DEBUG: Crawled (200) <GET https://www.charteroakscompany.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.independenttree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://www.timberlandtreeohio.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.whymonster.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://jstreeservicesllc.com/robots.txt> from <GET http://www.jstreeservicesllc.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://starwoodtree.com/> from <GET http://starwoodtree.com/>
DEBUG: Crawled (403) <GET http://cottstrees.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://www.timberlandtreeohio.com/> from <GET http://timberlandtreeohio.com/>
DEBUG: Redirecting (301) to <GET https://www.monstertreeservice.com/akron/contact-us/> from <GET https://www.whymonster.com/akron/contact-us/>
DEBUG: Crawled (403) <GET http://cottstrees.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://herculestree.com/> from <GET http://herculestree.com/>
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/akron/contact-us/> (referer: https://www.monstertreeservice.com/akron/?utm_source=GMB&utm_medium=organic&utm_campaign=doylestown)
INFO: Ignoring response <403 http://cottstrees.com/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET https://roquetree.com/robots.txt> from <GET http://roquetree.com/robots.txt>
DEBUG: Crawled (200) <GET http://www.timberlandtreeohio.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.independenttree.com/?utm_source=gmb&utm_medium=local&utm_campaign=website> (referer: None)
DEBUG: Crawled (200) <GET https://www.ewsmithtree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://treesaremybusiness.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://jstreeservicesllc.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.monstertreeservice.com/cleveland/?utm_source=GMB&utm_medium=organic&utm_campaign=AvonLake> (referer: None)
DEBUG: Crawled (200) <GET http://ohiotreecare.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.basictreecare.com/robots.txt> from <GET http://www.basictreecare.com/robots.txt>
DEBUG: Redirecting (301) to <GET http://haneytreeservice.com/robots.txt> from <GET http://www.haneytreeservice.com/robots.txt>
DEBUG: Crawled (200) <GET http://www.kiddertreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.ewsmithtree.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.kiddertreemov.com/> from <GET http://www.kiddertreeservice.com/>
DEBUG: Crawled (200) <GET https://treesaremybusiness.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://jstreeservicesllc.com/> from <GET http://www.jstreeservicesllc.com/>
DEBUG: Crawled (200) <GET https://www.independenttree.com/contact-us/> (referer: https://www.independenttree.com/?utm_source=gmb&utm_medium=local&utm_campaign=website)
DEBUG: Crawled (200) <GET https://roquetree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://herculestree.com/> (referer: None)
DEBUG: Crawled (200) <GET http://haneytreeservice.com/robots.txt> (referer: None)
DEBUG: Scraped from <200 https://www.independenttree.com/contact-us/>
{'emails': ['info@independenttree.com'],
 'facebook': 'https://www.facebook.com/IndependentTreeOH/',
 'instagram': 'https://www.instagram.com/independenttreeservice/',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://www.kiddertreemov.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://roquetree.com/> from <GET http://roquetree.com/>
DEBUG: Redirecting (301) to <GET https://www.jttreeservicellc.com/robots.txt> from <GET http://www.jttreeservicellc.com/robots.txt>
DEBUG: Crawled (200) <GET https://treesaremybusiness.com/contact-us/> (referer: https://treesaremybusiness.com/)
DEBUG: Redirecting (301) to <GET http://haneytreeservice.com/> from <GET http://www.haneytreeservice.com/>
DEBUG: Crawled (200) <GET https://www.basictreecare.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.kiddertreemov.com/> (referer: None)
DEBUG: Crawled (200) <GET http://haneytreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.roquetree.com/> from <GET https://roquetree.com/>
DEBUG: Redirecting (301) to <GET https://www.basictreecare.com/> from <GET http://www.basictreecare.com/>
DEBUG: Crawled (200) <GET http://www.timberlandtreeohio.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.ewsmithtree.com/contact-us/> (referer: https://www.ewsmithtree.com/)
DEBUG: Redirecting (301) to <GET http://haneytreeservice.com/index.html> from <GET http://haneytreeservice.com/>
DEBUG: Crawled (200) <GET https://herculestree.com/contact> (referer: https://herculestree.com/)
DEBUG: Crawled (200) <GET https://jstreeservicesllc.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.basictreecare.com/> (referer: None)
DEBUG: Crawled (200) <GET https://starwoodtree.com/> (referer: None)
DEBUG: Crawled (200) <GET https://herculestree.com/contact/> (referer: https://herculestree.com/)
DEBUG: Crawled (200) <GET https://www.davey.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://haneytreeservice.com/index.html> (referer: None)
DEBUG: Crawled (403) <GET https://www.kandatreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.jttreeservicellc.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET https://www.kandatreeservice.com/> (referer: None)
DEBUG: Crawled (403) <GET http://www.jjsfamilytreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://www.jjsfamilytreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.roquetree.com/robots.txt> (referer: None)
INFO: Ignoring response <403 https://www.kandatreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET https://www.ripleytreeservice.com/robots.txt> from <GET http://www.ripleytreeservice.com/robots.txt>
INFO: Ignoring response <403 http://www.jjsfamilytreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET https://www.jttreeservicellc.com/> from <GET http://www.jttreeservicellc.com/>
DEBUG: Crawled (200) <GET https://challengerstreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://herculestree.com/contact/request-quote> (referer: https://herculestree.com/)
DEBUG: Crawled (200) <GET https://www.roquetree.com/> (referer: None)
DEBUG: Crawled (200) <GET https://parkstree.net/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://herculestree.com/contact/request-quote/> (referer: https://herculestree.com/)
DEBUG: Crawled (200) <GET https://www.davey.com/residential-tree-services/local-offices/columbus-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Columbus> (referer: None)
DEBUG: Crawled (200) <GET https://parkstree.net/> (referer: None)
DEBUG: Crawled (200) <GET https://www.jttreeservicellc.com/> (referer: None)
DEBUG: Crawled (200) <GET https://oak-tree-services-tree-service.business.site/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.savatree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://jstreeservicesllc.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://extremetreeservicestoledo.com/robots.txt> from <GET https://www.extremetreeservicestoledo.com/robots.txt>
DEBUG: Crawled (200) <GET https://parkstree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.rawtreeserv.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://oak-tree-services-tree-service.business.site/?utm_source=gmb&utm_medium=referral> (referer: None)
DEBUG: Crawled (200) <GET https://www.savatree.com/dayton-ohio-tree-service-lawn-care?utm_source=GMB&utm_medium=organic&utm_campaign=dayton> (referer: None)
DEBUG: Crawled (200) <GET https://parkstree.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.ripleytreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://rawtreeserv.com/> from <GET https://www.rawtreeserv.com/>
DEBUG: Redirecting (301) to <GET https://www.ripleytreeservice.com/> from <GET http://www.ripleytreeservice.com/>
DEBUG: Crawled (200) <GET https://extremetreeservicestoledo.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.davey.com/residential-tree-services/local-offices/cincinnati-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Cincinnati> (referer: None)
DEBUG: Redirecting (301) to <GET https://extremetreeservicestoledo.com/> from <GET https://www.extremetreeservicestoledo.com/>
DEBUG: Crawled (200) <GET https://starwoodtree.com/contact-us/> (referer: https://starwoodtree.com/)
DEBUG: Redirecting (301) to <GET http://jstreeservicesllc.com/contact-us/> from <GET http://www.jstreeservicesllc.com/contact-us>
DEBUG: Crawled (200) <GET https://www.ripleytreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://extremetreeservicestoledo.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.stevestoledotree.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://rawtreeserv.com/robots.txt> (referer: None)
DEBUG: Scraped from <200 https://starwoodtree.com/contact-us/>
{'emails': ['support@starwoodtree.com'],
 'facebook': 'https://www.facebook.com/Starwoodtreeservice/',
 'instagram': '',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET http://blackstreemov.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://extremetreeservicestoledo.com/> (referer: None)
DEBUG: Crawled (200) <GET https://jstreeservicesllc.com/contact-us/> (referer: https://jstreeservicesllc.com/)
DEBUG: Redirecting (301) to <GET https://www.blackstreemov.com/> from <GET http://blackstreemov.com/>
DEBUG: Crawled (200) <GET https://www.davey.com/about/contact-us/?type=residential> (referer: https://www.davey.com/residential-tree-services/local-offices/columbus-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Columbus)
DEBUG: Crawled (403) <GET http://www.haymakertreeandlawn.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://affordabletreeservicesofohiollc.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET http://www.haymakertreeandlawn.com/> (referer: None)
DEBUG: Scraped from <200 https://jstreeservicesllc.com/contact-us/>
{'emails': ['j_s.tree@yahoo.com'],
 'facebook': 'http://www.facebook.com/jandstreeservices',
 'instagram': 'http://www.instagram.com/jandstreeservices',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://rawtreeserv.com/> (referer: None)
DEBUG: Crawled (200) <GET https://affordabletreeservicesofohiollc.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.davey.com/residential-tree-services/local-offices/northeast-cleveland-tree-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Northeast%20Cleveland> (referer: None)
DEBUG: Crawled (200) <GET https://www.blackstreemov.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://jstreeservicesllc.com/contact-us/> (referer: None)
INFO: Ignoring response <403 http://www.haymakertreeandlawn.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://treesrus-treeservice.business.site/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://extremetreeservicestoledo.com/contact/> (referer: https://extremetreeservicestoledo.com/)
DEBUG: Crawled (403) <GET http://www.cstreemulch.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.blackstreemov.com/> (referer: None)
DEBUG: Scraped from <200 http://jstreeservicesllc.com/contact-us/>
{'emails': ['j_s.tree@yahoo.com'],
 'facebook': 'http://www.facebook.com/jandstreeservices',
 'instagram': 'http://www.instagram.com/jandstreeservices',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (403) <GET http://www.cstreemulch.com/> (referer: None)
DEBUG: Scraped from <200 https://extremetreeservicestoledo.com/contact/>
{'emails': ['extremetreeswanton@gmail.com'],
 'facebook': 'https://www.facebook.com/ToledosExtremeTree/',
 'instagram': '',
 'linkedin': '',
 'twitter': 'https://twitter.com/SuperClimber101'}
DEBUG: Crawled (403) <GET http://www.ashtreeservicepro.com/robots.txt> (referer: None)
INFO: Ignoring response <403 http://www.cstreemulch.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (403) <GET http://www.ashtreeservicepro.com/> (referer: None)
DEBUG: Crawled (200) <GET https://treesrus-treeservice.business.site/?utm_source=gmb&utm_medium=referral> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.roguetreesolutions.com/robots.txt> from <GET http://www.roguetreesolutions.com/robots.txt>
DEBUG: Redirecting (301) to <GET http://www.toddstreeservice.com/robots.txt> from <GET http://toddstreeservice.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.blackstreemov.com/contact> (referer: https://www.blackstreemov.com/)
INFO: Ignoring response <403 http://www.ashtreeservicepro.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://extremetreeohio.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://hoffmantreeservice.com/robots.txt> from <GET http://hoffmantreeservice.com/robots.txt>
DEBUG: Crawled (200) <GET http://urbanloggersohio.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://extremetreeohio.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://urbanloggersohio.com/> from <GET http://urbanloggersohio.com/>
DEBUG: Crawled (404) <GET https://www.roguetreesolutions.com/robots.txt> (referer: None)
DEBUG: Rule at line 1 without any user agent to enforce it on.
DEBUG: Rule at line 2 without any user agent to enforce it on.
DEBUG: Rule at line 14 without any user agent to enforce it on.
DEBUG: Rule at line 15 without any user agent to enforce it on.
DEBUG: Rule at line 23 without any user agent to enforce it on.
DEBUG: Rule at line 28 without any user agent to enforce it on.
DEBUG: Rule at line 29 without any user agent to enforce it on.
DEBUG: Rule at line 32 without any user agent to enforce it on.
DEBUG: Rule at line 33 without any user agent to enforce it on.
DEBUG: Rule at line 45 without any user agent to enforce it on.
DEBUG: Rule at line 46 without any user agent to enforce it on.
DEBUG: Rule at line 47 without any user agent to enforce it on.
DEBUG: Rule at line 48 without any user agent to enforce it on.
DEBUG: Rule at line 49 without any user agent to enforce it on.
DEBUG: Rule at line 50 without any user agent to enforce it on.
DEBUG: Rule at line 51 without any user agent to enforce it on.
DEBUG: Rule at line 52 without any user agent to enforce it on.
DEBUG: Rule at line 53 without any user agent to enforce it on.
DEBUG: Rule at line 54 without any user agent to enforce it on.
DEBUG: Rule at line 55 without any user agent to enforce it on.
DEBUG: Rule at line 57 without any user agent to enforce it on.
DEBUG: Rule at line 58 without any user agent to enforce it on.
DEBUG: Rule at line 59 without any user agent to enforce it on.
DEBUG: Rule at line 61 without any user agent to enforce it on.
DEBUG: Rule at line 62 without any user agent to enforce it on.
DEBUG: Rule at line 63 without any user agent to enforce it on.
DEBUG: Rule at line 64 without any user agent to enforce it on.
DEBUG: Rule at line 65 without any user agent to enforce it on.
DEBUG: Rule at line 66 without any user agent to enforce it on.
DEBUG: Rule at line 67 without any user agent to enforce it on.
DEBUG: Rule at line 70 without any user agent to enforce it on.
DEBUG: Rule at line 72 without any user agent to enforce it on.
DEBUG: Rule at line 78 without any user agent to enforce it on.
DEBUG: Rule at line 79 without any user agent to enforce it on.
DEBUG: Rule at line 80 without any user agent to enforce it on.
DEBUG: Rule at line 81 without any user agent to enforce it on.
DEBUG: Rule at line 87 without any user agent to enforce it on.
DEBUG: Rule at line 88 without any user agent to enforce it on.
DEBUG: Rule at line 92 without any user agent to enforce it on.
DEBUG: Rule at line 94 without any user agent to enforce it on.
DEBUG: Rule at line 96 without any user agent to enforce it on.
DEBUG: Rule at line 98 without any user agent to enforce it on.
DEBUG: Rule at line 100 without any user agent to enforce it on.
DEBUG: Rule at line 101 without any user agent to enforce it on.
DEBUG: Rule at line 104 without any user agent to enforce it on.
DEBUG: Rule at line 106 without any user agent to enforce it on.
DEBUG: Rule at line 107 without any user agent to enforce it on.
DEBUG: Rule at line 108 without any user agent to enforce it on.
DEBUG: Rule at line 109 without any user agent to enforce it on.
DEBUG: Rule at line 110 without any user agent to enforce it on.
DEBUG: Rule at line 111 without any user agent to enforce it on.
DEBUG: Rule at line 112 without any user agent to enforce it on.
DEBUG: Rule at line 119 without any user agent to enforce it on.
DEBUG: Rule at line 120 without any user agent to enforce it on.
DEBUG: Rule at line 121 without any user agent to enforce it on.
DEBUG: Rule at line 122 without any user agent to enforce it on.
DEBUG: Rule at line 123 without any user agent to enforce it on.
DEBUG: Rule at line 128 without any user agent to enforce it on.
DEBUG: Rule at line 145 without any user agent to enforce it on.
DEBUG: Rule at line 155 without any user agent to enforce it on.
DEBUG: Rule at line 165 without any user agent to enforce it on.
DEBUG: Crawled (404) <GET http://www.boonestreeservice.com/robots.txt> (referer: None)
DEBUG: Rule at line 2 without any user agent to enforce it on.
DEBUG: Rule at line 8 without any user agent to enforce it on.
DEBUG: Rule at line 9 without any user agent to enforce it on.
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Crawled (403) <GET http://www.toddstreeservice.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.roguetreesolutions.com/> from <GET http://www.roguetreesolutions.com/>
DEBUG: Crawled (200) <GET https://hoffmantreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://rstreeservicellc.com/robots.txt> (referer: None)
DEBUG: Rule at line 1 without any user agent to enforce it on.
DEBUG: Rule at line 5 without any user agent to enforce it on.
DEBUG: Rule at line 9 without any user agent to enforce it on.
DEBUG: Rule at line 14 without any user agent to enforce it on.
DEBUG: Rule at line 19 without any user agent to enforce it on.
DEBUG: Rule at line 21 without any user agent to enforce it on.
DEBUG: Rule at line 22 without any user agent to enforce it on.
DEBUG: Rule at line 23 without any user agent to enforce it on.
DEBUG: Rule at line 26 without any user agent to enforce it on.
DEBUG: Rule at line 28 without any user agent to enforce it on.
DEBUG: Rule at line 29 without any user agent to enforce it on.
DEBUG: Rule at line 31 without any user agent to enforce it on.
DEBUG: Rule at line 32 without any user agent to enforce it on.
DEBUG: Rule at line 33 without any user agent to enforce it on.
DEBUG: Rule at line 34 without any user agent to enforce it on.
DEBUG: Rule at line 35 without any user agent to enforce it on.
DEBUG: Rule at line 36 without any user agent to enforce it on.
DEBUG: Rule at line 37 without any user agent to enforce it on.
DEBUG: Rule at line 39 without any user agent to enforce it on.
DEBUG: Rule at line 40 without any user agent to enforce it on.
DEBUG: Rule at line 43 without any user agent to enforce it on.
DEBUG: Rule at line 44 without any user agent to enforce it on.
DEBUG: Rule at line 46 without any user agent to enforce it on.
DEBUG: Rule at line 48 without any user agent to enforce it on.
DEBUG: Rule at line 51 without any user agent to enforce it on.
DEBUG: Rule at line 53 without any user agent to enforce it on.
DEBUG: Rule at line 54 without any user agent to enforce it on.
DEBUG: Rule at line 55 without any user agent to enforce it on.
DEBUG: Rule at line 56 without any user agent to enforce it on.
DEBUG: Rule at line 57 without any user agent to enforce it on.
DEBUG: Rule at line 58 without any user agent to enforce it on.
DEBUG: Rule at line 60 without any user agent to enforce it on.
DEBUG: Rule at line 61 without any user agent to enforce it on.
DEBUG: Rule at line 62 without any user agent to enforce it on.
DEBUG: Rule at line 63 without any user agent to enforce it on.
DEBUG: Rule at line 64 without any user agent to enforce it on.
DEBUG: Rule at line 65 without any user agent to enforce it on.
DEBUG: Rule at line 66 without any user agent to enforce it on.
DEBUG: Rule at line 67 without any user agent to enforce it on.
DEBUG: Rule at line 68 without any user agent to enforce it on.
DEBUG: Rule at line 69 without any user agent to enforce it on.
DEBUG: Rule at line 70 without any user agent to enforce it on.
DEBUG: Rule at line 71 without any user agent to enforce it on.
DEBUG: Rule at line 72 without any user agent to enforce it on.
DEBUG: Rule at line 73 without any user agent to enforce it on.
DEBUG: Rule at line 74 without any user agent to enforce it on.
DEBUG: Rule at line 75 without any user agent to enforce it on.
DEBUG: Rule at line 76 without any user agent to enforce it on.
DEBUG: Rule at line 77 without any user agent to enforce it on.
DEBUG: Rule at line 78 without any user agent to enforce it on.
DEBUG: Rule at line 79 without any user agent to enforce it on.
DEBUG: Rule at line 80 without any user agent to enforce it on.
DEBUG: Rule at line 81 without any user agent to enforce it on.
DEBUG: Rule at line 83 without any user agent to enforce it on.
DEBUG: Rule at line 85 without any user agent to enforce it on.
DEBUG: Rule at line 86 without any user agent to enforce it on.
DEBUG: Rule at line 87 without any user agent to enforce it on.
DEBUG: Rule at line 88 without any user agent to enforce it on.
DEBUG: Rule at line 89 without any user agent to enforce it on.
DEBUG: Rule at line 90 without any user agent to enforce it on.
DEBUG: Rule at line 91 without any user agent to enforce it on.
DEBUG: Rule at line 92 without any user agent to enforce it on.
DEBUG: Rule at line 93 without any user agent to enforce it on.
DEBUG: Rule at line 94 without any user agent to enforce it on.
DEBUG: Rule at line 95 without any user agent to enforce it on.
DEBUG: Rule at line 96 without any user agent to enforce it on.
DEBUG: Rule at line 98 without any user agent to enforce it on.
DEBUG: Rule at line 99 without any user agent to enforce it on.
DEBUG: Rule at line 100 without any user agent to enforce it on.
DEBUG: Rule at line 101 without any user agent to enforce it on.
DEBUG: Rule at line 102 without any user agent to enforce it on.
DEBUG: Rule at line 103 without any user agent to enforce it on.
DEBUG: Rule at line 104 without any user agent to enforce it on.
DEBUG: Rule at line 106 without any user agent to enforce it on.
DEBUG: Rule at line 107 without any user agent to enforce it on.
DEBUG: Rule at line 108 without any user agent to enforce it on.
DEBUG: Rule at line 110 without any user agent to enforce it on.
DEBUG: Rule at line 112 without any user agent to enforce it on.
DEBUG: Rule at line 113 without any user agent to enforce it on.
DEBUG: Rule at line 114 without any user agent to enforce it on.
DEBUG: Rule at line 115 without any user agent to enforce it on.
DEBUG: Rule at line 116 without any user agent to enforce it on.
DEBUG: Rule at line 117 without any user agent to enforce it on.
DEBUG: Rule at line 121 without any user agent to enforce it on.
DEBUG: Rule at line 122 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://www.stevestoledotree.com/?utm_source=googlelocal&utm_me&%20Excavating%22> (referer: None)
DEBUG: Crawled (200) <GET https://www.roguetreesolutions.com/> (referer: None)
DEBUG: Redirecting (301) to <GET http://www.toddstreeservice.com/> from <GET http://toddstreeservice.com/>
DEBUG: Redirecting (301) to <GET https://hoffmantreeservice.com/> from <GET http://hoffmantreeservice.com/>
DEBUG: Crawled (200) <GET http://www.boonestreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.davey.com/about/contact-us/?type=nonservicerequest> (referer: https://www.davey.com/residential-tree-services/local-offices/columbus-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Columbus)
DEBUG: Crawled (403) <GET http://www.toddstreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://hoffmantreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET http://rstreeservicellc.com/?utm_source=gmb&utm_medium=referral> (referer: None)
DEBUG: Crawled (403) <GET http://www.toddstreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://challengerstreeservice.com/> (referer: None)
DEBUG: Redirecting (302) to <GET https://www.greatdanetreeexperts.com/404.html> from <GET https://www.greatdanetreeexperts.com/robots.txt>
INFO: Ignoring response <403 http://www.toddstreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET https://barbertontree.com/robots.txt> from <GET https://www.barbertontree.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.davey.com/about/contact-us/?type=nonresidential> (referer: https://www.davey.com/residential-tree-services/local-offices/columbus-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Columbus)
DEBUG: Crawled (200) <GET https://www.greatdanetreeexperts.com/404.html> (referer: None)
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Rule at line 11 without any user agent to enforce it on.
DEBUG: Rule at line 12 without any user agent to enforce it on.
DEBUG: Rule at line 15 without any user agent to enforce it on.
DEBUG: Rule at line 17 without any user agent to enforce it on.
DEBUG: Rule at line 28 without any user agent to enforce it on.
DEBUG: Rule at line 43 without any user agent to enforce it on.
DEBUG: Rule at line 44 without any user agent to enforce it on.
DEBUG: Rule at line 45 without any user agent to enforce it on.
DEBUG: Rule at line 46 without any user agent to enforce it on.
DEBUG: Rule at line 47 without any user agent to enforce it on.
DEBUG: Rule at line 48 without any user agent to enforce it on.
DEBUG: Rule at line 49 without any user agent to enforce it on.
DEBUG: Rule at line 50 without any user agent to enforce it on.
DEBUG: Rule at line 54 without any user agent to enforce it on.
DEBUG: Rule at line 57 without any user agent to enforce it on.
DEBUG: Rule at line 62 without any user agent to enforce it on.
DEBUG: Rule at line 63 without any user agent to enforce it on.
DEBUG: Rule at line 67 without any user agent to enforce it on.
DEBUG: Rule at line 88 without any user agent to enforce it on.
DEBUG: Rule at line 136 without any user agent to enforce it on.
DEBUG: Rule at line 141 without any user agent to enforce it on.
DEBUG: Rule at line 144 without any user agent to enforce it on.
DEBUG: Rule at line 147 without any user agent to enforce it on.
DEBUG: Rule at line 156 without any user agent to enforce it on.
DEBUG: Rule at line 157 without any user agent to enforce it on.
DEBUG: Rule at line 191 without any user agent to enforce it on.
DEBUG: Redirecting (301) to <GET https://lamannatreeservice.com/robots.txt> from <GET http://www.lamannatreeservice.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://www.treeservicenow.net/robots.txt> from <GET http://www.treeservicenow.net/robots.txt>
DEBUG: Crawled (200) <GET https://www.davey.com/residential-tree-services/local-offices/akron-tree-service-and-lawn-service/?utm_source=google&utm_medium=organic&utm_campaign=gmb&utm_content=RC_Akron> (referer: None)
DEBUG: Redirecting (301) to <GET https://tree-works-ohio.com/robots.txt> from <GET http://tree-works-ohio.com/robots.txt>
DEBUG: Crawled (200) <GET https://urbanloggersohio.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://lamannatreeservice.com/?robots=1> from <GET https://lamannatreeservice.com/robots.txt>
DEBUG: Crawled (200) <GET https://lamannatreeservice.com/?robots=1> (referer: None)
DEBUG: Redirecting (301) to <GET https://lamannatreeservice.com/> from <GET http://www.lamannatreeservice.com/>
DEBUG: Crawled (200) <GET https://brushbandittree.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://lamannatreeservice.com/?robots=1> from <GET https://lamannatreeservice.com/robots.txt>
DEBUG: Crawled (200) <GET https://premiertreesllc.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://lamannatreeservice.com/?robots=1> (referer: None)
DEBUG: Crawled (200) <GET https://www.greatdanetreeexperts.com/> (referer: None)
DEBUG: Crawled (200) <GET https://lamannatreeservice.com/> (referer: None)
DEBUG: Crawled (200) <GET https://larochetree.com/robots.txt> (referer: None)
DEBUG: Retrying <GET https://premiertreesllc.com/> (failed 1 times): 429 Unknown Status
DEBUG: Redirecting (301) to <GET https://www.larochetree.com/> from <GET https://larochetree.com/>
DEBUG: Crawled (200) <GET https://urbanloggersohio.com/contact-urban-loggers/> (referer: https://urbanloggersohio.com/)
DEBUG: Crawled (200) <GET https://www.delmartreeservices.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://lamannatreeservice.com/contact/> (referer: https://lamannatreeservice.com/)
DEBUG: Crawled (403) <GET http://tomcotreecare.com/robots.txt> (referer: None)
DEBUG: Scraped from <200 https://urbanloggersohio.com/contact-urban-loggers/>
{'emails': ['Info@urbanloggersohio.com'],
 'facebook': '',
 'instagram': 'https://www.instagram.com/urbanloggersllc/',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://www.delmartreeservices.com/> (referer: None)
DEBUG: Retrying <GET https://premiertreesllc.com/> (failed 2 times): 429 Unknown Status
DEBUG: Crawled (403) <GET http://tomcotreecare.com/> (referer: None)
DEBUG: Crawled (200) <GET https://barbertontree.com/robots.txt> (referer: None)
DEBUG: Rule at line 3 without any user agent to enforce it on.
DEBUG: Crawled (403) <GET http://www.dolcestreeservice.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.larochetree.com/robots.txt> (referer: None)
ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\engine.py", line 152, in _next_request
    request = next(self.slot.start_requests)
  File "C:\Users\moffi\PycharmProjects\scraping_web\scraper\scraper\spiders\website_spider.py", line 40, in start_requests
    yield scrapy.Request(url=url, callback=self.parse_website)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\http\request\__init__.py", line 60, in __init__
    self._set_url(url)
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\http\request\__init__.py", line 108, in _set_url
    raise ValueError(f'Missing scheme in request url: {self._url}')
ValueError: Missing scheme in request url: 
DEBUG: Crawled (403) <GET http://www.dolcestreeservice.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://barbertontree.com/> from <GET https://www.barbertontree.com/>
INFO: Ignoring response <403 http://tomcotreecare.com/>: HTTP status code is not handled or not allowed
INFO: Ignoring response <403 http://www.dolcestreeservice.com/>: HTTP status code is not handled or not allowed
DEBUG: Retrying <GET https://barbertontree.com/robots.txt> (failed 1 times): 429 Unknown Status
DEBUG: Crawled (200) <GET https://www.larochetree.com/> (referer: None)
DEBUG: Crawled (200) <GET https://tree-works-ohio.com/robots.txt> (referer: None)
DEBUG: Crawled (404) <GET https://www.napierandson.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://premiertreesllc.com/> (referer: None)
DEBUG: Crawled (200) <GET https://brushbandittree.com/> (referer: None)
DEBUG: Crawled (200) <GET https://treeservicecolumbusohio.net/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.napierandson.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.larochetree.com/contact> (referer: https://www.larochetree.com/)
DEBUG: Redirecting (301) to <GET https://tree-works-ohio.com/> from <GET http://tree-works-ohio.com/>
DEBUG: Retrying <GET https://premiertreesllc.com/contact-us/> (failed 1 times): 429 Unknown Status
DEBUG: Retrying <GET https://premiertreesllc.com/contact-us/> (failed 2 times): 429 Unknown Status
DEBUG: Redirecting (301) to <GET https://treeservicenow.net/robots.txt> from <GET https://www.treeservicenow.net/robots.txt>
DEBUG: Crawled (200) <GET https://brushbandittree.com/contact-us/> (referer: https://brushbandittree.com/)
DEBUG: Crawled (200) <GET http://www.treeservicedelawareoh.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://barbertontree.com/robots.txt> (referer: None)
DEBUG: Rule at line 3 without any user agent to enforce it on.
ERROR: Gave up retrying <GET https://premiertreesllc.com/contact-us/> (failed 3 times): 429 Unknown Status
DEBUG: Crawled (429) <GET https://premiertreesllc.com/contact-us/> (referer: https://premiertreesllc.com/)
DEBUG: Crawled (200) <GET https://treeservicecolumbusohio.net/> (referer: None)
INFO: Ignoring response <429 https://premiertreesllc.com/contact-us/>: HTTP status code is not handled or not allowed
DEBUG: Retrying <GET https://barbertontree.com/> (failed 1 times): 429 Unknown Status
DEBUG: Crawled (200) <GET http://www.treeservicedelawareoh.com/> (referer: None)
DEBUG: Crawled (200) <GET https://tree-works-ohio.com/> (referer: None)
DEBUG: Retrying <GET https://barbertontree.com/> (failed 2 times): 429 Unknown Status
DEBUG: Crawled (200) <GET https://treeservicecolumbusohio.net/contact-us/> (referer: https://treeservicecolumbusohio.net/)
ERROR: Gave up retrying <GET https://barbertontree.com/> (failed 3 times): 429 Unknown Status
DEBUG: Crawled (429) <GET https://barbertontree.com/> (referer: None)
DEBUG: Crawled (200) <GET http://www.treeservicedelawareoh.com/contact-us/> (referer: http://www.treeservicedelawareoh.com/)
INFO: Ignoring response <429 https://barbertontree.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://treeservicenow.net/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://tree-works-ohio.com/contact-tree-works-for-all-your-emergency-services-tree-removal-stump-grinding-etc/> (referer: https://tree-works-ohio.com/)
DEBUG: Redirecting (301) to <GET https://www.treeservicenow.net/> from <GET http://www.treeservicenow.net/>
DEBUG: Redirecting (301) to <GET https://treeservicenow.net/> from <GET https://www.treeservicenow.net/>
DEBUG: Crawled (200) <GET https://treeservicenow.net/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://treeservicenow.net/> (referer: None)
DEBUG: Crawled (200) <GET https://treeservicenow.net/contact_us_1> (referer: https://treeservicenow.net/)
INFO: Closing spider (finished)
INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 64679,
 'downloader/request_count': 245,
 'downloader/request_method_count/GET': 245,
 'downloader/response_bytes': 3503515,
 'downloader/response_count': 245,
 'downloader/response_status_count/200': 146,
 'downloader/response_status_count/301': 55,
 'downloader/response_status_count/302': 1,
 'downloader/response_status_count/403': 29,
 'downloader/response_status_count/404': 5,
 'downloader/response_status_count/429': 9,
 'dupefilter/filtered': 59,
 'elapsed_time_seconds': 11.368154,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 9, 12, 34, 52, 852511),
 'httpcompression/response_bytes': 14896794,
 'httpcompression/response_count': 157,
 'httperror/response_ignored_count': 16,
 'httperror/response_ignored_status_count/403': 14,
 'httperror/response_ignored_status_count/429': 2,
 'item_scraped_count': 6,
 'log_count/DEBUG': 456,
 'log_count/ERROR': 3,
 'log_count/INFO': 26,
 'request_depth_max': 1,
 'response_received_count': 182,
 'retry/count': 7,
 'retry/max_reached': 2,
 'retry/reason_count/429 Unknown Status': 7,
 'robotstxt/request_count': 84,
 'robotstxt/response_count': 84,
 'robotstxt/response_status_count/200': 64,
 'robotstxt/response_status_count/403': 15,
 'robotstxt/response_status_count/404': 5,
 'scheduler/dequeued': 138,
 'scheduler/dequeued/memory': 138,
 'scheduler/enqueued': 138,
 'scheduler/enqueued/memory': 138,
 'start_time': datetime.datetime(2022, 12, 9, 12, 34, 41, 484357)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: d3dfd6babab51b93
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
DEBUG: Redirecting (301) to <GET https://www.cal-a-vie.com/robots.txt> from <GET http://www.cal-a-vie.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://kellysspa.com/robots.txt> from <GET http://kellysspa.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://www.goldenhaven.com/robots.txt> from <GET http://www.goldenhaven.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://www.kellysspa.com/robots.txt> from <GET https://kellysspa.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://www.glenivy.com/robots.txt> from <GET http://www.glenivy.com/robots.txt>
DEBUG: Crawled (200) <GET https://goldendoor.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://ranchovalencia.com/robots.txt> (referer: None)
DEBUG: Attempting to acquire lock 1903208985504 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 1903208985504 acquired on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Attempting to release lock 1903208985504 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 1903208985504 released on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Crawled (200) <GET https://www.cal-a-vie.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.cal-a-vie.com/?utm_source=google&utm_medium=organic&utm_campaign=business-listing> from <GET http://www.cal-a-vie.com/?utm_source=google&utm_medium=organic&utm_campaign=business-listing>
DEBUG: Crawled (200) <GET https://www.kellysspa.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://kellysspa.com/> from <GET http://kellysspa.com/>
DEBUG: Crawled (200) <GET https://www.omnihotels.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://cadayspa.com/robots.txt> from <GET http://cadayspa.com/robots.txt>
DEBUG: Crawled (200) <GET https://goldendoor.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.goldenhaven.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://cadayspa.com/robots.txt> (referer: None)
DEBUG: Filtered duplicate request: <GET https://goldendoor.com/contact-us/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG: Redirecting (301) to <GET https://www.goldenhaven.com/> from <GET http://www.goldenhaven.com/>
DEBUG: Redirecting (301) to <GET https://www.kellysspa.com/> from <GET https://kellysspa.com/>
DEBUG: Crawled (200) <GET https://www.cal-a-vie.com/?utm_source=google&utm_medium=organic&utm_campaign=business-listing> (referer: None)
DEBUG: Crawled (200) <GET https://www.kellysspa.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://cadayspa.com/> from <GET http://cadayspa.com/>
DEBUG: Crawled (200) <GET https://www.osmosis.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://www.wispausa.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://cadayspa.com/> (referer: None)
DEBUG: Crawled (404) <GET https://www.ritzcarlton.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.goldenhaven.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.kellysspa.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://locations.woodhousespas.com/dir/ca/walnut-creek/1636-cypress-st> from <GET http://walnutcreek.woodhousespas.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.osmosis.com/?y_source=1_MjI1MDUzNzItNzE1LWxvY2F0aW9uLndlYnNpdGU%3D> (referer: None)
DEBUG: Crawled (200) <GET https://www.madonnainn.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.cal-a-vie.com/contact-us> (referer: https://www.cal-a-vie.com/?utm_source=google&utm_medium=organic&utm_campaign=business-listing)
DEBUG: Redirecting (301) to <GET http://www.ritzcarlton.com/en/hotels/california/half-moon-bay/spa?scid=45f93f1b-bd77-45c9-8dab-83b6a417f6fe&y_source=1_MTY0OTQwNC03MTUtbG9jYXRpb24ud2Vic2l0ZQ%3D%3D> from <GET https://www.ritzcarlton.com/en/hotels/california/half-moon-bay/spa/?scid=45f93f1b-bd77-45c9-8dab-83b6a417f6fe&y_source=1_MTY0OTQwNC03MTUtbG9jYXRpb24ud2Vic2l0ZQ%3D%3D>
DEBUG: Redirecting (301) to <GET https://www.ritzcarlton.com/en/hotels/california/half-moon-bay/spa?scid=45f93f1b-bd77-45c9-8dab-83b6a417f6fe&y_source=1_MTY0OTQwNC03MTUtbG9jYXRpb24ud2Vic2l0ZQ%3D%3D> from <GET http://www.ritzcarlton.com/en/hotels/california/half-moon-bay/spa?scid=45f93f1b-bd77-45c9-8dab-83b6a417f6fe&y_source=1_MTY0OTQwNC03MTUtbG9jYXRpb24ud2Vic2l0ZQ%3D%3D>
DEBUG: Crawled (200) <GET https://goldendoor.com/contact-us/> (referer: https://goldendoor.com/)
DEBUG: Crawled (200) <GET https://www.omnihotels.com/hotels/san-diego-la-costa/spa?utm_source=GMBlisting&utm_medium=organic> (referer: None)
DEBUG: Crawled (200) <GET https://montereyplazahotel.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://aubergeresorts.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.madonnainn.com/spa> (referer: None)
DEBUG: Crawled (200) <GET https://www.fourseasons.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.ritzcarlton.com/en/hotels/california/half-moon-bay/spa?scid=45f93f1b-bd77-45c9-8dab-83b6a417f6fe&y_source=1_MTY0OTQwNC03MTUtbG9jYXRpb24ud2Vic2l0ZQ%3D%3D> (referer: None)
DEBUG: Redirecting (301) to <GET http://aubergeresorts.com/solage/wellness/spa/?utm_source=google%20my%20business&utm_medium=listing&utm_campaign=visit%20website> from <GET https://aubergeresorts.com/solage/wellness/spa?utm_source=google%20my%20business&utm_medium=listing&utm_campaign=visit%20website>
DEBUG: Crawled (200) <GET https://www.osmosis.com/contact/> (referer: https://www.osmosis.com/?y_source=1_MjI1MDUzNzItNzE1LWxvY2F0aW9uLndlYnNpdGU%3D)
DEBUG: Crawled (200) <GET https://locations.woodhousespas.com/dir/ca/walnut-creek/1636-cypress-st> (referer: None)
DEBUG: Rule at line 1 without any user agent to enforce it on.
DEBUG: Rule at line 15 without any user agent to enforce it on.
DEBUG: Rule at line 22 without any user agent to enforce it on.
DEBUG: Rule at line 23 without any user agent to enforce it on.
DEBUG: Rule at line 24 without any user agent to enforce it on.
DEBUG: Rule at line 25 without any user agent to enforce it on.
DEBUG: Rule at line 26 without any user agent to enforce it on.
DEBUG: Rule at line 30 without any user agent to enforce it on.
DEBUG: Rule at line 31 without any user agent to enforce it on.
DEBUG: Rule at line 32 without any user agent to enforce it on.
DEBUG: Rule at line 35 without any user agent to enforce it on.
DEBUG: Rule at line 36 without any user agent to enforce it on.
DEBUG: Rule at line 38 without any user agent to enforce it on.
DEBUG: Rule at line 40 without any user agent to enforce it on.
DEBUG: Rule at line 41 without any user agent to enforce it on.
DEBUG: Rule at line 62 without any user agent to enforce it on.
DEBUG: Rule at line 67 without any user agent to enforce it on.
DEBUG: Rule at line 68 without any user agent to enforce it on.
DEBUG: Rule at line 74 without any user agent to enforce it on.
DEBUG: Redirecting (301) to <GET https://locations.woodhousespas.com/dir/ca/walnut-creek/1636-cypress-st> from <GET http://walnutcreek.woodhousespas.com/?utm_source=google&utm_medium=yext>
DEBUG: Redirecting (301) to <GET https://www.aubergeresorts.com/solage/wellness/spa/?utm_source=google%20my%20business&utm_medium=listing&utm_campaign=visit%20website> from <GET http://aubergeresorts.com/solage/wellness/spa/?utm_source=google%20my%20business&utm_medium=listing&utm_campaign=visit%20website>
DEBUG: Crawled (200) <GET https://ranchovalencia.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.mytrilogylife.com/robots.txt> from <GET http://www.mytrilogylife.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.omnihotels.com/forms/contact-us> (referer: https://www.omnihotels.com/hotels/san-diego-la-costa/spa?utm_source=GMBlisting&utm_medium=organic)
DEBUG: Redirecting (301) to <GET https://glenivy.com/robots.txt> from <GET https://www.glenivy.com/robots.txt>
DEBUG: Crawled (200) <GET https://montereyplazahotel.com/spa/vista-blue-spa/?utm_source=google&utm_medium=bizlisting&utm_campaign=google_places> (referer: None)
DEBUG: Crawled (200) <GET https://locations.woodhousespas.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://locations.woodhousespas.com/dir/ca/walnut-creek/1636-cypress-st> (referer: None)
DEBUG: Crawled (200) <GET https://www.fourseasons.com/westlakevillage/spa/?seo=google_local_wes6_amer> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.wispausa.com/> from <GET http://www.wispausa.com/>
DEBUG: Crawled (200) <GET https://www.purpleorchid.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.aubergeresorts.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.silveradoresort.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.purpleorchid.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.southcoastwinery.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://montereyplazahotel.com/hotel/contact> (referer: https://montereyplazahotel.com/spa/vista-blue-spa/?utm_source=google&utm_medium=bizlisting&utm_campaign=google_places)
DEBUG: Redirecting (301) to <GET https://www.ritzcarlton.com/en/hotels/california/los-angeles/spa?scid=bb1a189a-fec3-4d19-a255-54ba596febe2> from <GET http://www.ritzcarlton.com/en/hotels/california/los-angeles/spa?scid=bb1a189a-fec3-4d19-a255-54ba596febe2>
DEBUG: Crawled (200) <GET https://www.mytrilogylife.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://the-spring.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.mytrilogylife.com/monarchdunes/spa/> from <GET http://www.mytrilogylife.com/monarchdunes/spa/>
DEBUG: Crawled (200) <GET https://www.purpleorchid.com/contact-us/> (referer: https://www.purpleorchid.com/)
DEBUG: Crawled (200) <GET https://www.aubergeresorts.com/solage/wellness/spa/?utm_source=google%20my%20business&utm_medium=listing&utm_campaign=visit%20website> (referer: None)
DEBUG: Crawled (200) <GET https://www.chaminade.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.ritzcarlton.com/en/hotels/california/santa-barbara/spa?scid=bb1a189a-fec3-4d19-a255-54ba596febe2> from <GET http://www.ritzcarlton.com/en/hotels/california/santa-barbara/spa?scid=bb1a189a-fec3-4d19-a255-54ba596febe2>
DEBUG: Redirecting (301) to <GET https://www.laquintaresort.com/robots.txt> from <GET http://www.laquintaresort.com/robots.txt>
DEBUG: Redirecting (301) to <GET http://boonhotels.com/robots.txt> from <GET http://www.boonhotels.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.mytrilogylife.com/monarchdunes/spa/> (referer: None)
DEBUG: Crawled (200) <GET https://www.ritzcarlton.com/en/hotels/california/los-angeles/spa?scid=bb1a189a-fec3-4d19-a255-54ba596febe2> (referer: None)
DEBUG: Crawled (200) <GET https://www.silveradoresort.com/napa-valley-spa?utm_medium=organic&utm_source=google&utm_campaign=spabusinesslisting> (referer: None)
DEBUG: Retrying <GET https://www.hyatt.com/robots.txt> (failed 1 times): 429 Unknown Status
DEBUG: Crawled (200) <GET https://ranchovalencia.com/contact-us/> (referer: https://ranchovalencia.com/)
DEBUG: Crawled (200) <GET https://glenivy.com/robots.txt> (referer: None)
DEBUG: Retrying <GET https://www.hyatt.com/robots.txt> (failed 2 times): 429 Unknown Status
DEBUG: Crawled (200) <GET https://www.ritzcarlton.com/en/hotels/california/santa-barbara/spa?scid=bb1a189a-fec3-4d19-a255-54ba596febe2> (referer: None)
DEBUG: Crawled (200) <GET https://aubergeresorts.com/solage/contact-us/> (referer: https://www.aubergeresorts.com/)
DEBUG: Crawled (200) <GET http://boonhotels.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.southcoastwinery.com/spa/> from <GET https://www.southcoastwinery.com/spa>
DEBUG: Redirecting (301) to <GET https://wispausa.com/> from <GET https://www.wispausa.com/>
DEBUG: Redirecting (301) to <GET https://www.glenivy.com/> from <GET http://www.glenivy.com/>
ERROR: Gave up retrying <GET https://www.hyatt.com/robots.txt> (failed 3 times): 429 Unknown Status
DEBUG: Crawled (429) <GET https://www.hyatt.com/robots.txt> (referer: None)
DEBUG: Rule at line 1 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://www.laquintaresort.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.boonhotels.com/> from <GET http://www.boonhotels.com/>
DEBUG: Retrying <GET https://www.hyatt.com/en-US/spas/Pacific-Waters-Spa/home.html> (failed 1 times): 429 Unknown Status
DEBUG: Crawled (200) <GET https://www.terranea.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.chaminade.com/?utm_source=google&utm_medium=organic&utm_campaign=gmb> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.laquintaresort.com/> from <GET http://www.laquintaresort.com/>
DEBUG: Retrying <GET https://www.hyatt.com/en-US/spas/Pacific-Waters-Spa/home.html> (failed 2 times): 429 Unknown Status
DEBUG: Crawled (200) <GET https://www.silveradoresort.com/contact-us> (referer: https://www.silveradoresort.com/napa-valley-spa?utm_medium=organic&utm_source=google&utm_campaign=spabusinesslisting)
DEBUG: Crawled (200) <GET https://the-spring.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.urbanretreatspa.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.terranea.com/offers?utm_source=google-knowledge-graph&utm_medium=organic&utm_campaign=my_business_page> (referer: None)
ERROR: Gave up retrying <GET https://www.hyatt.com/en-US/spas/Pacific-Waters-Spa/home.html> (failed 3 times): 429 Unknown Status
DEBUG: Crawled (429) <GET https://www.hyatt.com/en-US/spas/Pacific-Waters-Spa/home.html> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.evo-spa.com/robots.txt> from <GET http://www.evo-spa.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://glenivy.com/> from <GET https://www.glenivy.com/>
INFO: Ignoring response <429 https://www.hyatt.com/en-US/spas/Pacific-Waters-Spa/home.html>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://wispausa.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.urbanretreatspa.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.southcoastwinery.com/spa/> (referer: None)
DEBUG: Redirecting (301) to <GET https://boonhotels.com/> from <GET https://www.boonhotels.com/>
DEBUG: Crawled (200) <GET https://www.constantcontact.com/robots.txt> (referer: None)
DEBUG: Forbidden by robots.txt: <GET https://www.constantcontact.com/legal/service-provider>
DEBUG: Crawled (200) <GET https://www.evo-spa.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://wispausa.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.evo-spa.com/> from <GET http://www.evo-spa.com/>
DEBUG: Redirecting (301) to <GET https://www.sycamoresprings.com/robots.txt> from <GET http://www.sycamoresprings.com/robots.txt>
DEBUG: Crawled (200) <GET https://glenivy.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://davidrubensteinforum.uchicago.edu/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://boonhotels.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.sycamoresprings.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.evo-spa.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.sycamoresprings.com/> from <GET http://www.sycamoresprings.com/>
DEBUG: Crawled (200) <GET https://wispausa.com/contact/> (referer: https://wispausa.com/)
DEBUG: Crawled (200) <GET https://davidrubensteinforum.uchicago.edu/contact/> (referer: https://www.chaminade.com/?utm_source=google&utm_medium=organic&utm_campaign=gmb)
DEBUG: Crawled (200) <GET https://the-spring.com/contact/> (referer: https://the-spring.com/)
DEBUG: Crawled (200) <GET https://boonhotels.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.ranchovalencia.com/robots.txt> from <GET http://www.ranchovalencia.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://sweetwaterspa.com/robots.txt> from <GET http://www.sweetwaterspa.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.sycamoresprings.com/> (referer: None)
DEBUG: Crawled (200) <GET https://glenivy.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.ranchovalencia.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.ranchovalencia.com/> from <GET http://www.ranchovalencia.com/>
DEBUG: Crawled (200) <GET https://boonhotels.com/contact/> (referer: https://boonhotels.com/)
DEBUG: Redirecting (301) to <GET https://banyancay.com/robots.txt> from <GET https://www.banyancay.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.laquintaresort.com/> (referer: None)
DEBUG: Redirecting (301) to <GET http://wecarespa.com/robots.txt> from <GET http://www.wecarespa.com/robots.txt>
DEBUG: Crawled (200) <GET https://azurepalmhotsprings.com/robots.txt> (referer: None)
DEBUG: Crawled (404) <GET https://sweetwaterspa.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://beverlyhotsprings.com/robots.txt> from <GET http://www.beverlyhotsprings.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://sweetwaterspa.com/> from <GET http://www.sweetwaterspa.com/>
DEBUG: Redirecting (301) to <GET https://www.lapeauspafresno.com/robots.txt> from <GET http://www.lapeauspafresno.com/robots.txt>
DEBUG: Crawled (200) <GET https://azurepalmhotsprings.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://ranchovalencia.com/> from <GET https://www.ranchovalencia.com/>
DEBUG: Crawled (200) <GET https://www.canyonranch.com/robots.txt> (referer: None)
DEBUG: Crawled (404) <GET https://sweetwaterspa.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.canyonranch.com/woodside/northern-california-retreat/?utm_source=googlemybusiness&utm_medium=organic&utm_campaign=woodsidegmb&utm_term=googlemybusiness> (referer: None)
DEBUG: Crawled (200) <GET https://www.lapeauspafresno.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://wecarespa.com/robots.txt> (referer: None)
DEBUG: Rule at line 1 without any user agent to enforce it on.
DEBUG: Redirecting (301) to <GET https://www.lapeauspafresno.com/> from <GET http://www.lapeauspafresno.com/>
DEBUG: Crawled (200) <GET http://beverlyhotsprings.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://thesisleyspa.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://amenitiesspa.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://wecarespa.com/> from <GET http://www.wecarespa.com/>
DEBUG: Crawled (200) <GET https://banyancay.com/robots.txt> (referer: None)
DEBUG: Rule at line 2 without any user agent to enforce it on.
DEBUG: Rule at line 7 without any user agent to enforce it on.
DEBUG: Rule at line 9 without any user agent to enforce it on.
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Rule at line 11 without any user agent to enforce it on.
DEBUG: Rule at line 15 without any user agent to enforce it on.
DEBUG: Rule at line 16 without any user agent to enforce it on.
DEBUG: Rule at line 17 without any user agent to enforce it on.
DEBUG: Rule at line 18 without any user agent to enforce it on.
DEBUG: Rule at line 22 without any user agent to enforce it on.
DEBUG: Rule at line 23 without any user agent to enforce it on.
DEBUG: Rule at line 24 without any user agent to enforce it on.
DEBUG: Rule at line 25 without any user agent to enforce it on.
DEBUG: Rule at line 29 without any user agent to enforce it on.
DEBUG: Rule at line 30 without any user agent to enforce it on.
DEBUG: Rule at line 31 without any user agent to enforce it on.
DEBUG: Rule at line 32 without any user agent to enforce it on.
DEBUG: Rule at line 33 without any user agent to enforce it on.
DEBUG: Rule at line 37 without any user agent to enforce it on.
DEBUG: Rule at line 38 without any user agent to enforce it on.
DEBUG: Rule at line 39 without any user agent to enforce it on.
DEBUG: Rule at line 40 without any user agent to enforce it on.
DEBUG: Rule at line 44 without any user agent to enforce it on.
DEBUG: Rule at line 48 without any user agent to enforce it on.
DEBUG: Rule at line 49 without any user agent to enforce it on.
DEBUG: Rule at line 50 without any user agent to enforce it on.
DEBUG: Rule at line 51 without any user agent to enforce it on.
DEBUG: Rule at line 52 without any user agent to enforce it on.
DEBUG: Rule at line 53 without any user agent to enforce it on.
DEBUG: Rule at line 54 without any user agent to enforce it on.
DEBUG: Rule at line 55 without any user agent to enforce it on.
DEBUG: Rule at line 56 without any user agent to enforce it on.
DEBUG: Rule at line 57 without any user agent to enforce it on.
DEBUG: Rule at line 58 without any user agent to enforce it on.
DEBUG: Rule at line 59 without any user agent to enforce it on.
DEBUG: Rule at line 60 without any user agent to enforce it on.
DEBUG: Rule at line 61 without any user agent to enforce it on.
DEBUG: Rule at line 62 without any user agent to enforce it on.
DEBUG: Rule at line 63 without any user agent to enforce it on.
DEBUG: Rule at line 64 without any user agent to enforce it on.
DEBUG: Rule at line 65 without any user agent to enforce it on.
DEBUG: Rule at line 66 without any user agent to enforce it on.
DEBUG: Rule at line 70 without any user agent to enforce it on.
DEBUG: Rule at line 72 without any user agent to enforce it on.
DEBUG: Rule at line 80 without any user agent to enforce it on.
DEBUG: Rule at line 87 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://www.lapeauspafresno.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.indianspringscalistoga.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://thesisleyspa.com/los-angeles/> (referer: None)
DEBUG: Redirecting (301) to <GET http://beverlyhotsprings.com/> from <GET http://www.beverlyhotsprings.com/>
DEBUG: Redirecting (301) to <GET https://lagunacanyonspa.com/robots.txt> from <GET http://www.lagunacanyonspa.com/robots.txt>
DEBUG: Crawled (200) <GET https://azurepalmhotsprings.com/contact-us/> (referer: https://azurepalmhotsprings.com/)
DEBUG: Crawled (200) <GET https://www.estanciadayspa.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://lagunacanyonspa.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://www.carnerosresort.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://lagunacanyonspa.com/> from <GET http://www.lagunacanyonspa.com/>
DEBUG: Crawled (200) <GET https://www.indianspringscalistoga.com/?utm_source=google&utm_medium=organic&utm_campaign=business-listing> (referer: None)
DEBUG: Crawled (200) <GET https://lagunacanyonspa.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://paradisepoint.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://beverlyhotsprings.com/robots.txt> (referer: None)
DEBUG: Crawled (404) <GET https://azurepalmhotsprings.com/dev/contact-us/> (referer: https://azurepalmhotsprings.com/)
DEBUG: Crawled (200) <GET https://lagunacanyonspa.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.carnerosresort.com/> from <GET http://www.carnerosresort.com/>
INFO: Ignoring response <404 https://azurepalmhotsprings.com/dev/contact-us/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://wecarespa.com/robots.txt> (referer: None)
DEBUG: Rule at line 1 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://amenitiesspa.com/> (referer: None)
DEBUG: Crawled (200) <GET https://paradisepoint.com/california-resort-spa/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.sweetwaterspa.com/> from <GET https://sweetwaterspa.com/>
DEBUG: Redirecting (301) to <GET https://banyancay.com/contact-us.html> from <GET https://www.banyancay.com/contact-us.html>
DEBUG: Crawled (200) <GET https://www.indianspringscalistoga.com/contact-us> (referer: https://www.indianspringscalistoga.com/?utm_source=google&utm_medium=organic&utm_campaign=business-listing)
DEBUG: Crawled (200) <GET http://beverlyhotsprings.com/> (referer: None)
DEBUG: Redirecting (301) to <GET http://carnerosresort.com/> from <GET https://www.carnerosresort.com/>
DEBUG: Crawled (200) <GET https://www.cavallopoint.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://banyancay.com/robots.txt> (referer: None)
DEBUG: Rule at line 2 without any user agent to enforce it on.
DEBUG: Rule at line 7 without any user agent to enforce it on.
DEBUG: Rule at line 9 without any user agent to enforce it on.
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Rule at line 11 without any user agent to enforce it on.
DEBUG: Rule at line 15 without any user agent to enforce it on.
DEBUG: Rule at line 16 without any user agent to enforce it on.
DEBUG: Rule at line 17 without any user agent to enforce it on.
DEBUG: Rule at line 18 without any user agent to enforce it on.
DEBUG: Rule at line 22 without any user agent to enforce it on.
DEBUG: Rule at line 23 without any user agent to enforce it on.
DEBUG: Rule at line 24 without any user agent to enforce it on.
DEBUG: Rule at line 25 without any user agent to enforce it on.
DEBUG: Rule at line 29 without any user agent to enforce it on.
DEBUG: Rule at line 30 without any user agent to enforce it on.
DEBUG: Rule at line 31 without any user agent to enforce it on.
DEBUG: Rule at line 32 without any user agent to enforce it on.
DEBUG: Rule at line 33 without any user agent to enforce it on.
DEBUG: Rule at line 37 without any user agent to enforce it on.
DEBUG: Rule at line 38 without any user agent to enforce it on.
DEBUG: Rule at line 39 without any user agent to enforce it on.
DEBUG: Rule at line 40 without any user agent to enforce it on.
DEBUG: Rule at line 44 without any user agent to enforce it on.
DEBUG: Rule at line 48 without any user agent to enforce it on.
DEBUG: Rule at line 49 without any user agent to enforce it on.
DEBUG: Rule at line 50 without any user agent to enforce it on.
DEBUG: Rule at line 51 without any user agent to enforce it on.
DEBUG: Rule at line 52 without any user agent to enforce it on.
DEBUG: Rule at line 53 without any user agent to enforce it on.
DEBUG: Rule at line 54 without any user agent to enforce it on.
DEBUG: Rule at line 55 without any user agent to enforce it on.
DEBUG: Rule at line 56 without any user agent to enforce it on.
DEBUG: Rule at line 57 without any user agent to enforce it on.
DEBUG: Rule at line 58 without any user agent to enforce it on.
DEBUG: Rule at line 59 without any user agent to enforce it on.
DEBUG: Rule at line 60 without any user agent to enforce it on.
DEBUG: Rule at line 61 without any user agent to enforce it on.
DEBUG: Rule at line 62 without any user agent to enforce it on.
DEBUG: Rule at line 63 without any user agent to enforce it on.
DEBUG: Rule at line 64 without any user agent to enforce it on.
DEBUG: Rule at line 65 without any user agent to enforce it on.
DEBUG: Rule at line 66 without any user agent to enforce it on.
DEBUG: Rule at line 70 without any user agent to enforce it on.
DEBUG: Rule at line 72 without any user agent to enforce it on.
DEBUG: Rule at line 80 without any user agent to enforce it on.
DEBUG: Rule at line 87 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://wecarespa.com/> (referer: None)
DEBUG: Crawled (200) <GET https://thesisleyspa.com/contact-us/> (referer: https://thesisleyspa.com/los-angeles/)
DEBUG: Crawled (200) <GET https://carmelvalleyranch.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://carnerosresort.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://theravenspa.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://paradisepoint.com/contact-us/> (referer: https://paradisepoint.com/california-resort-spa/)
DEBUG: Redirecting (301) to <GET https://wecarespa.com/contact-us/> from <GET https://wecarespa.com/contact/>
DEBUG: Crawled (404) <GET https://www.ranchobernardoinn.com/robots.txt> (referer: None)
DEBUG: Rule at line 14 without any user agent to enforce it on.
DEBUG: Rule at line 15 without any user agent to enforce it on.
DEBUG: Rule at line 56 without any user agent to enforce it on.
DEBUG: Rule at line 57 without any user agent to enforce it on.
DEBUG: Rule at line 61 without any user agent to enforce it on.
DEBUG: Rule at line 62 without any user agent to enforce it on.
DEBUG: Rule at line 63 without any user agent to enforce it on.
DEBUG: Rule at line 67 without any user agent to enforce it on.
DEBUG: Rule at line 71 without any user agent to enforce it on.
DEBUG: Rule at line 73 without any user agent to enforce it on.
DEBUG: Rule at line 74 without any user agent to enforce it on.
DEBUG: Rule at line 83 without any user agent to enforce it on.
DEBUG: Rule at line 88 without any user agent to enforce it on.
DEBUG: Rule at line 101 without any user agent to enforce it on.
DEBUG: Rule at line 103 without any user agent to enforce it on.
DEBUG: Rule at line 104 without any user agent to enforce it on.
DEBUG: Rule at line 123 without any user agent to enforce it on.
DEBUG: Rule at line 141 without any user agent to enforce it on.
DEBUG: Rule at line 144 without any user agent to enforce it on.
DEBUG: Rule at line 145 without any user agent to enforce it on.
DEBUG: Rule at line 146 without any user agent to enforce it on.
DEBUG: Rule at line 152 without any user agent to enforce it on.
DEBUG: Rule at line 173 without any user agent to enforce it on.
DEBUG: Rule at line 241 without any user agent to enforce it on.
DEBUG: Rule at line 242 without any user agent to enforce it on.
DEBUG: Rule at line 249 without any user agent to enforce it on.
DEBUG: Rule at line 260 without any user agent to enforce it on.
DEBUG: Rule at line 265 without any user agent to enforce it on.
DEBUG: Rule at line 268 without any user agent to enforce it on.
DEBUG: Rule at line 271 without any user agent to enforce it on.
DEBUG: Rule at line 274 without any user agent to enforce it on.
DEBUG: Rule at line 277 without any user agent to enforce it on.
DEBUG: Rule at line 280 without any user agent to enforce it on.
DEBUG: Rule at line 283 without any user agent to enforce it on.
DEBUG: Rule at line 288 without any user agent to enforce it on.
DEBUG: Rule at line 293 without any user agent to enforce it on.
DEBUG: Rule at line 296 without any user agent to enforce it on.
DEBUG: Rule at line 299 without any user agent to enforce it on.
DEBUG: Rule at line 302 without any user agent to enforce it on.
DEBUG: Rule at line 305 without any user agent to enforce it on.
DEBUG: Rule at line 308 without any user agent to enforce it on.
DEBUG: Rule at line 311 without any user agent to enforce it on.
DEBUG: Rule at line 314 without any user agent to enforce it on.
DEBUG: Rule at line 320 without any user agent to enforce it on.
DEBUG: Rule at line 325 without any user agent to enforce it on.
DEBUG: Rule at line 330 without any user agent to enforce it on.
DEBUG: Rule at line 333 without any user agent to enforce it on.
DEBUG: Rule at line 336 without any user agent to enforce it on.
DEBUG: Rule at line 339 without any user agent to enforce it on.
DEBUG: Rule at line 342 without any user agent to enforce it on.
DEBUG: Rule at line 345 without any user agent to enforce it on.
DEBUG: Rule at line 348 without any user agent to enforce it on.
DEBUG: Rule at line 353 without any user agent to enforce it on.
DEBUG: Rule at line 358 without any user agent to enforce it on.
DEBUG: Rule at line 365 without any user agent to enforce it on.
DEBUG: Rule at line 368 without any user agent to enforce it on.
DEBUG: Rule at line 371 without any user agent to enforce it on.
DEBUG: Rule at line 374 without any user agent to enforce it on.
DEBUG: Rule at line 377 without any user agent to enforce it on.
DEBUG: Rule at line 380 without any user agent to enforce it on.
DEBUG: Rule at line 385 without any user agent to enforce it on.
DEBUG: Rule at line 390 without any user agent to enforce it on.
DEBUG: Rule at line 395 without any user agent to enforce it on.
DEBUG: Rule at line 398 without any user agent to enforce it on.
DEBUG: Rule at line 401 without any user agent to enforce it on.
DEBUG: Rule at line 404 without any user agent to enforce it on.
DEBUG: Rule at line 407 without any user agent to enforce it on.
DEBUG: Rule at line 410 without any user agent to enforce it on.
DEBUG: Rule at line 413 without any user agent to enforce it on.
DEBUG: Rule at line 416 without any user agent to enforce it on.
DEBUG: Rule at line 419 without any user agent to enforce it on.
DEBUG: Rule at line 422 without any user agent to enforce it on.
DEBUG: Rule at line 425 without any user agent to enforce it on.
DEBUG: Rule at line 430 without any user agent to enforce it on.
DEBUG: Rule at line 435 without any user agent to enforce it on.
DEBUG: Rule at line 438 without any user agent to enforce it on.
DEBUG: Rule at line 441 without any user agent to enforce it on.
DEBUG: Rule at line 444 without any user agent to enforce it on.
DEBUG: Rule at line 450 without any user agent to enforce it on.
DEBUG: Rule at line 453 without any user agent to enforce it on.
DEBUG: Rule at line 458 without any user agent to enforce it on.
DEBUG: Rule at line 463 without any user agent to enforce it on.
DEBUG: Rule at line 466 without any user agent to enforce it on.
DEBUG: Rule at line 469 without any user agent to enforce it on.
DEBUG: Rule at line 472 without any user agent to enforce it on.
DEBUG: Rule at line 475 without any user agent to enforce it on.
DEBUG: Rule at line 478 without any user agent to enforce it on.
DEBUG: Rule at line 490 without any user agent to enforce it on.
DEBUG: Rule at line 491 without any user agent to enforce it on.
DEBUG: Rule at line 511 without any user agent to enforce it on.
DEBUG: Rule at line 543 without any user agent to enforce it on.
DEBUG: Rule at line 545 without any user agent to enforce it on.
DEBUG: Rule at line 550 without any user agent to enforce it on.
DEBUG: Rule at line 551 without any user agent to enforce it on.
DEBUG: Rule at line 552 without any user agent to enforce it on.
DEBUG: Rule at line 620 without any user agent to enforce it on.
DEBUG: Rule at line 627 without any user agent to enforce it on.
DEBUG: Rule at line 629 without any user agent to enforce it on.
DEBUG: Rule at line 633 without any user agent to enforce it on.
DEBUG: Rule at line 637 without any user agent to enforce it on.
DEBUG: Rule at line 641 without any user agent to enforce it on.
DEBUG: Rule at line 642 without any user agent to enforce it on.
DEBUG: Rule at line 643 without any user agent to enforce it on.
DEBUG: Rule at line 644 without any user agent to enforce it on.
DEBUG: Rule at line 658 without any user agent to enforce it on.
DEBUG: Rule at line 662 without any user agent to enforce it on.
DEBUG: Rule at line 663 without any user agent to enforce it on.
DEBUG: Rule at line 664 without any user agent to enforce it on.
DEBUG: Rule at line 665 without any user agent to enforce it on.
DEBUG: Rule at line 666 without any user agent to enforce it on.
DEBUG: Rule at line 684 without any user agent to enforce it on.
DEBUG: Rule at line 685 without any user agent to enforce it on.
DEBUG: Rule at line 686 without any user agent to enforce it on.
DEBUG: Rule at line 693 without any user agent to enforce it on.
DEBUG: Rule at line 694 without any user agent to enforce it on.
DEBUG: Rule at line 698 without any user agent to enforce it on.
DEBUG: Rule at line 699 without any user agent to enforce it on.
DEBUG: Rule at line 700 without any user agent to enforce it on.
DEBUG: Rule at line 701 without any user agent to enforce it on.
DEBUG: Rule at line 702 without any user agent to enforce it on.
DEBUG: Rule at line 703 without any user agent to enforce it on.
DEBUG: Rule at line 705 without any user agent to enforce it on.
DEBUG: Rule at line 707 without any user agent to enforce it on.
DEBUG: Rule at line 747 without any user agent to enforce it on.
DEBUG: Rule at line 754 without any user agent to enforce it on.
DEBUG: Rule at line 755 without any user agent to enforce it on.
DEBUG: Rule at line 780 without any user agent to enforce it on.
DEBUG: Rule at line 787 without any user agent to enforce it on.
DEBUG: Rule at line 788 without any user agent to enforce it on.
DEBUG: Rule at line 818 without any user agent to enforce it on.
DEBUG: Rule at line 821 without any user agent to enforce it on.
DEBUG: Rule at line 828 without any user agent to enforce it on.
DEBUG: Rule at line 829 without any user agent to enforce it on.
DEBUG: Rule at line 834 without any user agent to enforce it on.
DEBUG: Rule at line 836 without any user agent to enforce it on.
DEBUG: Rule at line 839 without any user agent to enforce it on.
DEBUG: Rule at line 840 without any user agent to enforce it on.
DEBUG: Rule at line 841 without any user agent to enforce it on.
DEBUG: Rule at line 894 without any user agent to enforce it on.
DEBUG: Rule at line 895 without any user agent to enforce it on.
DEBUG: Rule at line 896 without any user agent to enforce it on.
DEBUG: Rule at line 897 without any user agent to enforce it on.
DEBUG: Rule at line 898 without any user agent to enforce it on.
DEBUG: Rule at line 900 without any user agent to enforce it on.
DEBUG: Rule at line 905 without any user agent to enforce it on.
DEBUG: Rule at line 906 without any user agent to enforce it on.
DEBUG: Rule at line 907 without any user agent to enforce it on.
DEBUG: Rule at line 908 without any user agent to enforce it on.
DEBUG: Rule at line 913 without any user agent to enforce it on.
DEBUG: Rule at line 932 without any user agent to enforce it on.
DEBUG: Rule at line 933 without any user agent to enforce it on.
DEBUG: Rule at line 944 without any user agent to enforce it on.
DEBUG: Rule at line 953 without any user agent to enforce it on.
DEBUG: Rule at line 956 without any user agent to enforce it on.
DEBUG: Rule at line 961 without any user agent to enforce it on.
DEBUG: Rule at line 970 without any user agent to enforce it on.
DEBUG: Rule at line 986 without any user agent to enforce it on.
DEBUG: Rule at line 987 without any user agent to enforce it on.
DEBUG: Rule at line 988 without any user agent to enforce it on.
DEBUG: Rule at line 989 without any user agent to enforce it on.
DEBUG: Rule at line 995 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET http://beverlyhotsprings.com/contact/> (referer: http://beverlyhotsprings.com/)
DEBUG: Crawled (200) <GET https://www.estanciadayspa.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://carnerosresort.com/> from <GET http://carnerosresort.com/>
DEBUG: Crawled (200) <GET https://wecarespa.com/contact-us/> (referer: https://wecarespa.com/)
DEBUG: Crawled (200) <GET https://theravenspa.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.trilogyspa.com/robots.txt> from <GET http://www.trilogyspa.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://www.thebluedoorhanford.com/robots.txt> from <GET http://www.thebluedoorhanford.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.cavallopoint.com/spa?utm_source=google-gmb&utm_medium=organic&utm_campaign=gmb> (referer: None)
DEBUG: Redirecting (301) to <GET https://banyancay.com/> from <GET https://banyancay.com/contact-us.html>
DEBUG: Crawled (200) <GET https://www.thebluedoorhanford.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.trilogyspa.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.miraclesprings.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://harbin.org/robots.txt> from <GET http://www.harbin.org/robots.txt>
DEBUG: Redirecting (301) to <GET https://www.trilogyspa.com/> from <GET http://www.trilogyspa.com/>
DEBUG: Redirecting (301) to <GET https://www.thebluedoorhanford.com/> from <GET http://www.thebluedoorhanford.com/>
DEBUG: Crawled (200) <GET https://carnerosresort.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.thebluedoorhanford.com/> (referer: None)
DEBUG: Crawled (200) <GET https://carmelvalleyranch.com/spa/spa-aiyana/> (referer: None)
DEBUG: Crawled (200) <GET https://www.trilogyspa.com/> (referer: None)
DEBUG: Crawled (200) <GET https://thehealingcornerca.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.teahousespa.com/robots.txt> from <GET http://www.teahousespa.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.paseahotel.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.missioninn.com/robots.txt> from <GET http://www.missioninn.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.sweetwaterspa.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://harbin.org/robots.txt> from <GET http://harbin.org/robots.txt>
DEBUG: Redirecting (301) to <GET https://doubleeagle.com/robots.txt> from <GET http://doubleeagle.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.ranchobernardoinn.com/spa/overview?utm_source=gmb&utm_medium=yext> (referer: None)
DEBUG: Crawled (200) <GET https://www.miraclesprings.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.missioninn.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.missioninn.com/> from <GET http://www.missioninn.com/>
DEBUG: Crawled (200) <GET https://thehealingcornerca.com/> (referer: None)
DEBUG: Crawled (200) <GET https://banyancay.com/> (referer: https://www.chaminade.com/?utm_source=google&utm_medium=organic&utm_campaign=gmb)
DEBUG: Crawled (200) <GET https://www.teahousespa.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.paseahotel.com/?utm_medium=organic&utm_source=google&utm_campaign=business-listing> (referer: None)
DEBUG: Crawled (200) <GET https://www.missioninn.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.teahousespa.com/> from <GET http://www.teahousespa.com/>
DEBUG: Crawled (200) <GET https://www.teahousespa.com/> (referer: None)
DEBUG: Crawled (200) <GET https://harbin.org/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.thelandingtahoe.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.montagehotels.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET https://www.montagehotels.com/lagunabeach/> (referer: None)
INFO: Ignoring response <403 https://www.montagehotels.com/lagunabeach/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET https://harbin.org/> from <GET http://www.harbin.org/>
DEBUG: Crawled (200) <GET https://www.estancialajolla.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.paseahotel.com/contact/press-room> (referer: https://www.paseahotel.com/?utm_medium=organic&utm_source=google&utm_campaign=business-listing)
DEBUG: Crawled (200) <GET https://www.miraclesprings.com/contact/> (referer: https://www.miraclesprings.com/)
DEBUG: Crawled (200) <GET https://harbin.org/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://mysheerbliss.com/robots.txt> from <GET https://www.mysheerbliss.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://www.doubleeagle.com/robots.txt> from <GET https://doubleeagle.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.estancialajolla.com/spa/?utm_source=gmb-spa&utm_medium=organic&utm_campaign=gmb> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.bardessono.com/robots.txt> from <GET https://bardessono.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.silveradoresort.com/?utm_source=google&utm_medium=organic&utm_campaign=business-listing> (referer: None)
DEBUG: Crawled (200) <GET https://carmelvalleyranch.com/contact/> (referer: https://carmelvalleyranch.com/spa/spa-aiyana/)
DEBUG: Crawled (200) <GET https://www.paseahotel.com/contact/blogs> (referer: https://www.paseahotel.com/?utm_medium=organic&utm_source=google&utm_campaign=business-listing)
DEBUG: Crawled (200) <GET https://mysheerbliss.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://mysheerbliss.com/> from <GET https://www.mysheerbliss.com/>
DEBUG: Crawled (200) <GET https://www.thelandingtahoe.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.sweetwaterspa.com/contact-us/> (referer: https://www.sweetwaterspa.com/)
DEBUG: Crawled (200) <GET https://harbin.org/> (referer: None)
DEBUG: Crawled (200) <GET http://www.athenaspa-mv.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.paseahotel.com/contact> (referer: https://www.paseahotel.com/?utm_medium=organic&utm_source=google&utm_campaign=business-listing)
DEBUG: Crawled (200) <GET https://mysheerbliss.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.senspa.com/robots.txt> from <GET http://www.senspa.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.estancialajolla.com/contact/> (referer: https://www.estancialajolla.com/spa/?utm_source=gmb-spa&utm_medium=organic&utm_campaign=gmb)
DEBUG: Crawled (200) <GET https://mysheerbliss.com/> (referer: None)
DEBUG: Crawled (200) <GET https://petitespa.net/robots.txt> (referer: None)
DEBUG: Retrying <GET https://www.shuttersonthebeach.com/robots.txt> (failed 1 times): 502 Bad Gateway
DEBUG: Retrying <GET https://www.shuttersonthebeach.com/robots.txt> (failed 2 times): 502 Bad Gateway
DEBUG: Crawled (200) <GET https://www.bardessono.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://petitespa.net/> (referer: None)
DEBUG: Crawled (200) <GET https://www.romanspahotsprings.com/robots.txt> (referer: None)
ERROR: Gave up retrying <GET https://www.shuttersonthebeach.com/robots.txt> (failed 3 times): 502 Bad Gateway
DEBUG: Crawled (502) <GET https://www.shuttersonthebeach.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.senspa.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://mysheerbliss.com/robots.txt> from <GET https://sheer-bliss-organic-spa.myshopify.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://www.bardessono.com/> from <GET https://bardessono.com/>
DEBUG: Crawled (200) <GET http://metropolissalonspa.com/robots.txt> (referer: None)
DEBUG: Forbidden by robots.txt: <GET http://metropolissalonspa.com/>
DEBUG: Retrying <GET https://www.shuttersonthebeach.com/spa/one-spa> (failed 1 times): 502 Bad Gateway
DEBUG: Crawled (200) <GET https://mysheerbliss.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.senspa.com/> from <GET http://www.senspa.com/>
DEBUG: Crawled (200) <GET https://milkandhoneyspa.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://mysheerbliss.com/pages/contact-us-spa-policy> from <GET https://sheer-bliss-organic-spa.myshopify.com/pages/contact-us-spa-policy>
DEBUG: Retrying <GET https://www.shuttersonthebeach.com/spa/one-spa> (failed 2 times): 502 Bad Gateway
DEBUG: Crawled (200) <GET https://www.bardessono.com/robots.txt> (referer: None)
DEBUG: Crawled (451) <GET https://www.morongocasinoresort.com/robots.txt> (referer: None)
DEBUG: Rule at line 3 without any user agent to enforce it on.
DEBUG: Rule at line 4 without any user agent to enforce it on.
DEBUG: Rule at line 6 without any user agent to enforce it on.
DEBUG: Rule at line 27 without any user agent to enforce it on.
DEBUG: Rule at line 28 without any user agent to enforce it on.
DEBUG: Rule at line 29 without any user agent to enforce it on.
DEBUG: Rule at line 30 without any user agent to enforce it on.
DEBUG: Rule at line 31 without any user agent to enforce it on.
DEBUG: Rule at line 32 without any user agent to enforce it on.
DEBUG: Rule at line 35 without any user agent to enforce it on.
DEBUG: Rule at line 38 without any user agent to enforce it on.
DEBUG: Rule at line 41 without any user agent to enforce it on.
DEBUG: Rule at line 43 without any user agent to enforce it on.
DEBUG: Rule at line 44 without any user agent to enforce it on.
DEBUG: Rule at line 45 without any user agent to enforce it on.
DEBUG: Rule at line 46 without any user agent to enforce it on.
DEBUG: Rule at line 49 without any user agent to enforce it on.
DEBUG: Rule at line 50 without any user agent to enforce it on.
DEBUG: Rule at line 56 without any user agent to enforce it on.
DEBUG: Rule at line 57 without any user agent to enforce it on.
DEBUG: Rule at line 58 without any user agent to enforce it on.
DEBUG: Rule at line 64 without any user agent to enforce it on.
DEBUG: Rule at line 69 without any user agent to enforce it on.
DEBUG: Rule at line 70 without any user agent to enforce it on.
DEBUG: Rule at line 71 without any user agent to enforce it on.
DEBUG: Rule at line 72 without any user agent to enforce it on.
DEBUG: Rule at line 76 without any user agent to enforce it on.
DEBUG: Rule at line 77 without any user agent to enforce it on.
DEBUG: Rule at line 78 without any user agent to enforce it on.
DEBUG: Rule at line 79 without any user agent to enforce it on.
DEBUG: Rule at line 82 without any user agent to enforce it on.
DEBUG: Rule at line 86 without any user agent to enforce it on.
DEBUG: Rule at line 89 without any user agent to enforce it on.
DEBUG: Rule at line 90 without any user agent to enforce it on.
DEBUG: Rule at line 91 without any user agent to enforce it on.
DEBUG: Rule at line 92 without any user agent to enforce it on.
DEBUG: Rule at line 93 without any user agent to enforce it on.
DEBUG: Rule at line 94 without any user agent to enforce it on.
DEBUG: Rule at line 98 without any user agent to enforce it on.
DEBUG: Rule at line 102 without any user agent to enforce it on.
DEBUG: Rule at line 103 without any user agent to enforce it on.
DEBUG: Rule at line 104 without any user agent to enforce it on.
DEBUG: Rule at line 108 without any user agent to enforce it on.
DEBUG: Rule at line 109 without any user agent to enforce it on.
DEBUG: Rule at line 110 without any user agent to enforce it on.
DEBUG: Rule at line 114 without any user agent to enforce it on.
DEBUG: Rule at line 115 without any user agent to enforce it on.
DEBUG: Rule at line 116 without any user agent to enforce it on.
DEBUG: Rule at line 118 without any user agent to enforce it on.
DEBUG: Rule at line 119 without any user agent to enforce it on.
DEBUG: Rule at line 120 without any user agent to enforce it on.
DEBUG: Rule at line 121 without any user agent to enforce it on.
DEBUG: Rule at line 122 without any user agent to enforce it on.
DEBUG: Rule at line 123 without any user agent to enforce it on.
DEBUG: Rule at line 124 without any user agent to enforce it on.
DEBUG: Rule at line 125 without any user agent to enforce it on.
DEBUG: Rule at line 126 without any user agent to enforce it on.
DEBUG: Rule at line 127 without any user agent to enforce it on.
DEBUG: Rule at line 128 without any user agent to enforce it on.
DEBUG: Rule at line 131 without any user agent to enforce it on.
DEBUG: Rule at line 132 without any user agent to enforce it on.
DEBUG: Rule at line 133 without any user agent to enforce it on.
DEBUG: Rule at line 137 without any user agent to enforce it on.
DEBUG: Rule at line 140 without any user agent to enforce it on.
DEBUG: Rule at line 143 without any user agent to enforce it on.
DEBUG: Rule at line 144 without any user agent to enforce it on.
DEBUG: Rule at line 145 without any user agent to enforce it on.
DEBUG: Rule at line 146 without any user agent to enforce it on.
DEBUG: Rule at line 147 without any user agent to enforce it on.
DEBUG: Rule at line 148 without any user agent to enforce it on.
DEBUG: Rule at line 150 without any user agent to enforce it on.
DEBUG: Rule at line 151 without any user agent to enforce it on.
DEBUG: Rule at line 154 without any user agent to enforce it on.
DEBUG: Rule at line 155 without any user agent to enforce it on.
DEBUG: Rule at line 156 without any user agent to enforce it on.
DEBUG: Rule at line 157 without any user agent to enforce it on.
DEBUG: Rule at line 160 without any user agent to enforce it on.
DEBUG: Rule at line 161 without any user agent to enforce it on.
DEBUG: Rule at line 164 without any user agent to enforce it on.
DEBUG: Rule at line 167 without any user agent to enforce it on.
DEBUG: Rule at line 170 without any user agent to enforce it on.
DEBUG: Rule at line 173 without any user agent to enforce it on.
DEBUG: Rule at line 174 without any user agent to enforce it on.
DEBUG: Rule at line 189 without any user agent to enforce it on.
ERROR: Gave up retrying <GET https://www.shuttersonthebeach.com/spa/one-spa> (failed 3 times): 502 Bad Gateway
DEBUG: Crawled (502) <GET https://www.shuttersonthebeach.com/spa/one-spa> (referer: None)
DEBUG: Crawled (200) <GET http://www.athenaspa-mv.com/> (referer: None)
INFO: Ignoring response <502 https://www.shuttersonthebeach.com/spa/one-spa>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://www.senspa.com/> (referer: None)
DEBUG: Crawled (200) <GET https://mysheerbliss.com/pages/contact-us-spa-policy> (referer: https://mysheerbliss.com/)
DEBUG: Crawled (451) <GET https://www.morongocasinoresort.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.bardessono.com/> (referer: None)
INFO: Ignoring response <451 https://www.morongocasinoresort.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://www.doubleeagle.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.bardessono.com/contact.htm> (referer: https://www.bardessono.com/)
DEBUG: Crawled (200) <GET https://milkandhoneyspa.com/?utm_source=google&utm_medium=organic&utm_campaign=gmb-listing&utm_content=brentwood> (referer: None)
DEBUG: Crawled (200) <GET https://www.romanspahotsprings.com/?utm_source=google%20my%20business&utm_medium=listing&utm_campaign=visit%20website> (referer: None)
DEBUG: Redirecting (301) to <GET https://oceanohalfmoonbay.com/robots.txt> from <GET http://www.oceanohalfmoonbay.com/robots.txt>
DEBUG: Crawled (200) <GET http://www.athenaspa-mv.com/contact-us> (referer: http://www.athenaspa-mv.com/)
DEBUG: Redirecting (301) to <GET https://doubleeagle.com/> from <GET http://doubleeagle.com/>
DEBUG: Crawled (404) <GET https://www.ventanabigsur.com/robots.txt> (referer: None)
DEBUG: Rule at line 20 without any user agent to enforce it on.
DEBUG: Rule at line 43 without any user agent to enforce it on.
DEBUG: Rule at line 54 without any user agent to enforce it on.
DEBUG: Rule at line 55 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://oceanohalfmoonbay.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.romanspahotsprings.com/contact> (referer: https://www.romanspahotsprings.com/?utm_source=google%20my%20business&utm_medium=listing&utm_campaign=visit%20website)
DEBUG: Redirecting (301) to <GET https://www.doubleeagle.com/> from <GET https://doubleeagle.com/>
DEBUG: Redirecting (301) to <GET https://oceanohalfmoonbay.com/> from <GET http://www.oceanohalfmoonbay.com/>
DEBUG: Crawled (200) <GET https://oceanohalfmoonbay.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://oceanohalfmoonbay.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.doubleeagle.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://oceanohalfmoonbay.com/contact/> (referer: https://oceanohalfmoonbay.com/)
DEBUG: Crawled (200) <GET https://www.ventanabigsur.com/spa/overview> (referer: None)
DEBUG: Crawled (200) <GET https://www.doubleeagle.com/> (referer: None)
INFO: Closing spider (finished)
INFO: Dumping Scrapy stats:
{'downloader/exception_count': 2,
 'downloader/exception_type_count/scrapy.exceptions.IgnoreRequest': 2,
 'downloader/request_bytes': 81448,
 'downloader/request_count': 287,
 'downloader/request_method_count/GET': 287,
 'downloader/response_bytes': 5590976,
 'downloader/response_count': 287,
 'downloader/response_status_count/200': 184,
 'downloader/response_status_count/301': 82,
 'downloader/response_status_count/403': 1,
 'downloader/response_status_count/404': 6,
 'downloader/response_status_count/429': 6,
 'downloader/response_status_count/451': 2,
 'downloader/response_status_count/502': 6,
 'dupefilter/filtered': 29,
 'elapsed_time_seconds': 23.117353,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 9, 12, 55, 26, 694133),
 'httpcompression/response_bytes': 18974351,
 'httpcompression/response_count': 169,
 'httperror/response_ignored_count': 5,
 'httperror/response_ignored_status_count/403': 1,
 'httperror/response_ignored_status_count/404': 1,
 'httperror/response_ignored_status_count/429': 1,
 'httperror/response_ignored_status_count/451': 1,
 'httperror/response_ignored_status_count/502': 1,
 'log_count/DEBUG': 660,
 'log_count/ERROR': 4,
 'log_count/INFO': 15,
 'request_depth_max': 1,
 'response_received_count': 197,
 'retry/count': 8,
 'retry/max_reached': 4,
 'retry/reason_count/429 Unknown Status': 4,
 'retry/reason_count/502 Bad Gateway': 4,
 'robotstxt/forbidden': 2,
 'robotstxt/request_count': 92,
 'robotstxt/response_count': 92,
 'robotstxt/response_status_count/200': 84,
 'robotstxt/response_status_count/404': 5,
 'robotstxt/response_status_count/429': 1,
 'robotstxt/response_status_count/451': 1,
 'robotstxt/response_status_count/502': 1,
 'scheduler/dequeued': 160,
 'scheduler/dequeued/memory': 160,
 'scheduler/enqueued': 160,
 'scheduler/enqueued/memory': 160,
 'start_time': datetime.datetime(2022, 12, 9, 12, 55, 3, 576780)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: c6fc2a761244e918
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
DEBUG: Redirecting (301) to <GET https://kellysspa.com/robots.txt> from <GET http://kellysspa.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://www.cal-a-vie.com/robots.txt> from <GET http://www.cal-a-vie.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://www.kellysspa.com/robots.txt> from <GET https://kellysspa.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://www.goldenhaven.com/robots.txt> from <GET http://www.goldenhaven.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.kellysspa.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.glenivy.com/robots.txt> from <GET http://www.glenivy.com/robots.txt>
DEBUG: Attempting to acquire lock 2623880652800 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2623880652800 acquired on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Attempting to release lock 2623880652800 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2623880652800 released on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Crawled (200) <GET https://www.omnihotels.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://kellysspa.com/> from <GET http://kellysspa.com/>
DEBUG: Crawled (200) <GET https://ranchovalencia.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.kellysspa.com/> from <GET https://kellysspa.com/>
DEBUG: Crawled (200) <GET https://goldendoor.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.cal-a-vie.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.kellysspa.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://cadayspa.com/robots.txt> from <GET http://cadayspa.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://www.cal-a-vie.com/?utm_source=google&utm_medium=organic&utm_campaign=business-listing> from <GET http://www.cal-a-vie.com/?utm_source=google&utm_medium=organic&utm_campaign=business-listing>
DEBUG: Crawled (200) <GET https://www.kellysspa.com/> (referer: None)
DEBUG: Crawled (200) <GET https://cadayspa.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://cadayspa.com/> from <GET http://cadayspa.com/>
DEBUG: Crawled (200) <GET https://goldendoor.com/> (referer: None)
DEBUG: Crawled (200) <GET http://www.wispausa.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.goldenhaven.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://cadayspa.com/> (referer: None)
DEBUG: Filtered duplicate request: <GET https://goldendoor.com/contact-us/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG: Crawled (200) <GET https://www.cal-a-vie.com/?utm_source=google&utm_medium=organic&utm_campaign=business-listing> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.goldenhaven.com/> from <GET http://www.goldenhaven.com/>
DEBUG: Crawled (200) <GET https://www.osmosis.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.madonnainn.com/robots.txt> (referer: None)
DEBUG: Crawled (404) <GET https://www.ritzcarlton.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://www.ritzcarlton.com/en/hotels/california/half-moon-bay/spa?scid=45f93f1b-bd77-45c9-8dab-83b6a417f6fe&y_source=1_MTY0OTQwNC03MTUtbG9jYXRpb24ud2Vic2l0ZQ%3D%3D> from <GET https://www.ritzcarlton.com/en/hotels/california/half-moon-bay/spa/?scid=45f93f1b-bd77-45c9-8dab-83b6a417f6fe&y_source=1_MTY0OTQwNC03MTUtbG9jYXRpb24ud2Vic2l0ZQ%3D%3D>
DEBUG: Redirecting (301) to <GET https://locations.woodhousespas.com/dir/ca/walnut-creek/1636-cypress-st> from <GET http://walnutcreek.woodhousespas.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.cal-a-vie.com/contact-us> (referer: https://www.cal-a-vie.com/?utm_source=google&utm_medium=organic&utm_campaign=business-listing)
DEBUG: Crawled (200) <GET https://www.omnihotels.com/hotels/san-diego-la-costa/spa?utm_source=GMBlisting&utm_medium=organic> (referer: None)
DEBUG: Crawled (200) <GET https://www.goldenhaven.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.osmosis.com/?y_source=1_MjI1MDUzNzItNzE1LWxvY2F0aW9uLndlYnNpdGU%3D> (referer: None)
DEBUG: Crawled (200) <GET https://montereyplazahotel.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.ritzcarlton.com/en/hotels/california/half-moon-bay/spa?scid=45f93f1b-bd77-45c9-8dab-83b6a417f6fe&y_source=1_MTY0OTQwNC03MTUtbG9jYXRpb24ud2Vic2l0ZQ%3D%3D> from <GET http://www.ritzcarlton.com/en/hotels/california/half-moon-bay/spa?scid=45f93f1b-bd77-45c9-8dab-83b6a417f6fe&y_source=1_MTY0OTQwNC03MTUtbG9jYXRpb24ud2Vic2l0ZQ%3D%3D>
DEBUG: Scraped from <200 https://www.cal-a-vie.com/contact-us>
{'emails': ['cavinfo@cal-a-vie.com'],
 'facebook': 'https://www.facebook.com/CalaVie',
 'instagram': 'https://www.instagram.com/calaviespa',
 'linkedin': '',
 'twitter': 'https://twitter.com/calaviespa'}
DEBUG: Crawled (200) <GET https://www.ritzcarlton.com/en/hotels/california/half-moon-bay/spa?scid=45f93f1b-bd77-45c9-8dab-83b6a417f6fe&y_source=1_MTY0OTQwNC03MTUtbG9jYXRpb24ud2Vic2l0ZQ%3D%3D> (referer: None)
DEBUG: Crawled (200) <GET https://aubergeresorts.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.madonnainn.com/spa> (referer: None)
DEBUG: Crawled (200) <GET https://www.fourseasons.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.fourseasons.com/westlakevillage/spa/?seo=google_local_wes6_amer> (referer: None)
DEBUG: Crawled (200) <GET https://locations.woodhousespas.com/dir/ca/walnut-creek/1636-cypress-st> (referer: None)
DEBUG: Rule at line 1 without any user agent to enforce it on.
DEBUG: Rule at line 15 without any user agent to enforce it on.
DEBUG: Rule at line 22 without any user agent to enforce it on.
DEBUG: Rule at line 23 without any user agent to enforce it on.
DEBUG: Rule at line 24 without any user agent to enforce it on.
DEBUG: Rule at line 25 without any user agent to enforce it on.
DEBUG: Rule at line 26 without any user agent to enforce it on.
DEBUG: Rule at line 30 without any user agent to enforce it on.
DEBUG: Rule at line 31 without any user agent to enforce it on.
DEBUG: Rule at line 32 without any user agent to enforce it on.
DEBUG: Rule at line 35 without any user agent to enforce it on.
DEBUG: Rule at line 36 without any user agent to enforce it on.
DEBUG: Rule at line 38 without any user agent to enforce it on.
DEBUG: Rule at line 40 without any user agent to enforce it on.
DEBUG: Rule at line 41 without any user agent to enforce it on.
DEBUG: Rule at line 62 without any user agent to enforce it on.
DEBUG: Rule at line 67 without any user agent to enforce it on.
DEBUG: Rule at line 68 without any user agent to enforce it on.
DEBUG: Rule at line 74 without any user agent to enforce it on.
DEBUG: Redirecting (301) to <GET http://aubergeresorts.com/solage/wellness/spa/?utm_source=google%20my%20business&utm_medium=listing&utm_campaign=visit%20website> from <GET https://aubergeresorts.com/solage/wellness/spa?utm_source=google%20my%20business&utm_medium=listing&utm_campaign=visit%20website>
DEBUG: Redirecting (301) to <GET https://locations.woodhousespas.com/dir/ca/walnut-creek/1636-cypress-st> from <GET http://walnutcreek.woodhousespas.com/?utm_source=google&utm_medium=yext>
DEBUG: Crawled (200) <GET https://locations.woodhousespas.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://goldendoor.com/contact-us/> (referer: https://goldendoor.com/)
DEBUG: Crawled (200) <GET https://locations.woodhousespas.com/dir/ca/walnut-creek/1636-cypress-st> (referer: None)
DEBUG: Crawled (200) <GET https://www.osmosis.com/contact/> (referer: https://www.osmosis.com/?y_source=1_MjI1MDUzNzItNzE1LWxvY2F0aW9uLndlYnNpdGU%3D)
DEBUG: Redirecting (301) to <GET https://www.aubergeresorts.com/solage/wellness/spa/?utm_source=google%20my%20business&utm_medium=listing&utm_campaign=visit%20website> from <GET http://aubergeresorts.com/solage/wellness/spa/?utm_source=google%20my%20business&utm_medium=listing&utm_campaign=visit%20website>
DEBUG: Crawled (200) <GET https://www.southcoastwinery.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://glenivy.com/robots.txt> from <GET https://www.glenivy.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://www.mytrilogylife.com/robots.txt> from <GET http://www.mytrilogylife.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.aubergeresorts.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.purpleorchid.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.wispausa.com/> from <GET http://www.wispausa.com/>
DEBUG: Crawled (200) <GET https://www.silveradoresort.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://ranchovalencia.com/> (referer: None)
DEBUG: Crawled (200) <GET https://montereyplazahotel.com/spa/vista-blue-spa/?utm_source=google&utm_medium=bizlisting&utm_campaign=google_places> (referer: None)
DEBUG: Crawled (200) <GET https://www.omnihotels.com/forms/contact-us> (referer: https://www.omnihotels.com/hotels/san-diego-la-costa/spa?utm_source=GMBlisting&utm_medium=organic)
DEBUG: Crawled (200) <GET https://www.purpleorchid.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.chaminade.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.aubergeresorts.com/solage/wellness/spa/?utm_source=google%20my%20business&utm_medium=listing&utm_campaign=visit%20website> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.ritzcarlton.com/en/hotels/california/los-angeles/spa?scid=bb1a189a-fec3-4d19-a255-54ba596febe2> from <GET http://www.ritzcarlton.com/en/hotels/california/los-angeles/spa?scid=bb1a189a-fec3-4d19-a255-54ba596febe2>
DEBUG: Redirecting (301) to <GET https://www.laquintaresort.com/robots.txt> from <GET http://www.laquintaresort.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.purpleorchid.com/contact-us/> (referer: https://www.purpleorchid.com/)
DEBUG: Crawled (200) <GET https://www.ritzcarlton.com/en/hotels/california/los-angeles/spa?scid=bb1a189a-fec3-4d19-a255-54ba596febe2> (referer: None)
DEBUG: Crawled (200) <GET https://montereyplazahotel.com/hotel/contact> (referer: https://montereyplazahotel.com/spa/vista-blue-spa/?utm_source=google&utm_medium=bizlisting&utm_campaign=google_places)
DEBUG: Redirecting (301) to <GET https://www.ritzcarlton.com/en/hotels/california/santa-barbara/spa?scid=bb1a189a-fec3-4d19-a255-54ba596febe2> from <GET http://www.ritzcarlton.com/en/hotels/california/santa-barbara/spa?scid=bb1a189a-fec3-4d19-a255-54ba596febe2>
DEBUG: Crawled (200) <GET https://www.ritzcarlton.com/en/hotels/california/santa-barbara/spa?scid=bb1a189a-fec3-4d19-a255-54ba596febe2> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.southcoastwinery.com/spa/> from <GET https://www.southcoastwinery.com/spa>
DEBUG: Crawled (200) <GET https://www.mytrilogylife.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://boonhotels.com/robots.txt> from <GET http://www.boonhotels.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.silveradoresort.com/napa-valley-spa?utm_medium=organic&utm_source=google&utm_campaign=spabusinesslisting> (referer: None)
DEBUG: Crawled (200) <GET http://boonhotels.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.mytrilogylife.com/monarchdunes/spa/> from <GET http://www.mytrilogylife.com/monarchdunes/spa/>
DEBUG: Crawled (200) <GET https://the-spring.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://glenivy.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://aubergeresorts.com/solage/contact-us/> (referer: https://www.aubergeresorts.com/)
DEBUG: Redirecting (301) to <GET https://www.glenivy.com/> from <GET http://www.glenivy.com/>
DEBUG: Redirecting (301) to <GET https://www.boonhotels.com/> from <GET http://www.boonhotels.com/>
DEBUG: Crawled (200) <GET https://www.mytrilogylife.com/monarchdunes/spa/> (referer: None)
DEBUG: Crawled (200) <GET https://www.laquintaresort.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.southcoastwinery.com/spa/> (referer: None)
DEBUG: Retrying <GET https://www.hyatt.com/robots.txt> (failed 1 times): 429 Unknown Status
DEBUG: Crawled (200) <GET https://www.chaminade.com/?utm_source=google&utm_medium=organic&utm_campaign=gmb> (referer: None)
DEBUG: Crawled (200) <GET https://www.terranea.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://wispausa.com/> from <GET https://www.wispausa.com/>
DEBUG: Redirecting (301) to <GET https://www.laquintaresort.com/> from <GET http://www.laquintaresort.com/>
DEBUG: Crawled (200) <GET https://www.silveradoresort.com/contact-us> (referer: https://www.silveradoresort.com/napa-valley-spa?utm_medium=organic&utm_source=google&utm_campaign=spabusinesslisting)
DEBUG: Crawled (200) <GET https://www.urbanretreatspa.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://ranchovalencia.com/contact-us/> (referer: https://ranchovalencia.com/)
DEBUG: Retrying <GET https://www.hyatt.com/robots.txt> (failed 2 times): 429 Unknown Status
DEBUG: Redirecting (301) to <GET https://www.evo-spa.com/robots.txt> from <GET http://www.evo-spa.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://boonhotels.com/> from <GET https://www.boonhotels.com/>
DEBUG: Crawled (200) <GET https://www.terranea.com/offers?utm_source=google-knowledge-graph&utm_medium=organic&utm_campaign=my_business_page> (referer: None)
ERROR: Gave up retrying <GET https://www.hyatt.com/robots.txt> (failed 3 times): 429 Unknown Status
DEBUG: Crawled (429) <GET https://www.hyatt.com/robots.txt> (referer: None)
DEBUG: Rule at line 1 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://boonhotels.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://glenivy.com/> from <GET https://www.glenivy.com/>
DEBUG: Crawled (200) <GET https://www.urbanretreatspa.com/> (referer: None)
DEBUG: Retrying <GET https://www.hyatt.com/en-US/spas/Pacific-Waters-Spa/home.html> (failed 1 times): 429 Unknown Status
DEBUG: Crawled (200) <GET https://wispausa.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.evo-spa.com/robots.txt> (referer: None)
DEBUG: Retrying <GET https://www.hyatt.com/en-US/spas/Pacific-Waters-Spa/home.html> (failed 2 times): 429 Unknown Status
DEBUG: Crawled (200) <GET https://the-spring.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.evo-spa.com/> from <GET http://www.evo-spa.com/>
ERROR: Gave up retrying <GET https://www.hyatt.com/en-US/spas/Pacific-Waters-Spa/home.html> (failed 3 times): 429 Unknown Status
DEBUG: Crawled (429) <GET https://www.hyatt.com/en-US/spas/Pacific-Waters-Spa/home.html> (referer: None)
DEBUG: Crawled (200) <GET https://boonhotels.com/> (referer: None)
INFO: Ignoring response <429 https://www.hyatt.com/en-US/spas/Pacific-Waters-Spa/home.html>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET https://banyancay.com/robots.txt> from <GET https://www.banyancay.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://www.sycamoresprings.com/robots.txt> from <GET http://www.sycamoresprings.com/robots.txt>
DEBUG: Crawled (200) <GET https://glenivy.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://wispausa.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.evo-spa.com/> (referer: None)
DEBUG: Crawled (200) <GET https://davidrubensteinforum.uchicago.edu/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://boonhotels.com/contact/> (referer: https://boonhotels.com/)
DEBUG: Crawled (200) <GET https://www.sycamoresprings.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://thesisleyspa.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.sycamoresprings.com/> from <GET http://www.sycamoresprings.com/>
DEBUG: Crawled (200) <GET https://wispausa.com/contact/> (referer: https://wispausa.com/)
DEBUG: Redirecting (301) to <GET https://www.ranchovalencia.com/robots.txt> from <GET http://www.ranchovalencia.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.constantcontact.com/robots.txt> (referer: None)
DEBUG: Forbidden by robots.txt: <GET https://www.constantcontact.com/legal/service-provider>
DEBUG: Scraped from <200 https://wispausa.com/contact/>
{'emails': ['info@wispausa.com'],
 'facebook': 'https://www.facebook.com/wispa.usa',
 'instagram': 'https://www.instagram.com/wispa_usa/',
 'linkedin': '',
 'twitter': 'https://twitter.com/WiSpa_USA'}
DEBUG: Crawled (200) <GET https://banyancay.com/robots.txt> (referer: None)
DEBUG: Rule at line 2 without any user agent to enforce it on.
DEBUG: Rule at line 7 without any user agent to enforce it on.
DEBUG: Rule at line 9 without any user agent to enforce it on.
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Rule at line 11 without any user agent to enforce it on.
DEBUG: Rule at line 15 without any user agent to enforce it on.
DEBUG: Rule at line 16 without any user agent to enforce it on.
DEBUG: Rule at line 17 without any user agent to enforce it on.
DEBUG: Rule at line 18 without any user agent to enforce it on.
DEBUG: Rule at line 22 without any user agent to enforce it on.
DEBUG: Rule at line 23 without any user agent to enforce it on.
DEBUG: Rule at line 24 without any user agent to enforce it on.
DEBUG: Rule at line 25 without any user agent to enforce it on.
DEBUG: Rule at line 29 without any user agent to enforce it on.
DEBUG: Rule at line 30 without any user agent to enforce it on.
DEBUG: Rule at line 31 without any user agent to enforce it on.
DEBUG: Rule at line 32 without any user agent to enforce it on.
DEBUG: Rule at line 33 without any user agent to enforce it on.
DEBUG: Rule at line 37 without any user agent to enforce it on.
DEBUG: Rule at line 38 without any user agent to enforce it on.
DEBUG: Rule at line 39 without any user agent to enforce it on.
DEBUG: Rule at line 40 without any user agent to enforce it on.
DEBUG: Rule at line 44 without any user agent to enforce it on.
DEBUG: Rule at line 48 without any user agent to enforce it on.
DEBUG: Rule at line 49 without any user agent to enforce it on.
DEBUG: Rule at line 50 without any user agent to enforce it on.
DEBUG: Rule at line 51 without any user agent to enforce it on.
DEBUG: Rule at line 52 without any user agent to enforce it on.
DEBUG: Rule at line 53 without any user agent to enforce it on.
DEBUG: Rule at line 54 without any user agent to enforce it on.
DEBUG: Rule at line 55 without any user agent to enforce it on.
DEBUG: Rule at line 56 without any user agent to enforce it on.
DEBUG: Rule at line 57 without any user agent to enforce it on.
DEBUG: Rule at line 58 without any user agent to enforce it on.
DEBUG: Rule at line 59 without any user agent to enforce it on.
DEBUG: Rule at line 60 without any user agent to enforce it on.
DEBUG: Rule at line 61 without any user agent to enforce it on.
DEBUG: Rule at line 62 without any user agent to enforce it on.
DEBUG: Rule at line 63 without any user agent to enforce it on.
DEBUG: Rule at line 64 without any user agent to enforce it on.
DEBUG: Rule at line 65 without any user agent to enforce it on.
DEBUG: Rule at line 66 without any user agent to enforce it on.
DEBUG: Rule at line 70 without any user agent to enforce it on.
DEBUG: Rule at line 72 without any user agent to enforce it on.
DEBUG: Rule at line 80 without any user agent to enforce it on.
DEBUG: Rule at line 87 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://www.sycamoresprings.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://sweetwaterspa.com/robots.txt> from <GET http://www.sweetwaterspa.com/robots.txt>
DEBUG: Crawled (200) <GET https://glenivy.com/> (referer: None)
DEBUG: Crawled (200) <GET https://davidrubensteinforum.uchicago.edu/contact/> (referer: https://www.chaminade.com/?utm_source=google&utm_medium=organic&utm_campaign=gmb)
DEBUG: Crawled (200) <GET https://www.ranchovalencia.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://the-spring.com/contact/> (referer: https://the-spring.com/)
DEBUG: Scraped from <200 https://the-spring.com/contact/>
{'emails': ['info@the-spring.com'],
 'facebook': 'https://www.facebook.com/thespringresortandspa/',
 'instagram': 'https://www.instagram.com/thespringresort/',
 'linkedin': '',
 'twitter': ''}
DEBUG: Redirecting (301) to <GET https://www.ranchovalencia.com/> from <GET http://www.ranchovalencia.com/>
DEBUG: Redirecting (301) to <GET http://wecarespa.com/robots.txt> from <GET http://www.wecarespa.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.laquintaresort.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.lapeauspafresno.com/robots.txt> from <GET http://www.lapeauspafresno.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.canyonranch.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.canyonranch.com/woodside/northern-california-retreat/?utm_source=googlemybusiness&utm_medium=organic&utm_campaign=woodsidegmb&utm_term=googlemybusiness> (referer: None)
DEBUG: Crawled (404) <GET https://sweetwaterspa.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://beverlyhotsprings.com/robots.txt> from <GET http://www.beverlyhotsprings.com/robots.txt>
DEBUG: Crawled (200) <GET https://azurepalmhotsprings.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://wecarespa.com/robots.txt> (referer: None)
DEBUG: Rule at line 1 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://www.lapeauspafresno.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://sweetwaterspa.com/> from <GET http://www.sweetwaterspa.com/>
DEBUG: Redirecting (301) to <GET https://banyancay.com/contact-us.html> from <GET https://www.banyancay.com/contact-us.html>
DEBUG: Redirecting (301) to <GET https://lagunacanyonspa.com/robots.txt> from <GET http://www.lagunacanyonspa.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://www.lapeauspafresno.com/> from <GET http://www.lapeauspafresno.com/>
DEBUG: Crawled (200) <GET https://lagunacanyonspa.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://azurepalmhotsprings.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://lagunacanyonspa.com/> from <GET http://www.lagunacanyonspa.com/>
DEBUG: Crawled (200) <GET https://banyancay.com/robots.txt> (referer: None)
DEBUG: Rule at line 2 without any user agent to enforce it on.
DEBUG: Rule at line 7 without any user agent to enforce it on.
DEBUG: Rule at line 9 without any user agent to enforce it on.
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Rule at line 11 without any user agent to enforce it on.
DEBUG: Rule at line 15 without any user agent to enforce it on.
DEBUG: Rule at line 16 without any user agent to enforce it on.
DEBUG: Rule at line 17 without any user agent to enforce it on.
DEBUG: Rule at line 18 without any user agent to enforce it on.
DEBUG: Rule at line 22 without any user agent to enforce it on.
DEBUG: Rule at line 23 without any user agent to enforce it on.
DEBUG: Rule at line 24 without any user agent to enforce it on.
DEBUG: Rule at line 25 without any user agent to enforce it on.
DEBUG: Rule at line 29 without any user agent to enforce it on.
DEBUG: Rule at line 30 without any user agent to enforce it on.
DEBUG: Rule at line 31 without any user agent to enforce it on.
DEBUG: Rule at line 32 without any user agent to enforce it on.
DEBUG: Rule at line 33 without any user agent to enforce it on.
DEBUG: Rule at line 37 without any user agent to enforce it on.
DEBUG: Rule at line 38 without any user agent to enforce it on.
DEBUG: Rule at line 39 without any user agent to enforce it on.
DEBUG: Rule at line 40 without any user agent to enforce it on.
DEBUG: Rule at line 44 without any user agent to enforce it on.
DEBUG: Rule at line 48 without any user agent to enforce it on.
DEBUG: Rule at line 49 without any user agent to enforce it on.
DEBUG: Rule at line 50 without any user agent to enforce it on.
DEBUG: Rule at line 51 without any user agent to enforce it on.
DEBUG: Rule at line 52 without any user agent to enforce it on.
DEBUG: Rule at line 53 without any user agent to enforce it on.
DEBUG: Rule at line 54 without any user agent to enforce it on.
DEBUG: Rule at line 55 without any user agent to enforce it on.
DEBUG: Rule at line 56 without any user agent to enforce it on.
DEBUG: Rule at line 57 without any user agent to enforce it on.
DEBUG: Rule at line 58 without any user agent to enforce it on.
DEBUG: Rule at line 59 without any user agent to enforce it on.
DEBUG: Rule at line 60 without any user agent to enforce it on.
DEBUG: Rule at line 61 without any user agent to enforce it on.
DEBUG: Rule at line 62 without any user agent to enforce it on.
DEBUG: Rule at line 63 without any user agent to enforce it on.
DEBUG: Rule at line 64 without any user agent to enforce it on.
DEBUG: Rule at line 65 without any user agent to enforce it on.
DEBUG: Rule at line 66 without any user agent to enforce it on.
DEBUG: Rule at line 70 without any user agent to enforce it on.
DEBUG: Rule at line 72 without any user agent to enforce it on.
DEBUG: Rule at line 80 without any user agent to enforce it on.
DEBUG: Rule at line 87 without any user agent to enforce it on.
DEBUG: Redirecting (301) to <GET https://wecarespa.com/> from <GET http://www.wecarespa.com/>
DEBUG: Crawled (200) <GET https://lagunacanyonspa.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://ranchovalencia.com/> from <GET https://www.ranchovalencia.com/>
DEBUG: Crawled (200) <GET https://www.indianspringscalistoga.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://beverlyhotsprings.com/robots.txt> (referer: None)
DEBUG: Crawled (404) <GET https://sweetwaterspa.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://lagunacanyonspa.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://banyancay.com/> from <GET https://banyancay.com/contact-us.html>
DEBUG: Crawled (200) <GET http://www.carnerosresort.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.lapeauspafresno.com/> (referer: None)
DEBUG: Crawled (200) <GET https://thesisleyspa.com/los-angeles/> (referer: None)
DEBUG: Crawled (200) <GET https://amenitiesspa.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://beverlyhotsprings.com/> from <GET http://www.beverlyhotsprings.com/>
DEBUG: Crawled (200) <GET https://banyancay.com/> (referer: https://www.chaminade.com/?utm_source=google&utm_medium=organic&utm_campaign=gmb)
DEBUG: Crawled (200) <GET https://www.estanciadayspa.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.indianspringscalistoga.com/?utm_source=google&utm_medium=organic&utm_campaign=business-listing> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.carnerosresort.com/> from <GET http://www.carnerosresort.com/>
DEBUG: Crawled (200) <GET https://paradisepoint.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.estanciadayspa.com/> (referer: None)
DEBUG: Crawled (200) <GET http://beverlyhotsprings.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.cavallopoint.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://wecarespa.com/robots.txt> (referer: None)
DEBUG: Rule at line 1 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://paradisepoint.com/california-resort-spa/> (referer: None)
DEBUG: Crawled (200) <GET https://azurepalmhotsprings.com/contact-us/> (referer: https://azurepalmhotsprings.com/)
DEBUG: Crawled (200) <GET https://thesisleyspa.com/contact-us/> (referer: https://thesisleyspa.com/los-angeles/)
DEBUG: Scraped from <200 https://azurepalmhotsprings.com/contact-us/>
{'emails': ['info@azurepalm.com'],
 'facebook': 'https://www.facebook.com/azurepalmhotsprings',
 'instagram': 'https://www.instagram.com/azurepalmhotsprings/',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://theravenspa.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://carnerosresort.com/> from <GET https://www.carnerosresort.com/>
DEBUG: Crawled (200) <GET http://carnerosresort.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://amenitiesspa.com/> (referer: None)
DEBUG: Crawled (404) <GET https://azurepalmhotsprings.com/dev/contact-us/> (referer: https://azurepalmhotsprings.com/)
INFO: Ignoring response <404 https://azurepalmhotsprings.com/dev/contact-us/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://paradisepoint.com/contact-us/> (referer: https://paradisepoint.com/california-resort-spa/)
DEBUG: Crawled (200) <GET https://www.indianspringscalistoga.com/contact-us> (referer: https://www.indianspringscalistoga.com/?utm_source=google&utm_medium=organic&utm_campaign=business-listing)
DEBUG: Redirecting (301) to <GET https://carnerosresort.com/> from <GET http://carnerosresort.com/>
DEBUG: Crawled (200) <GET https://wecarespa.com/> (referer: None)
DEBUG: Crawled (200) <GET https://thehealingcornerca.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://carmelvalleyranch.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://theravenspa.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.cavallopoint.com/spa?utm_source=google-gmb&utm_medium=organic&utm_campaign=gmb> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.trilogyspa.com/robots.txt> from <GET http://www.trilogyspa.com/robots.txt>
DEBUG: Crawled (200) <GET https://carmelvalleyranch.com/spa/spa-aiyana/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.sweetwaterspa.com/> from <GET https://sweetwaterspa.com/>
DEBUG: Crawled (404) <GET https://www.ranchobernardoinn.com/robots.txt> (referer: None)
DEBUG: Rule at line 14 without any user agent to enforce it on.
DEBUG: Rule at line 15 without any user agent to enforce it on.
DEBUG: Rule at line 56 without any user agent to enforce it on.
DEBUG: Rule at line 57 without any user agent to enforce it on.
DEBUG: Rule at line 61 without any user agent to enforce it on.
DEBUG: Rule at line 62 without any user agent to enforce it on.
DEBUG: Rule at line 63 without any user agent to enforce it on.
DEBUG: Rule at line 67 without any user agent to enforce it on.
DEBUG: Rule at line 71 without any user agent to enforce it on.
DEBUG: Rule at line 73 without any user agent to enforce it on.
DEBUG: Rule at line 74 without any user agent to enforce it on.
DEBUG: Rule at line 83 without any user agent to enforce it on.
DEBUG: Rule at line 88 without any user agent to enforce it on.
DEBUG: Rule at line 101 without any user agent to enforce it on.
DEBUG: Rule at line 103 without any user agent to enforce it on.
DEBUG: Rule at line 104 without any user agent to enforce it on.
DEBUG: Rule at line 123 without any user agent to enforce it on.
DEBUG: Rule at line 141 without any user agent to enforce it on.
DEBUG: Rule at line 144 without any user agent to enforce it on.
DEBUG: Rule at line 145 without any user agent to enforce it on.
DEBUG: Rule at line 146 without any user agent to enforce it on.
DEBUG: Rule at line 152 without any user agent to enforce it on.
DEBUG: Rule at line 173 without any user agent to enforce it on.
DEBUG: Rule at line 241 without any user agent to enforce it on.
DEBUG: Rule at line 242 without any user agent to enforce it on.
DEBUG: Rule at line 249 without any user agent to enforce it on.
DEBUG: Rule at line 260 without any user agent to enforce it on.
DEBUG: Rule at line 265 without any user agent to enforce it on.
DEBUG: Rule at line 268 without any user agent to enforce it on.
DEBUG: Rule at line 271 without any user agent to enforce it on.
DEBUG: Rule at line 274 without any user agent to enforce it on.
DEBUG: Rule at line 277 without any user agent to enforce it on.
DEBUG: Rule at line 280 without any user agent to enforce it on.
DEBUG: Rule at line 283 without any user agent to enforce it on.
DEBUG: Rule at line 288 without any user agent to enforce it on.
DEBUG: Rule at line 293 without any user agent to enforce it on.
DEBUG: Rule at line 296 without any user agent to enforce it on.
DEBUG: Rule at line 299 without any user agent to enforce it on.
DEBUG: Rule at line 302 without any user agent to enforce it on.
DEBUG: Rule at line 305 without any user agent to enforce it on.
DEBUG: Rule at line 308 without any user agent to enforce it on.
DEBUG: Rule at line 311 without any user agent to enforce it on.
DEBUG: Rule at line 314 without any user agent to enforce it on.
DEBUG: Rule at line 320 without any user agent to enforce it on.
DEBUG: Rule at line 325 without any user agent to enforce it on.
DEBUG: Rule at line 330 without any user agent to enforce it on.
DEBUG: Rule at line 333 without any user agent to enforce it on.
DEBUG: Rule at line 336 without any user agent to enforce it on.
DEBUG: Rule at line 339 without any user agent to enforce it on.
DEBUG: Rule at line 342 without any user agent to enforce it on.
DEBUG: Rule at line 345 without any user agent to enforce it on.
DEBUG: Rule at line 348 without any user agent to enforce it on.
DEBUG: Rule at line 353 without any user agent to enforce it on.
DEBUG: Rule at line 358 without any user agent to enforce it on.
DEBUG: Rule at line 365 without any user agent to enforce it on.
DEBUG: Rule at line 368 without any user agent to enforce it on.
DEBUG: Rule at line 371 without any user agent to enforce it on.
DEBUG: Rule at line 374 without any user agent to enforce it on.
DEBUG: Rule at line 377 without any user agent to enforce it on.
DEBUG: Rule at line 380 without any user agent to enforce it on.
DEBUG: Rule at line 385 without any user agent to enforce it on.
DEBUG: Rule at line 390 without any user agent to enforce it on.
DEBUG: Rule at line 395 without any user agent to enforce it on.
DEBUG: Rule at line 398 without any user agent to enforce it on.
DEBUG: Rule at line 401 without any user agent to enforce it on.
DEBUG: Rule at line 404 without any user agent to enforce it on.
DEBUG: Rule at line 407 without any user agent to enforce it on.
DEBUG: Rule at line 410 without any user agent to enforce it on.
DEBUG: Rule at line 413 without any user agent to enforce it on.
DEBUG: Rule at line 416 without any user agent to enforce it on.
DEBUG: Rule at line 419 without any user agent to enforce it on.
DEBUG: Rule at line 422 without any user agent to enforce it on.
DEBUG: Rule at line 425 without any user agent to enforce it on.
DEBUG: Rule at line 430 without any user agent to enforce it on.
DEBUG: Rule at line 435 without any user agent to enforce it on.
DEBUG: Rule at line 438 without any user agent to enforce it on.
DEBUG: Rule at line 441 without any user agent to enforce it on.
DEBUG: Rule at line 444 without any user agent to enforce it on.
DEBUG: Rule at line 450 without any user agent to enforce it on.
DEBUG: Rule at line 453 without any user agent to enforce it on.
DEBUG: Rule at line 458 without any user agent to enforce it on.
DEBUG: Rule at line 463 without any user agent to enforce it on.
DEBUG: Rule at line 466 without any user agent to enforce it on.
DEBUG: Rule at line 469 without any user agent to enforce it on.
DEBUG: Rule at line 472 without any user agent to enforce it on.
DEBUG: Rule at line 475 without any user agent to enforce it on.
DEBUG: Rule at line 478 without any user agent to enforce it on.
DEBUG: Rule at line 490 without any user agent to enforce it on.
DEBUG: Rule at line 491 without any user agent to enforce it on.
DEBUG: Rule at line 511 without any user agent to enforce it on.
DEBUG: Rule at line 543 without any user agent to enforce it on.
DEBUG: Rule at line 545 without any user agent to enforce it on.
DEBUG: Rule at line 550 without any user agent to enforce it on.
DEBUG: Rule at line 551 without any user agent to enforce it on.
DEBUG: Rule at line 552 without any user agent to enforce it on.
DEBUG: Rule at line 620 without any user agent to enforce it on.
DEBUG: Rule at line 627 without any user agent to enforce it on.
DEBUG: Rule at line 629 without any user agent to enforce it on.
DEBUG: Rule at line 633 without any user agent to enforce it on.
DEBUG: Rule at line 637 without any user agent to enforce it on.
DEBUG: Rule at line 641 without any user agent to enforce it on.
DEBUG: Rule at line 642 without any user agent to enforce it on.
DEBUG: Rule at line 643 without any user agent to enforce it on.
DEBUG: Rule at line 644 without any user agent to enforce it on.
DEBUG: Rule at line 658 without any user agent to enforce it on.
DEBUG: Rule at line 662 without any user agent to enforce it on.
DEBUG: Rule at line 663 without any user agent to enforce it on.
DEBUG: Rule at line 664 without any user agent to enforce it on.
DEBUG: Rule at line 665 without any user agent to enforce it on.
DEBUG: Rule at line 666 without any user agent to enforce it on.
DEBUG: Rule at line 684 without any user agent to enforce it on.
DEBUG: Rule at line 685 without any user agent to enforce it on.
DEBUG: Rule at line 686 without any user agent to enforce it on.
DEBUG: Rule at line 693 without any user agent to enforce it on.
DEBUG: Rule at line 694 without any user agent to enforce it on.
DEBUG: Rule at line 698 without any user agent to enforce it on.
DEBUG: Rule at line 699 without any user agent to enforce it on.
DEBUG: Rule at line 700 without any user agent to enforce it on.
DEBUG: Rule at line 701 without any user agent to enforce it on.
DEBUG: Rule at line 702 without any user agent to enforce it on.
DEBUG: Rule at line 703 without any user agent to enforce it on.
DEBUG: Rule at line 705 without any user agent to enforce it on.
DEBUG: Rule at line 707 without any user agent to enforce it on.
DEBUG: Rule at line 747 without any user agent to enforce it on.
DEBUG: Rule at line 754 without any user agent to enforce it on.
DEBUG: Rule at line 755 without any user agent to enforce it on.
DEBUG: Rule at line 780 without any user agent to enforce it on.
DEBUG: Rule at line 787 without any user agent to enforce it on.
DEBUG: Rule at line 788 without any user agent to enforce it on.
DEBUG: Rule at line 818 without any user agent to enforce it on.
DEBUG: Rule at line 821 without any user agent to enforce it on.
DEBUG: Rule at line 828 without any user agent to enforce it on.
DEBUG: Rule at line 829 without any user agent to enforce it on.
DEBUG: Rule at line 834 without any user agent to enforce it on.
DEBUG: Rule at line 836 without any user agent to enforce it on.
DEBUG: Rule at line 839 without any user agent to enforce it on.
DEBUG: Rule at line 840 without any user agent to enforce it on.
DEBUG: Rule at line 841 without any user agent to enforce it on.
DEBUG: Rule at line 894 without any user agent to enforce it on.
DEBUG: Rule at line 895 without any user agent to enforce it on.
DEBUG: Rule at line 896 without any user agent to enforce it on.
DEBUG: Rule at line 897 without any user agent to enforce it on.
DEBUG: Rule at line 898 without any user agent to enforce it on.
DEBUG: Rule at line 900 without any user agent to enforce it on.
DEBUG: Rule at line 905 without any user agent to enforce it on.
DEBUG: Rule at line 906 without any user agent to enforce it on.
DEBUG: Rule at line 907 without any user agent to enforce it on.
DEBUG: Rule at line 908 without any user agent to enforce it on.
DEBUG: Rule at line 913 without any user agent to enforce it on.
DEBUG: Rule at line 932 without any user agent to enforce it on.
DEBUG: Rule at line 933 without any user agent to enforce it on.
DEBUG: Rule at line 944 without any user agent to enforce it on.
DEBUG: Rule at line 953 without any user agent to enforce it on.
DEBUG: Rule at line 956 without any user agent to enforce it on.
DEBUG: Rule at line 961 without any user agent to enforce it on.
DEBUG: Rule at line 970 without any user agent to enforce it on.
DEBUG: Rule at line 986 without any user agent to enforce it on.
DEBUG: Rule at line 987 without any user agent to enforce it on.
DEBUG: Rule at line 988 without any user agent to enforce it on.
DEBUG: Rule at line 989 without any user agent to enforce it on.
DEBUG: Rule at line 995 without any user agent to enforce it on.
DEBUG: Redirecting (301) to <GET https://www.thebluedoorhanford.com/robots.txt> from <GET http://www.thebluedoorhanford.com/robots.txt>
DEBUG: Crawled (200) <GET http://beverlyhotsprings.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.sweetwaterspa.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://wecarespa.com/contact-us/> from <GET https://wecarespa.com/contact/>
DEBUG: Crawled (200) <GET https://www.miraclesprings.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.trilogyspa.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://wecarespa.com/contact-us/> (referer: https://wecarespa.com/)
DEBUG: Redirecting (301) to <GET http://harbin.org/robots.txt> from <GET http://www.harbin.org/robots.txt>
DEBUG: Redirecting (301) to <GET https://www.teahousespa.com/robots.txt> from <GET http://www.teahousespa.com/robots.txt>
DEBUG: Crawled (200) <GET https://carnerosresort.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.sweetwaterspa.com/contact-us/> (referer: https://www.sweetwaterspa.com/)
DEBUG: Redirecting (301) to <GET https://www.trilogyspa.com/> from <GET http://www.trilogyspa.com/>
DEBUG: Scraped from <200 https://wecarespa.com/contact-us/>
{'emails': ['info@wecarespa.com'],
 'facebook': 'https://www.facebook.com/WeCareSpa',
 'instagram': 'https://www.instagram.com/wecarespaca/',
 'linkedin': '',
 'twitter': 'https://twitter.com/wecarespa'}
DEBUG: Scraped from <200 https://www.sweetwaterspa.com/contact-us/>
{'emails': ['lodging@sweetwaterspa.com'],
 'facebook': 'https://www.facebook.com/SweetwaterInnandSpa/',
 'instagram': 'https://www.instagram.com/sweetwaterinnspa/',
 'linkedin': '',
 'twitter': 'https://twitter.com/SweetwaterInnCA'}
DEBUG: Crawled (200) <GET https://www.thebluedoorhanford.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.trilogyspa.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.missioninn.com/robots.txt> from <GET http://www.missioninn.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://www.thebluedoorhanford.com/> from <GET http://www.thebluedoorhanford.com/>
DEBUG: Redirecting (301) to <GET https://harbin.org/robots.txt> from <GET http://harbin.org/robots.txt>
DEBUG: Crawled (200) <GET https://www.paseahotel.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.missioninn.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://beverlyhotsprings.com/contact/> (referer: http://beverlyhotsprings.com/)
DEBUG: Redirecting (301) to <GET https://www.missioninn.com/> from <GET http://www.missioninn.com/>
DEBUG: Scraped from <200 http://beverlyhotsprings.com/contact/>
{'emails': ['beverlyhotsprings@gmail.com'],
 'facebook': 'https://www.facebook.com/beverlyhotsprings',
 'instagram': '',
 'linkedin': '',
 'twitter': 'https://twitter.com/BHSdayspa'}
DEBUG: Crawled (200) <GET https://www.teahousespa.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.thelandingtahoe.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.missioninn.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.thelandingtahoe.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.thebluedoorhanford.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.teahousespa.com/> from <GET http://www.teahousespa.com/>
DEBUG: Crawled (200) <GET https://www.montagehotels.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET https://www.montagehotels.com/lagunabeach/> (referer: None)
DEBUG: Crawled (200) <GET https://www.teahousespa.com/> (referer: None)
INFO: Ignoring response <403 https://www.montagehotels.com/lagunabeach/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://www.paseahotel.com/?utm_medium=organic&utm_source=google&utm_campaign=business-listing> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.bardessono.com/robots.txt> from <GET https://bardessono.com/robots.txt>
DEBUG: Crawled (200) <GET https://harbin.org/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.ranchobernardoinn.com/spa/overview?utm_source=gmb&utm_medium=yext> (referer: None)
DEBUG: Redirecting (301) to <GET https://mysheerbliss.com/robots.txt> from <GET https://www.mysheerbliss.com/robots.txt>
DEBUG: Crawled (200) <GET https://carmelvalleyranch.com/contact/> (referer: https://carmelvalleyranch.com/spa/spa-aiyana/)
DEBUG: Crawled (200) <GET https://www.silveradoresort.com/?utm_source=google&utm_medium=organic&utm_campaign=business-listing> (referer: None)
DEBUG: Crawled (200) <GET https://mysheerbliss.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.estancialajolla.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://harbin.org/> from <GET http://www.harbin.org/>
DEBUG: Redirecting (301) to <GET https://doubleeagle.com/robots.txt> from <GET http://doubleeagle.com/robots.txt>
DEBUG: Crawled (200) <GET http://www.athenaspa-mv.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.paseahotel.com/contact/press-room> (referer: https://www.paseahotel.com/?utm_medium=organic&utm_source=google&utm_campaign=business-listing)
DEBUG: Crawled (200) <GET https://www.paseahotel.com/contact/blogs> (referer: https://www.paseahotel.com/?utm_medium=organic&utm_source=google&utm_campaign=business-listing)
DEBUG: Redirecting (301) to <GET https://mysheerbliss.com/> from <GET https://www.mysheerbliss.com/>
DEBUG: Crawled (200) <GET https://www.romanspahotsprings.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://mysheerbliss.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://www.athenaspa-mv.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.romanspahotsprings.com/?utm_source=google%20my%20business&utm_medium=listing&utm_campaign=visit%20website> (referer: None)
DEBUG: Crawled (200) <GET https://harbin.org/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.bardessono.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://mysheerbliss.com/> (referer: None)
DEBUG: Crawled (200) <GET https://milkandhoneyspa.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.estancialajolla.com/spa/?utm_source=gmb-spa&utm_medium=organic&utm_campaign=gmb> (referer: None)
DEBUG: Crawled (200) <GET https://petitespa.net/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.romanspahotsprings.com/contact> (referer: https://www.romanspahotsprings.com/?utm_source=google%20my%20business&utm_medium=listing&utm_campaign=visit%20website)
DEBUG: Redirecting (301) to <GET https://www.bardessono.com/> from <GET https://bardessono.com/>
DEBUG: Crawled (200) <GET https://petitespa.net/> (referer: None)
DEBUG: Scraped from <200 https://www.romanspahotsprings.com/contact>
{'emails': ['Reservations75@Romanspahotsprings.com'],
 'facebook': 'https://www.facebook.com/romanspahotsprings',
 'instagram': 'https://www.instagram.com/romanspahotspringsresort/?ref=badge',
 'linkedin': '',
 'twitter': 'https://twitter.com/romansparesort'}
DEBUG: Crawled (200) <GET http://www.athenaspa-mv.com/contact-us> (referer: http://www.athenaspa-mv.com/)
DEBUG: Crawled (200) <GET https://www.bardessono.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://mysheerbliss.com/robots.txt> from <GET https://sheer-bliss-organic-spa.myshopify.com/robots.txt>
DEBUG: Retrying <GET https://www.shuttersonthebeach.com/robots.txt> (failed 1 times): 502 Bad Gateway
DEBUG: Redirecting (301) to <GET https://www.senspa.com/robots.txt> from <GET http://www.senspa.com/robots.txt>
DEBUG: Crawled (200) <GET https://mysheerbliss.com/robots.txt> (referer: None)
DEBUG: Retrying <GET https://www.shuttersonthebeach.com/robots.txt> (failed 2 times): 502 Bad Gateway
DEBUG: Crawled (200) <GET https://harbin.org/> (referer: None)
DEBUG: Crawled (200) <GET https://www.estancialajolla.com/contact/> (referer: https://www.estancialajolla.com/spa/?utm_source=gmb-spa&utm_medium=organic&utm_campaign=gmb)
DEBUG: Redirecting (301) to <GET https://mysheerbliss.com/pages/contact-us-spa-policy> from <GET https://sheer-bliss-organic-spa.myshopify.com/pages/contact-us-spa-policy>
ERROR: Gave up retrying <GET https://www.shuttersonthebeach.com/robots.txt> (failed 3 times): 502 Bad Gateway
DEBUG: Crawled (502) <GET https://www.shuttersonthebeach.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://metropolissalonspa.com/robots.txt> (referer: None)
DEBUG: Forbidden by robots.txt: <GET http://metropolissalonspa.com/>
DEBUG: Crawled (200) <GET https://mysheerbliss.com/pages/contact-us-spa-policy> (referer: https://mysheerbliss.com/)
DEBUG: Crawled (200) <GET https://www.bardessono.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.paseahotel.com/contact> (referer: https://www.paseahotel.com/?utm_medium=organic&utm_source=google&utm_campaign=business-listing)
DEBUG: Retrying <GET https://www.shuttersonthebeach.com/spa/one-spa> (failed 1 times): 502 Bad Gateway
DEBUG: Redirecting (301) to <GET https://oceanohalfmoonbay.com/robots.txt> from <GET http://www.oceanohalfmoonbay.com/robots.txt>
DEBUG: Retrying <GET https://www.shuttersonthebeach.com/spa/one-spa> (failed 2 times): 502 Bad Gateway
DEBUG: Crawled (200) <GET https://milkandhoneyspa.com/?utm_source=google&utm_medium=organic&utm_campaign=gmb-listing&utm_content=brentwood> (referer: None)
DEBUG: Crawled (404) <GET https://www.ventanabigsur.com/robots.txt> (referer: None)
DEBUG: Rule at line 20 without any user agent to enforce it on.
DEBUG: Rule at line 43 without any user agent to enforce it on.
DEBUG: Rule at line 54 without any user agent to enforce it on.
DEBUG: Rule at line 55 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://www.senspa.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://oceanohalfmoonbay.com/robots.txt> (referer: None)
ERROR: Gave up retrying <GET https://www.shuttersonthebeach.com/spa/one-spa> (failed 3 times): 502 Bad Gateway
DEBUG: Crawled (502) <GET https://www.shuttersonthebeach.com/spa/one-spa> (referer: None)
DEBUG: Redirecting (301) to <GET https://oceanohalfmoonbay.com/> from <GET http://www.oceanohalfmoonbay.com/>
DEBUG: Crawled (200) <GET https://www.bardessono.com/contact.htm> (referer: https://www.bardessono.com/)
DEBUG: Redirecting (301) to <GET https://www.rushcreeklodge.com/robots.txt> from <GET http://www.rushcreeklodge.com/robots.txt>
DEBUG: Crawled (200) <GET https://oceanohalfmoonbay.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.senspa.com/> from <GET http://www.senspa.com/>
INFO: Ignoring response <502 https://www.shuttersonthebeach.com/spa/one-spa>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET https://www.doubleeagle.com/robots.txt> from <GET https://doubleeagle.com/robots.txt>
DEBUG: Crawled (200) <GET https://oceanohalfmoonbay.com/> (referer: None)
DEBUG: Crawled (451) <GET https://www.morongocasinoresort.com/robots.txt> (referer: None)
DEBUG: Rule at line 3 without any user agent to enforce it on.
DEBUG: Rule at line 4 without any user agent to enforce it on.
DEBUG: Rule at line 6 without any user agent to enforce it on.
DEBUG: Rule at line 27 without any user agent to enforce it on.
DEBUG: Rule at line 28 without any user agent to enforce it on.
DEBUG: Rule at line 29 without any user agent to enforce it on.
DEBUG: Rule at line 30 without any user agent to enforce it on.
DEBUG: Rule at line 31 without any user agent to enforce it on.
DEBUG: Rule at line 32 without any user agent to enforce it on.
DEBUG: Rule at line 35 without any user agent to enforce it on.
DEBUG: Rule at line 38 without any user agent to enforce it on.
DEBUG: Rule at line 41 without any user agent to enforce it on.
DEBUG: Rule at line 43 without any user agent to enforce it on.
DEBUG: Rule at line 44 without any user agent to enforce it on.
DEBUG: Rule at line 45 without any user agent to enforce it on.
DEBUG: Rule at line 46 without any user agent to enforce it on.
DEBUG: Rule at line 49 without any user agent to enforce it on.
DEBUG: Rule at line 50 without any user agent to enforce it on.
DEBUG: Rule at line 56 without any user agent to enforce it on.
DEBUG: Rule at line 57 without any user agent to enforce it on.
DEBUG: Rule at line 58 without any user agent to enforce it on.
DEBUG: Rule at line 64 without any user agent to enforce it on.
DEBUG: Rule at line 69 without any user agent to enforce it on.
DEBUG: Rule at line 70 without any user agent to enforce it on.
DEBUG: Rule at line 71 without any user agent to enforce it on.
DEBUG: Rule at line 72 without any user agent to enforce it on.
DEBUG: Rule at line 76 without any user agent to enforce it on.
DEBUG: Rule at line 77 without any user agent to enforce it on.
DEBUG: Rule at line 78 without any user agent to enforce it on.
DEBUG: Rule at line 79 without any user agent to enforce it on.
DEBUG: Rule at line 82 without any user agent to enforce it on.
DEBUG: Rule at line 86 without any user agent to enforce it on.
DEBUG: Rule at line 89 without any user agent to enforce it on.
DEBUG: Rule at line 90 without any user agent to enforce it on.
DEBUG: Rule at line 91 without any user agent to enforce it on.
DEBUG: Rule at line 92 without any user agent to enforce it on.
DEBUG: Rule at line 93 without any user agent to enforce it on.
DEBUG: Rule at line 94 without any user agent to enforce it on.
DEBUG: Rule at line 98 without any user agent to enforce it on.
DEBUG: Rule at line 102 without any user agent to enforce it on.
DEBUG: Rule at line 103 without any user agent to enforce it on.
DEBUG: Rule at line 104 without any user agent to enforce it on.
DEBUG: Rule at line 108 without any user agent to enforce it on.
DEBUG: Rule at line 109 without any user agent to enforce it on.
DEBUG: Rule at line 110 without any user agent to enforce it on.
DEBUG: Rule at line 114 without any user agent to enforce it on.
DEBUG: Rule at line 115 without any user agent to enforce it on.
DEBUG: Rule at line 116 without any user agent to enforce it on.
DEBUG: Rule at line 118 without any user agent to enforce it on.
DEBUG: Rule at line 119 without any user agent to enforce it on.
DEBUG: Rule at line 120 without any user agent to enforce it on.
DEBUG: Rule at line 121 without any user agent to enforce it on.
DEBUG: Rule at line 122 without any user agent to enforce it on.
DEBUG: Rule at line 123 without any user agent to enforce it on.
DEBUG: Rule at line 124 without any user agent to enforce it on.
DEBUG: Rule at line 125 without any user agent to enforce it on.
DEBUG: Rule at line 126 without any user agent to enforce it on.
DEBUG: Rule at line 127 without any user agent to enforce it on.
DEBUG: Rule at line 128 without any user agent to enforce it on.
DEBUG: Rule at line 131 without any user agent to enforce it on.
DEBUG: Rule at line 132 without any user agent to enforce it on.
DEBUG: Rule at line 133 without any user agent to enforce it on.
DEBUG: Rule at line 137 without any user agent to enforce it on.
DEBUG: Rule at line 140 without any user agent to enforce it on.
DEBUG: Rule at line 143 without any user agent to enforce it on.
DEBUG: Rule at line 144 without any user agent to enforce it on.
DEBUG: Rule at line 145 without any user agent to enforce it on.
DEBUG: Rule at line 146 without any user agent to enforce it on.
DEBUG: Rule at line 147 without any user agent to enforce it on.
DEBUG: Rule at line 148 without any user agent to enforce it on.
DEBUG: Rule at line 150 without any user agent to enforce it on.
DEBUG: Rule at line 151 without any user agent to enforce it on.
DEBUG: Rule at line 154 without any user agent to enforce it on.
DEBUG: Rule at line 155 without any user agent to enforce it on.
DEBUG: Rule at line 156 without any user agent to enforce it on.
DEBUG: Rule at line 157 without any user agent to enforce it on.
DEBUG: Rule at line 160 without any user agent to enforce it on.
DEBUG: Rule at line 161 without any user agent to enforce it on.
DEBUG: Rule at line 164 without any user agent to enforce it on.
DEBUG: Rule at line 167 without any user agent to enforce it on.
DEBUG: Rule at line 170 without any user agent to enforce it on.
DEBUG: Rule at line 173 without any user agent to enforce it on.
DEBUG: Rule at line 174 without any user agent to enforce it on.
DEBUG: Rule at line 189 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://paradisepoint.com/?utm_source=google-local&utm_medium=organic&utm_campaign=gmb> (referer: None)
DEBUG: Crawled (451) <GET https://www.morongocasinoresort.com/> (referer: None)
DEBUG: Crawled (200) <GET https://oceanohalfmoonbay.com/contact/> (referer: https://oceanohalfmoonbay.com/)
DEBUG: Crawled (200) <GET https://www.senspa.com/> (referer: None)
INFO: Ignoring response <451 https://www.morongocasinoresort.com/>: HTTP status code is not handled or not allowed
DEBUG: Scraped from <200 https://oceanohalfmoonbay.com/contact/>
{'emails': ['admin@oceanohalfmoonbay.com'],
 'facebook': 'https://www.facebook.com/oceanohotelspa/',
 'instagram': 'https://www.instagram.com/oceanohotelspa/?ref=badge',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://www.macarthurplace.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://aquasoleilhotel.com/robots.txt> from <GET http://www.aquasoleilhotel.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.rushcreeklodge.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.thespaatfgv.com/robots.txt> from <GET http://www.thespaatfgv.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://www.rushcreeklodge.com/> from <GET http://www.rushcreeklodge.com/>
DEBUG: Crawled (200) <GET http://www.thankyouandbewell.com/robots.txt> (referer: None)
DEBUG: Forbidden by robots.txt: <GET http://www.thankyouandbewell.com/>
DEBUG: Redirecting (301) to <GET https://www.miramonteresort.com/robots.txt> from <GET http://www.miramonteresort.com/robots.txt>
DEBUG: Crawled (200) <GET https://aquasoleilhotel.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://aquasoleilhotel.com/> from <GET http://www.aquasoleilhotel.com/>
DEBUG: Crawled (200) <GET https://aquasoleilhotel.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://truerest.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://winerose.com/robots.txt> from <GET http://winerose.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.macarthurplace.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.ventanabigsur.com/spa/overview> (referer: None)
DEBUG: Crawled (200) <GET https://www.rushcreeklodge.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.lidopalms.com/robots.txt> from <GET http://www.lidopalms.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.miramonteresort.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.lidopalms.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.lidopalms.com/> from <GET http://www.lidopalms.com/>
DEBUG: Crawled (200) <GET https://thehealingcornerca.com/> (referer: None)
DEBUG: Crawled (200) <GET https://winerose.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.doubleeagle.com/robots.txt> (referer: None)
ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\engine.py", line 152, in _next_request
    request = next(self.slot.start_requests)
  File "C:\Users\moffi\PycharmProjects\scraping_web\scraper\scraper\spiders\website_spider.py", line 34, in start_requests
    for row in reader:
  File "C:\Users\moffi\AppData\Local\Programs\Python\Python310\lib\encodings\cp1252.py", line 23, in decode
    return codecs.charmap_decode(input,self.errors,decoding_table)[0]
UnicodeDecodeError: 'charmap' codec can't decode byte 0x81 in position 2777: character maps to <undefined>
DEBUG: Redirecting (301) to <GET https://www.miramonteresort.com/spa> from <GET http://www.miramonteresort.com/spa>
DEBUG: Redirecting (301) to <GET https://winerose.com/> from <GET http://winerose.com/>
DEBUG: Crawled (200) <GET https://www.rushcreeklodge.com/us/contact-us/> (referer: https://www.rushcreeklodge.com/)
DEBUG: Crawled (200) <GET https://www.lidopalms.com/> (referer: None)
DEBUG: Crawled (403) <GET https://truerest.com/locations/fresno/> (referer: None)
DEBUG: Scraped from <200 https://www.rushcreeklodge.com/us/contact-us/>
{'emails': ['info@rushcreeklodge.com.'],
 'facebook': 'https://www.facebook.com/RushCreekLodge/',
 'instagram': 'https://www.instagram.com/yosemite_rushcreek/',
 'linkedin': '',
 'twitter': ''}
INFO: Ignoring response <403 https://truerest.com/locations/fresno/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://winerose.com/> (referer: None)
DEBUG: Redirecting (302) to <GET https://www.kissmemedspa.com/robots.txt> from <GET http://kissmemedspa.com/robots.txt>
DEBUG: Crawled (200) <GET https://aquasoleilhotel.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.miramonteresort.com/the-well-spa/> from <GET https://www.miramonteresort.com/spa>
DEBUG: Crawled (200) <GET https://www.miramonteresort.com/the-well-spa/> (referer: None)
DEBUG: Crawled (200) <GET https://mystiquemedicalspa.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://aquasoleilhotel.com/contact/> (referer: https://aquasoleilhotel.com/)
DEBUG: Redirecting (301) to <GET https://doubleeagle.com/> from <GET http://doubleeagle.com/>
DEBUG: Crawled (200) <GET https://www.macarthurplace.com/contact/> (referer: https://www.macarthurplace.com/)
DEBUG: Crawled (200) <GET https://mystiquemedicalspa.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.kissmemedspa.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://mystiquemedicalspa.com/about-us/contact/> (referer: https://mystiquemedicalspa.com/)
DEBUG: Scraped from <200 https://mystiquemedicalspa.com/about-us/contact/>
{'emails': ['questions@mystiquemedicalspa.com'],
 'facebook': 'https://www.facebook.com/MystiqueFresno',
 'instagram': 'https://www.instagram.com/mystiquemedspa/',
 'linkedin': '',
 'twitter': ''}
DEBUG: Redirecting (302) to <GET https://www.kissmemedspa.com/> from <GET http://kissmemedspa.com/>
DEBUG: Crawled (200) <GET https://www.kissmemedspa.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.doubleeagle.com/> from <GET https://doubleeagle.com/>
DEBUG: Crawled (200) <GET https://www.kissmemedspa.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.doubleeagle.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://thespaatfgv.com/robots.txt> from <GET https://www.thespaatfgv.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.doubleeagle.com/> (referer: None)
DEBUG: Crawled (200) <GET https://thespaatfgv.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.thespaatfgv.com/> from <GET http://www.thespaatfgv.com/>
DEBUG: Redirecting (301) to <GET https://thespaatfgv.com/> from <GET https://www.thespaatfgv.com/>
DEBUG: Crawled (200) <GET https://thespaatfgv.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://thespaatfgv.com/> (referer: None)
DEBUG: Crawled (200) <GET https://thespaatfgv.com/contact/> (referer: https://thespaatfgv.com/)
DEBUG: Scraped from <200 https://thespaatfgv.com/contact/>
{'emails': ['tammie@hungryhairsalon.com'],
 'facebook': 'https://www.facebook.com/SpaHungryHair/',
 'instagram': 'https://www.instagram.com/themedicalspa_hungryhair/',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://www.miraclesprings.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.miraclesprings.com/contact/> (referer: https://www.miraclesprings.com/)
INFO: Closing spider (finished)
INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/scrapy.exceptions.IgnoreRequest': 3,
 'downloader/request_bytes': 92554,
 'downloader/request_count': 334,
 'downloader/request_method_count/GET': 334,
 'downloader/response_bytes': 6254568,
 'downloader/response_count': 334,
 'downloader/response_status_count/200': 213,
 'downloader/response_status_count/301': 97,
 'downloader/response_status_count/302': 2,
 'downloader/response_status_count/403': 2,
 'downloader/response_status_count/404': 6,
 'downloader/response_status_count/429': 6,
 'downloader/response_status_count/451': 2,
 'downloader/response_status_count/502': 6,
 'dupefilter/filtered': 42,
 'elapsed_time_seconds': 30.76051,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 9, 13, 25, 18, 378224),
 'httpcompression/response_bytes': 21180161,
 'httpcompression/response_count': 196,
 'httperror/response_ignored_count': 6,
 'httperror/response_ignored_status_count/403': 2,
 'httperror/response_ignored_status_count/404': 1,
 'httperror/response_ignored_status_count/429': 1,
 'httperror/response_ignored_status_count/451': 1,
 'httperror/response_ignored_status_count/502': 1,
 'item_scraped_count': 12,
 'log_count/DEBUG': 720,
 'log_count/ERROR': 5,
 'log_count/INFO': 16,
 'request_depth_max': 1,
 'response_received_count': 227,
 'retry/count': 8,
 'retry/max_reached': 4,
 'retry/reason_count/429 Unknown Status': 4,
 'retry/reason_count/502 Bad Gateway': 4,
 'robotstxt/forbidden': 3,
 'robotstxt/request_count': 106,
 'robotstxt/response_count': 106,
 'robotstxt/response_status_count/200': 98,
 'robotstxt/response_status_count/404': 5,
 'robotstxt/response_status_count/429': 1,
 'robotstxt/response_status_count/451': 1,
 'robotstxt/response_status_count/502': 1,
 'scheduler/dequeued': 186,
 'scheduler/dequeued/memory': 186,
 'scheduler/enqueued': 186,
 'scheduler/enqueued/memory': 186,
 'start_time': datetime.datetime(2022, 12, 9, 13, 24, 47, 617714)}
INFO: Spider closed (finished)
INFO: Scrapy 2.7.1 started (bot: scraper)
INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.0, Twisted 22.10.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Windows-10-10.0.19044-SP0
INFO: Overridden settings:
{'BOT_NAME': 'scraper',
 'NEWSPIDER_MODULE': 'scraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
DEBUG: Using selector: SelectSelector
DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
INFO: Telnet Password: 1ad21271b3eb6e11
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
[]
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO: Telnet console listening on 127.0.0.1:6023
DEBUG: Redirecting (301) to <GET https://www.cal-a-vie.com/robots.txt> from <GET http://www.cal-a-vie.com/robots.txt>
DEBUG: Attempting to acquire lock 2479866446896 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2479866446896 acquired on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Attempting to release lock 2479866446896 on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Lock 2479866446896 released on C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\tldextract\.suffix_cache/publicsuffix.org-tlds\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock
DEBUG: Crawled (200) <GET https://www.omnihotels.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://kellysspa.com/robots.txt> from <GET http://kellysspa.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://www.kellysspa.com/robots.txt> from <GET https://kellysspa.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://www.goldenhaven.com/robots.txt> from <GET http://www.goldenhaven.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://www.glenivy.com/robots.txt> from <GET http://www.glenivy.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.kellysspa.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.omnihotels.com/hotels/san-diego-la-costa/spa?utm_source=GMBlisting&utm_medium=organic> (referer: None)
DEBUG: Crawled (200) <GET https://ranchovalencia.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://goldendoor.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://kellysspa.com/> from <GET http://kellysspa.com/>
DEBUG: Crawled (200) <GET https://www.cal-a-vie.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://cadayspa.com/robots.txt> from <GET http://cadayspa.com/robots.txt>
DEBUG: Crawled (200) <GET http://www.wispausa.com/robots.txt> (referer: None)
DEBUG: Filtered duplicate request: <GET https://www.omnihotels.com/forms/contact-us> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG: Redirecting (301) to <GET https://www.cal-a-vie.com/?utm_source=google&utm_medium=organic&utm_campaign=business-listing> from <GET http://www.cal-a-vie.com/?utm_source=google&utm_medium=organic&utm_campaign=business-listing>
DEBUG: Redirecting (301) to <GET https://www.kellysspa.com/> from <GET https://kellysspa.com/>
DEBUG: Crawled (200) <GET https://www.kellysspa.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://cadayspa.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://cadayspa.com/> from <GET http://cadayspa.com/>
DEBUG: Crawled (200) <GET https://www.kellysspa.com/> (referer: None)
DEBUG: Crawled (200) <GET https://cadayspa.com/> (referer: None)
DEBUG: Crawled (200) <GET https://goldendoor.com/> (referer: None)
DEBUG: Crawled (404) <GET https://www.ritzcarlton.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.goldenhaven.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://www.ritzcarlton.com/en/hotels/california/half-moon-bay/spa?scid=45f93f1b-bd77-45c9-8dab-83b6a417f6fe&y_source=1_MTY0OTQwNC03MTUtbG9jYXRpb24ud2Vic2l0ZQ%3D%3D> from <GET https://www.ritzcarlton.com/en/hotels/california/half-moon-bay/spa/?scid=45f93f1b-bd77-45c9-8dab-83b6a417f6fe&y_source=1_MTY0OTQwNC03MTUtbG9jYXRpb24ud2Vic2l0ZQ%3D%3D>
DEBUG: Redirecting (301) to <GET https://www.ritzcarlton.com/en/hotels/california/half-moon-bay/spa?scid=45f93f1b-bd77-45c9-8dab-83b6a417f6fe&y_source=1_MTY0OTQwNC03MTUtbG9jYXRpb24ud2Vic2l0ZQ%3D%3D> from <GET http://www.ritzcarlton.com/en/hotels/california/half-moon-bay/spa?scid=45f93f1b-bd77-45c9-8dab-83b6a417f6fe&y_source=1_MTY0OTQwNC03MTUtbG9jYXRpb24ud2Vic2l0ZQ%3D%3D>
DEBUG: Crawled (200) <GET https://www.omnihotels.com/forms/contact-us> (referer: https://www.omnihotels.com/hotels/san-diego-la-costa/spa?utm_source=GMBlisting&utm_medium=organic)
DEBUG: Crawled (200) <GET https://www.cal-a-vie.com/?utm_source=google&utm_medium=organic&utm_campaign=business-listing> (referer: None)
DEBUG: Crawled (200) <GET https://www.ritzcarlton.com/en/hotels/california/half-moon-bay/spa?scid=45f93f1b-bd77-45c9-8dab-83b6a417f6fe&y_source=1_MTY0OTQwNC03MTUtbG9jYXRpb24ud2Vic2l0ZQ%3D%3D> (referer: None)
DEBUG: Crawled (200) <GET https://www.osmosis.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.goldenhaven.com/> from <GET http://www.goldenhaven.com/>
DEBUG: Crawled (200) <GET https://www.madonnainn.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://goldendoor.com/contact-us/> (referer: https://goldendoor.com/)
DEBUG: Redirecting (301) to <GET https://locations.woodhousespas.com/dir/ca/walnut-creek/1636-cypress-st> from <GET http://walnutcreek.woodhousespas.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.osmosis.com/?y_source=1_MjI1MDUzNzItNzE1LWxvY2F0aW9uLndlYnNpdGU%3D> (referer: None)
DEBUG: Crawled (200) <GET https://montereyplazahotel.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://locations.woodhousespas.com/dir/ca/walnut-creek/1636-cypress-st> (referer: None)
DEBUG: Rule at line 1 without any user agent to enforce it on.
DEBUG: Rule at line 15 without any user agent to enforce it on.
DEBUG: Rule at line 22 without any user agent to enforce it on.
DEBUG: Rule at line 23 without any user agent to enforce it on.
DEBUG: Rule at line 24 without any user agent to enforce it on.
DEBUG: Rule at line 25 without any user agent to enforce it on.
DEBUG: Rule at line 26 without any user agent to enforce it on.
DEBUG: Rule at line 30 without any user agent to enforce it on.
DEBUG: Rule at line 31 without any user agent to enforce it on.
DEBUG: Rule at line 32 without any user agent to enforce it on.
DEBUG: Rule at line 35 without any user agent to enforce it on.
DEBUG: Rule at line 36 without any user agent to enforce it on.
DEBUG: Rule at line 38 without any user agent to enforce it on.
DEBUG: Rule at line 40 without any user agent to enforce it on.
DEBUG: Rule at line 41 without any user agent to enforce it on.
DEBUG: Rule at line 62 without any user agent to enforce it on.
DEBUG: Rule at line 67 without any user agent to enforce it on.
DEBUG: Rule at line 68 without any user agent to enforce it on.
DEBUG: Rule at line 74 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://www.goldenhaven.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.madonnainn.com/spa> (referer: None)
DEBUG: Crawled (200) <GET https://www.cal-a-vie.com/contact-us> (referer: https://www.cal-a-vie.com/?utm_source=google&utm_medium=organic&utm_campaign=business-listing)
DEBUG: Crawled (200) <GET https://aubergeresorts.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://locations.woodhousespas.com/dir/ca/walnut-creek/1636-cypress-st> from <GET http://walnutcreek.woodhousespas.com/?utm_source=google&utm_medium=yext>
DEBUG: Crawled (200) <GET https://www.fourseasons.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.fourseasons.com/westlakevillage/spa/?seo=google_local_wes6_amer> (referer: None)
DEBUG: Scraped from <200 https://www.cal-a-vie.com/contact-us>
{'emails': ['cavinfo@cal-a-vie.com'],
 'facebook': 'https://www.facebook.com/CalaVie',
 'instagram': 'https://www.instagram.com/calaviespa',
 'linkedin': '',
 'twitter': 'https://twitter.com/calaviespa'}
DEBUG: Crawled (200) <GET https://locations.woodhousespas.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET http://aubergeresorts.com/solage/wellness/spa/?utm_source=google%20my%20business&utm_medium=listing&utm_campaign=visit%20website> from <GET https://aubergeresorts.com/solage/wellness/spa?utm_source=google%20my%20business&utm_medium=listing&utm_campaign=visit%20website>
DEBUG: Crawled (200) <GET https://locations.woodhousespas.com/dir/ca/walnut-creek/1636-cypress-st> (referer: None)
DEBUG: Crawled (200) <GET https://www.southcoastwinery.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.mytrilogylife.com/robots.txt> from <GET http://www.mytrilogylife.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.osmosis.com/contact/> (referer: https://www.osmosis.com/?y_source=1_MjI1MDUzNzItNzE1LWxvY2F0aW9uLndlYnNpdGU%3D)
DEBUG: Redirecting (301) to <GET https://www.aubergeresorts.com/solage/wellness/spa/?utm_source=google%20my%20business&utm_medium=listing&utm_campaign=visit%20website> from <GET http://aubergeresorts.com/solage/wellness/spa/?utm_source=google%20my%20business&utm_medium=listing&utm_campaign=visit%20website>
DEBUG: Redirecting (301) to <GET https://glenivy.com/robots.txt> from <GET https://www.glenivy.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.purpleorchid.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://montereyplazahotel.com/spa/vista-blue-spa/?utm_source=google&utm_medium=bizlisting&utm_campaign=google_places> (referer: None)
DEBUG: Crawled (200) <GET https://www.silveradoresort.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.aubergeresorts.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://ranchovalencia.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.purpleorchid.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.chaminade.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.ritzcarlton.com/en/hotels/california/los-angeles/spa?scid=bb1a189a-fec3-4d19-a255-54ba596febe2> from <GET http://www.ritzcarlton.com/en/hotels/california/los-angeles/spa?scid=bb1a189a-fec3-4d19-a255-54ba596febe2>
DEBUG: Redirecting (301) to <GET http://boonhotels.com/robots.txt> from <GET http://www.boonhotels.com/robots.txt>
DEBUG: Crawled (200) <GET http://boonhotels.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.laquintaresort.com/robots.txt> from <GET http://www.laquintaresort.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.ritzcarlton.com/en/hotels/california/los-angeles/spa?scid=bb1a189a-fec3-4d19-a255-54ba596febe2> (referer: None)
DEBUG: Crawled (200) <GET https://www.mytrilogylife.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.southcoastwinery.com/spa/> from <GET https://www.southcoastwinery.com/spa>
DEBUG: Crawled (200) <GET https://montereyplazahotel.com/hotel/contact> (referer: https://montereyplazahotel.com/spa/vista-blue-spa/?utm_source=google&utm_medium=bizlisting&utm_campaign=google_places)
DEBUG: Crawled (200) <GET https://www.purpleorchid.com/contact-us/> (referer: https://www.purpleorchid.com/)
DEBUG: Redirecting (301) to <GET https://www.ritzcarlton.com/en/hotels/california/santa-barbara/spa?scid=bb1a189a-fec3-4d19-a255-54ba596febe2> from <GET http://www.ritzcarlton.com/en/hotels/california/santa-barbara/spa?scid=bb1a189a-fec3-4d19-a255-54ba596febe2>
DEBUG: Redirecting (301) to <GET https://www.mytrilogylife.com/monarchdunes/spa/> from <GET http://www.mytrilogylife.com/monarchdunes/spa/>
DEBUG: Redirecting (301) to <GET https://www.boonhotels.com/> from <GET http://www.boonhotels.com/>
DEBUG: Crawled (200) <GET https://www.silveradoresort.com/napa-valley-spa?utm_medium=organic&utm_source=google&utm_campaign=spabusinesslisting> (referer: None)
DEBUG: Crawled (200) <GET https://the-spring.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.ritzcarlton.com/en/hotels/california/santa-barbara/spa?scid=bb1a189a-fec3-4d19-a255-54ba596febe2> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.wispausa.com/> from <GET http://www.wispausa.com/>
DEBUG: Crawled (200) <GET https://www.aubergeresorts.com/solage/wellness/spa/?utm_source=google%20my%20business&utm_medium=listing&utm_campaign=visit%20website> (referer: None)
DEBUG: Retrying <GET https://www.hyatt.com/robots.txt> (failed 1 times): 429 Unknown Status
DEBUG: Crawled (200) <GET https://www.mytrilogylife.com/monarchdunes/spa/> (referer: None)
DEBUG: Crawled (200) <GET https://glenivy.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.southcoastwinery.com/spa/> (referer: None)
DEBUG: Retrying <GET https://www.hyatt.com/robots.txt> (failed 2 times): 429 Unknown Status
DEBUG: Crawled (200) <GET https://www.laquintaresort.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.chaminade.com/?utm_source=google&utm_medium=organic&utm_campaign=gmb> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.glenivy.com/> from <GET http://www.glenivy.com/>
DEBUG: Crawled (200) <GET https://www.terranea.com/robots.txt> (referer: None)
ERROR: Gave up retrying <GET https://www.hyatt.com/robots.txt> (failed 3 times): 429 Unknown Status
DEBUG: Crawled (429) <GET https://www.hyatt.com/robots.txt> (referer: None)
DEBUG: Rule at line 1 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://aubergeresorts.com/solage/contact-us/> (referer: https://www.aubergeresorts.com/)
DEBUG: Redirecting (301) to <GET https://boonhotels.com/> from <GET https://www.boonhotels.com/>
DEBUG: Redirecting (301) to <GET https://www.laquintaresort.com/> from <GET http://www.laquintaresort.com/>
DEBUG: Crawled (200) <GET https://www.silveradoresort.com/contact-us> (referer: https://www.silveradoresort.com/napa-valley-spa?utm_medium=organic&utm_source=google&utm_campaign=spabusinesslisting)
DEBUG: Crawled (200) <GET https://boonhotels.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.urbanretreatspa.com/robots.txt> (referer: None)
DEBUG: Retrying <GET https://www.hyatt.com/en-US/spas/Pacific-Waters-Spa/home.html> (failed 1 times): 429 Unknown Status
DEBUG: Crawled (200) <GET https://www.terranea.com/offers?utm_source=google-knowledge-graph&utm_medium=organic&utm_campaign=my_business_page> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.evo-spa.com/robots.txt> from <GET http://www.evo-spa.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.urbanretreatspa.com/> (referer: None)
DEBUG: Crawled (200) <GET https://ranchovalencia.com/contact-us/> (referer: https://ranchovalencia.com/)
DEBUG: Redirecting (301) to <GET http://beverlyhotsprings.com/robots.txt> from <GET http://www.beverlyhotsprings.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://glenivy.com/> from <GET https://www.glenivy.com/>
DEBUG: Crawled (200) <GET https://thesisleyspa.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://boonhotels.com/> (referer: None)
DEBUG: Crawled (200) <GET https://the-spring.com/> (referer: None)
DEBUG: Retrying <GET https://www.hyatt.com/en-US/spas/Pacific-Waters-Spa/home.html> (failed 2 times): 429 Unknown Status
DEBUG: Crawled (200) <GET https://www.evo-spa.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.sycamoresprings.com/robots.txt> from <GET http://www.sycamoresprings.com/robots.txt>
DEBUG: Crawled (200) <GET https://thesisleyspa.com/los-angeles/> (referer: None)
DEBUG: Redirecting (301) to <GET https://banyancay.com/robots.txt> from <GET https://www.banyancay.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://www.evo-spa.com/> from <GET http://www.evo-spa.com/>
ERROR: Gave up retrying <GET https://www.hyatt.com/en-US/spas/Pacific-Waters-Spa/home.html> (failed 3 times): 429 Unknown Status
DEBUG: Crawled (429) <GET https://www.hyatt.com/en-US/spas/Pacific-Waters-Spa/home.html> (referer: None)
DEBUG: Redirecting (301) to <GET https://wispausa.com/> from <GET https://www.wispausa.com/>
INFO: Ignoring response <429 https://www.hyatt.com/en-US/spas/Pacific-Waters-Spa/home.html>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://www.sycamoresprings.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://davidrubensteinforum.uchicago.edu/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://beverlyhotsprings.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.evo-spa.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://sweetwaterspa.com/robots.txt> from <GET http://www.sweetwaterspa.com/robots.txt>
DEBUG: Crawled (200) <GET https://glenivy.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.sycamoresprings.com/> from <GET http://www.sycamoresprings.com/>
DEBUG: Crawled (200) <GET https://boonhotels.com/contact/> (referer: https://boonhotels.com/)
DEBUG: Redirecting (301) to <GET http://beverlyhotsprings.com/> from <GET http://www.beverlyhotsprings.com/>
DEBUG: Crawled (200) <GET https://davidrubensteinforum.uchicago.edu/contact/> (referer: https://www.chaminade.com/?utm_source=google&utm_medium=organic&utm_campaign=gmb)
DEBUG: Crawled (200) <GET https://banyancay.com/robots.txt> (referer: None)
DEBUG: Rule at line 2 without any user agent to enforce it on.
DEBUG: Rule at line 7 without any user agent to enforce it on.
DEBUG: Rule at line 9 without any user agent to enforce it on.
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Rule at line 11 without any user agent to enforce it on.
DEBUG: Rule at line 15 without any user agent to enforce it on.
DEBUG: Rule at line 16 without any user agent to enforce it on.
DEBUG: Rule at line 17 without any user agent to enforce it on.
DEBUG: Rule at line 18 without any user agent to enforce it on.
DEBUG: Rule at line 22 without any user agent to enforce it on.
DEBUG: Rule at line 23 without any user agent to enforce it on.
DEBUG: Rule at line 24 without any user agent to enforce it on.
DEBUG: Rule at line 25 without any user agent to enforce it on.
DEBUG: Rule at line 29 without any user agent to enforce it on.
DEBUG: Rule at line 30 without any user agent to enforce it on.
DEBUG: Rule at line 31 without any user agent to enforce it on.
DEBUG: Rule at line 32 without any user agent to enforce it on.
DEBUG: Rule at line 33 without any user agent to enforce it on.
DEBUG: Rule at line 37 without any user agent to enforce it on.
DEBUG: Rule at line 38 without any user agent to enforce it on.
DEBUG: Rule at line 39 without any user agent to enforce it on.
DEBUG: Rule at line 40 without any user agent to enforce it on.
DEBUG: Rule at line 44 without any user agent to enforce it on.
DEBUG: Rule at line 48 without any user agent to enforce it on.
DEBUG: Rule at line 49 without any user agent to enforce it on.
DEBUG: Rule at line 50 without any user agent to enforce it on.
DEBUG: Rule at line 51 without any user agent to enforce it on.
DEBUG: Rule at line 52 without any user agent to enforce it on.
DEBUG: Rule at line 53 without any user agent to enforce it on.
DEBUG: Rule at line 54 without any user agent to enforce it on.
DEBUG: Rule at line 55 without any user agent to enforce it on.
DEBUG: Rule at line 56 without any user agent to enforce it on.
DEBUG: Rule at line 57 without any user agent to enforce it on.
DEBUG: Rule at line 58 without any user agent to enforce it on.
DEBUG: Rule at line 59 without any user agent to enforce it on.
DEBUG: Rule at line 60 without any user agent to enforce it on.
DEBUG: Rule at line 61 without any user agent to enforce it on.
DEBUG: Rule at line 62 without any user agent to enforce it on.
DEBUG: Rule at line 63 without any user agent to enforce it on.
DEBUG: Rule at line 64 without any user agent to enforce it on.
DEBUG: Rule at line 65 without any user agent to enforce it on.
DEBUG: Rule at line 66 without any user agent to enforce it on.
DEBUG: Rule at line 70 without any user agent to enforce it on.
DEBUG: Rule at line 72 without any user agent to enforce it on.
DEBUG: Rule at line 80 without any user agent to enforce it on.
DEBUG: Rule at line 87 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://wispausa.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://thesisleyspa.com/contact-us/> (referer: https://thesisleyspa.com/los-angeles/)
DEBUG: Redirecting (301) to <GET https://www.ranchovalencia.com/robots.txt> from <GET http://www.ranchovalencia.com/robots.txt>
DEBUG: Crawled (200) <GET https://azurepalmhotsprings.com/robots.txt> (referer: None)
DEBUG: Crawled (404) <GET https://sweetwaterspa.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.constantcontact.com/robots.txt> (referer: None)
DEBUG: Forbidden by robots.txt: <GET https://www.constantcontact.com/legal/service-provider>
DEBUG: Crawled (200) <GET http://beverlyhotsprings.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://sweetwaterspa.com/> from <GET http://www.sweetwaterspa.com/>
DEBUG: Crawled (200) <GET https://www.sycamoresprings.com/> (referer: None)
DEBUG: Crawled (200) <GET https://wispausa.com/> (referer: None)
DEBUG: Crawled (404) <GET https://sweetwaterspa.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://glenivy.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.ranchovalencia.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://the-spring.com/contact/> (referer: https://the-spring.com/)
DEBUG: Redirecting (301) to <GET http://wecarespa.com/robots.txt> from <GET http://www.wecarespa.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.canyonranch.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.ranchovalencia.com/> from <GET http://www.ranchovalencia.com/>
DEBUG: Scraped from <200 https://the-spring.com/contact/>
{'emails': ['info@the-spring.com'],
 'facebook': 'https://www.facebook.com/thespringresortandspa/',
 'instagram': 'https://www.instagram.com/thespringresort/',
 'linkedin': '',
 'twitter': ''}
DEBUG: Redirecting (301) to <GET https://www.lapeauspafresno.com/robots.txt> from <GET http://www.lapeauspafresno.com/robots.txt>
DEBUG: Crawled (200) <GET https://azurepalmhotsprings.com/> (referer: None)
DEBUG: Crawled (200) <GET https://wispausa.com/contact/> (referer: https://wispausa.com/)
DEBUG: Scraped from <200 https://wispausa.com/contact/>
{'emails': ['info@wispausa.com'],
 'facebook': 'https://www.facebook.com/wispa.usa',
 'instagram': 'https://www.instagram.com/wispa_usa/',
 'linkedin': '',
 'twitter': 'https://twitter.com/WiSpa_USA'}
DEBUG: Crawled (200) <GET https://www.canyonranch.com/woodside/northern-california-retreat/?utm_source=googlemybusiness&utm_medium=organic&utm_campaign=woodsidegmb&utm_term=googlemybusiness> (referer: None)
DEBUG: Crawled (200) <GET http://beverlyhotsprings.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://banyancay.com/contact-us.html> from <GET https://www.banyancay.com/contact-us.html>
DEBUG: Crawled (200) <GET https://www.indianspringscalistoga.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://wecarespa.com/robots.txt> (referer: None)
DEBUG: Rule at line 1 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://www.lapeauspafresno.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://banyancay.com/robots.txt> (referer: None)
DEBUG: Rule at line 2 without any user agent to enforce it on.
DEBUG: Rule at line 7 without any user agent to enforce it on.
DEBUG: Rule at line 9 without any user agent to enforce it on.
DEBUG: Rule at line 10 without any user agent to enforce it on.
DEBUG: Rule at line 11 without any user agent to enforce it on.
DEBUG: Rule at line 15 without any user agent to enforce it on.
DEBUG: Rule at line 16 without any user agent to enforce it on.
DEBUG: Rule at line 17 without any user agent to enforce it on.
DEBUG: Rule at line 18 without any user agent to enforce it on.
DEBUG: Rule at line 22 without any user agent to enforce it on.
DEBUG: Rule at line 23 without any user agent to enforce it on.
DEBUG: Rule at line 24 without any user agent to enforce it on.
DEBUG: Rule at line 25 without any user agent to enforce it on.
DEBUG: Rule at line 29 without any user agent to enforce it on.
DEBUG: Rule at line 30 without any user agent to enforce it on.
DEBUG: Rule at line 31 without any user agent to enforce it on.
DEBUG: Rule at line 32 without any user agent to enforce it on.
DEBUG: Rule at line 33 without any user agent to enforce it on.
DEBUG: Rule at line 37 without any user agent to enforce it on.
DEBUG: Rule at line 38 without any user agent to enforce it on.
DEBUG: Rule at line 39 without any user agent to enforce it on.
DEBUG: Rule at line 40 without any user agent to enforce it on.
DEBUG: Rule at line 44 without any user agent to enforce it on.
DEBUG: Rule at line 48 without any user agent to enforce it on.
DEBUG: Rule at line 49 without any user agent to enforce it on.
DEBUG: Rule at line 50 without any user agent to enforce it on.
DEBUG: Rule at line 51 without any user agent to enforce it on.
DEBUG: Rule at line 52 without any user agent to enforce it on.
DEBUG: Rule at line 53 without any user agent to enforce it on.
DEBUG: Rule at line 54 without any user agent to enforce it on.
DEBUG: Rule at line 55 without any user agent to enforce it on.
DEBUG: Rule at line 56 without any user agent to enforce it on.
DEBUG: Rule at line 57 without any user agent to enforce it on.
DEBUG: Rule at line 58 without any user agent to enforce it on.
DEBUG: Rule at line 59 without any user agent to enforce it on.
DEBUG: Rule at line 60 without any user agent to enforce it on.
DEBUG: Rule at line 61 without any user agent to enforce it on.
DEBUG: Rule at line 62 without any user agent to enforce it on.
DEBUG: Rule at line 63 without any user agent to enforce it on.
DEBUG: Rule at line 64 without any user agent to enforce it on.
DEBUG: Rule at line 65 without any user agent to enforce it on.
DEBUG: Rule at line 66 without any user agent to enforce it on.
DEBUG: Rule at line 70 without any user agent to enforce it on.
DEBUG: Rule at line 72 without any user agent to enforce it on.
DEBUG: Rule at line 80 without any user agent to enforce it on.
DEBUG: Rule at line 87 without any user agent to enforce it on.
DEBUG: Redirecting (301) to <GET https://www.lapeauspafresno.com/> from <GET http://www.lapeauspafresno.com/>
DEBUG: Redirecting (301) to <GET https://lagunacanyonspa.com/robots.txt> from <GET http://www.lagunacanyonspa.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://banyancay.com/> from <GET https://banyancay.com/contact-us.html>
DEBUG: Crawled (200) <GET https://lagunacanyonspa.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://paradisepoint.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://lagunacanyonspa.com/> from <GET http://www.lagunacanyonspa.com/>
DEBUG: Redirecting (301) to <GET https://wecarespa.com/> from <GET http://www.wecarespa.com/>
DEBUG: Crawled (200) <GET http://www.carnerosresort.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.indianspringscalistoga.com/?utm_source=google&utm_medium=organic&ut0> (referer: None)
DEBUG: Crawled (200) <GET https://banyancay.com/> (referer: https://www.chaminade.com/?utm_source=google&utm_medium=organic&utm_campaign=gmb)
DEBUG: Redirecting (301) to <GET https://ranchovalencia.com/> from <GET https://www.ranchovalencia.com/>
DEBUG: Crawled (200) <GET https://lagunacanyonspa.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://beverlyhotsprings.com/contact/> (referer: http://beverlyhotsprings.com/)
DEBUG: Crawled (200) <GET https://lagunacanyonspa.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.lapeauspafresno.com/> (referer: None)
DEBUG: Crawled (200) <GET https://azurepalmhotsprings.com/contact-us/> (referer: https://azurepalmhotsprings.com/)
DEBUG: Scraped from <200 http://beverlyhotsprings.com/contact/>
{'emails': ['beverlyhotsprings@gmail.com'],
 'facebook': 'https://www.facebook.com/beverlyhotsprings',
 'instagram': '',
 'linkedin': '',
 'twitter': 'https://twitter.com/BHSdayspa'}
DEBUG: Crawled (200) <GET https://www.estanciadayspa.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://amenitiesspa.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://paradisepoint.com/california-resort-spa/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.carnerosresort.com/> from <GET http://www.carnerosresort.com/>
DEBUG: Crawled (200) <GET https://www.indianspringscalistoga.com/?utm_source=google&utm_medium=organic&utm_campaign=business-listing> (referer: None)
DEBUG: Scraped from <200 https://azurepalmhotsprings.com/contact-us/>
{'emails': ['info@azurepalm.com'],
 'facebook': 'https://www.facebook.com/azurepalmhotsprings',
 'instagram': 'https://www.instagram.com/azurepalmhotsprings/',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET https://www.laquintaresort.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.estanciadayspa.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.cavallopoint.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.cavallopoint.com/spa?utm_source=google-gmb&utm_medium=organic&utm_campaign=gmb> (referer: None)
DEBUG: Crawled (200) <GET https://carmelvalleyranch.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://theravenspa.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.indianspringscalistoga.com/contact-us> (referer: https://www.indianspringscalistoga.com/?utm_source=google&utm_medium=organic&ut0)
DEBUG: Redirecting (301) to <GET https://www.sweetwaterspa.com/> from <GET https://sweetwaterspa.com/>
DEBUG: Crawled (200) <GET https://paradisepoint.com/contact-us/> (referer: https://paradisepoint.com/california-resort-spa/)
DEBUG: Crawled (200) <GET https://carmelvalleyranch.com/spa/spa-aiyana/> (referer: None)
DEBUG: Crawled (200) <GET https://wecarespa.com/robots.txt> (referer: None)
DEBUG: Rule at line 1 without any user agent to enforce it on.
DEBUG: Redirecting (301) to <GET http://carnerosresort.com/> from <GET https://www.carnerosresort.com/>
DEBUG: Redirecting (301) to <GET https://www.thebluedoorhanford.com/robots.txt> from <GET http://www.thebluedoorhanford.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://www.trilogyspa.com/robots.txt> from <GET http://www.trilogyspa.com/robots.txt>
DEBUG: Crawled (200) <GET http://carnerosresort.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://theravenspa.com/> (referer: None)
DEBUG: Crawled (200) <GET https://carmelvalleyranch.com/contact/> (referer: https://carmelvalleyranch.com/spa/spa-aiyana/)
DEBUG: Crawled (200) <GET https://amenitiesspa.com/> (referer: None)
DEBUG: Crawled (404) <GET https://azurepalmhotsprings.com/dev/contact-us/> (referer: https://azurepalmhotsprings.com/)
DEBUG: Crawled (200) <GET https://www.sweetwaterspa.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.trilogyspa.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.thebluedoorhanford.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://carnerosresort.com/> from <GET http://carnerosresort.com/>
DEBUG: Redirecting (301) to <GET https://www.trilogyspa.com/> from <GET http://www.trilogyspa.com/>
DEBUG: Redirecting (301) to <GET https://doubleeagle.com/robots.txt> from <GET http://doubleeagle.com/robots.txt>
DEBUG: Crawled (200) <GET https://thehealingcornerca.com/robots.txt> (referer: None)
DEBUG: Crawled (404) <GET https://www.ranchobernardoinn.com/robots.txt> (referer: None)
DEBUG: Rule at line 14 without any user agent to enforce it on.
DEBUG: Rule at line 15 without any user agent to enforce it on.
DEBUG: Rule at line 56 without any user agent to enforce it on.
DEBUG: Rule at line 57 without any user agent to enforce it on.
DEBUG: Rule at line 61 without any user agent to enforce it on.
DEBUG: Rule at line 62 without any user agent to enforce it on.
DEBUG: Rule at line 63 without any user agent to enforce it on.
DEBUG: Rule at line 67 without any user agent to enforce it on.
DEBUG: Rule at line 71 without any user agent to enforce it on.
DEBUG: Rule at line 73 without any user agent to enforce it on.
DEBUG: Rule at line 74 without any user agent to enforce it on.
DEBUG: Rule at line 83 without any user agent to enforce it on.
DEBUG: Rule at line 88 without any user agent to enforce it on.
DEBUG: Rule at line 101 without any user agent to enforce it on.
DEBUG: Rule at line 103 without any user agent to enforce it on.
DEBUG: Rule at line 104 without any user agent to enforce it on.
DEBUG: Rule at line 123 without any user agent to enforce it on.
DEBUG: Rule at line 141 without any user agent to enforce it on.
DEBUG: Rule at line 144 without any user agent to enforce it on.
DEBUG: Rule at line 145 without any user agent to enforce it on.
DEBUG: Rule at line 146 without any user agent to enforce it on.
DEBUG: Rule at line 152 without any user agent to enforce it on.
DEBUG: Rule at line 173 without any user agent to enforce it on.
DEBUG: Rule at line 241 without any user agent to enforce it on.
DEBUG: Rule at line 242 without any user agent to enforce it on.
DEBUG: Rule at line 249 without any user agent to enforce it on.
DEBUG: Rule at line 260 without any user agent to enforce it on.
DEBUG: Rule at line 265 without any user agent to enforce it on.
DEBUG: Rule at line 268 without any user agent to enforce it on.
DEBUG: Rule at line 271 without any user agent to enforce it on.
DEBUG: Rule at line 274 without any user agent to enforce it on.
DEBUG: Rule at line 277 without any user agent to enforce it on.
DEBUG: Rule at line 280 without any user agent to enforce it on.
DEBUG: Rule at line 283 without any user agent to enforce it on.
DEBUG: Rule at line 288 without any user agent to enforce it on.
DEBUG: Rule at line 293 without any user agent to enforce it on.
DEBUG: Rule at line 296 without any user agent to enforce it on.
DEBUG: Rule at line 299 without any user agent to enforce it on.
DEBUG: Rule at line 302 without any user agent to enforce it on.
DEBUG: Rule at line 305 without any user agent to enforce it on.
DEBUG: Rule at line 308 without any user agent to enforce it on.
DEBUG: Rule at line 311 without any user agent to enforce it on.
DEBUG: Rule at line 314 without any user agent to enforce it on.
DEBUG: Rule at line 320 without any user agent to enforce it on.
DEBUG: Rule at line 325 without any user agent to enforce it on.
DEBUG: Rule at line 330 without any user agent to enforce it on.
DEBUG: Rule at line 333 without any user agent to enforce it on.
DEBUG: Rule at line 336 without any user agent to enforce it on.
DEBUG: Rule at line 339 without any user agent to enforce it on.
DEBUG: Rule at line 342 without any user agent to enforce it on.
DEBUG: Rule at line 345 without any user agent to enforce it on.
DEBUG: Rule at line 348 without any user agent to enforce it on.
DEBUG: Rule at line 353 without any user agent to enforce it on.
DEBUG: Rule at line 358 without any user agent to enforce it on.
DEBUG: Rule at line 365 without any user agent to enforce it on.
DEBUG: Rule at line 368 without any user agent to enforce it on.
DEBUG: Rule at line 371 without any user agent to enforce it on.
DEBUG: Rule at line 374 without any user agent to enforce it on.
DEBUG: Rule at line 377 without any user agent to enforce it on.
DEBUG: Rule at line 380 without any user agent to enforce it on.
DEBUG: Rule at line 385 without any user agent to enforce it on.
DEBUG: Rule at line 390 without any user agent to enforce it on.
DEBUG: Rule at line 395 without any user agent to enforce it on.
DEBUG: Rule at line 398 without any user agent to enforce it on.
DEBUG: Rule at line 401 without any user agent to enforce it on.
DEBUG: Rule at line 404 without any user agent to enforce it on.
DEBUG: Rule at line 407 without any user agent to enforce it on.
DEBUG: Rule at line 410 without any user agent to enforce it on.
DEBUG: Rule at line 413 without any user agent to enforce it on.
DEBUG: Rule at line 416 without any user agent to enforce it on.
DEBUG: Rule at line 419 without any user agent to enforce it on.
DEBUG: Rule at line 422 without any user agent to enforce it on.
DEBUG: Rule at line 425 without any user agent to enforce it on.
DEBUG: Rule at line 430 without any user agent to enforce it on.
DEBUG: Rule at line 435 without any user agent to enforce it on.
DEBUG: Rule at line 438 without any user agent to enforce it on.
DEBUG: Rule at line 441 without any user agent to enforce it on.
DEBUG: Rule at line 444 without any user agent to enforce it on.
DEBUG: Rule at line 450 without any user agent to enforce it on.
DEBUG: Rule at line 453 without any user agent to enforce it on.
DEBUG: Rule at line 458 without any user agent to enforce it on.
DEBUG: Rule at line 463 without any user agent to enforce it on.
DEBUG: Rule at line 466 without any user agent to enforce it on.
DEBUG: Rule at line 469 without any user agent to enforce it on.
DEBUG: Rule at line 472 without any user agent to enforce it on.
DEBUG: Rule at line 475 without any user agent to enforce it on.
DEBUG: Rule at line 478 without any user agent to enforce it on.
DEBUG: Rule at line 490 without any user agent to enforce it on.
DEBUG: Rule at line 491 without any user agent to enforce it on.
DEBUG: Rule at line 511 without any user agent to enforce it on.
DEBUG: Rule at line 543 without any user agent to enforce it on.
DEBUG: Rule at line 545 without any user agent to enforce it on.
DEBUG: Rule at line 550 without any user agent to enforce it on.
DEBUG: Rule at line 551 without any user agent to enforce it on.
DEBUG: Rule at line 552 without any user agent to enforce it on.
DEBUG: Rule at line 620 without any user agent to enforce it on.
DEBUG: Rule at line 627 without any user agent to enforce it on.
DEBUG: Rule at line 629 without any user agent to enforce it on.
DEBUG: Rule at line 633 without any user agent to enforce it on.
DEBUG: Rule at line 637 without any user agent to enforce it on.
DEBUG: Rule at line 641 without any user agent to enforce it on.
DEBUG: Rule at line 642 without any user agent to enforce it on.
DEBUG: Rule at line 643 without any user agent to enforce it on.
DEBUG: Rule at line 644 without any user agent to enforce it on.
DEBUG: Rule at line 658 without any user agent to enforce it on.
DEBUG: Rule at line 662 without any user agent to enforce it on.
DEBUG: Rule at line 663 without any user agent to enforce it on.
DEBUG: Rule at line 664 without any user agent to enforce it on.
DEBUG: Rule at line 665 without any user agent to enforce it on.
DEBUG: Rule at line 666 without any user agent to enforce it on.
DEBUG: Rule at line 684 without any user agent to enforce it on.
DEBUG: Rule at line 685 without any user agent to enforce it on.
DEBUG: Rule at line 686 without any user agent to enforce it on.
DEBUG: Rule at line 693 without any user agent to enforce it on.
DEBUG: Rule at line 694 without any user agent to enforce it on.
DEBUG: Rule at line 698 without any user agent to enforce it on.
DEBUG: Rule at line 699 without any user agent to enforce it on.
DEBUG: Rule at line 700 without any user agent to enforce it on.
DEBUG: Rule at line 701 without any user agent to enforce it on.
DEBUG: Rule at line 702 without any user agent to enforce it on.
DEBUG: Rule at line 703 without any user agent to enforce it on.
DEBUG: Rule at line 705 without any user agent to enforce it on.
DEBUG: Rule at line 707 without any user agent to enforce it on.
DEBUG: Rule at line 747 without any user agent to enforce it on.
DEBUG: Rule at line 754 without any user agent to enforce it on.
DEBUG: Rule at line 755 without any user agent to enforce it on.
DEBUG: Rule at line 780 without any user agent to enforce it on.
DEBUG: Rule at line 787 without any user agent to enforce it on.
DEBUG: Rule at line 788 without any user agent to enforce it on.
DEBUG: Rule at line 818 without any user agent to enforce it on.
DEBUG: Rule at line 821 without any user agent to enforce it on.
DEBUG: Rule at line 828 without any user agent to enforce it on.
DEBUG: Rule at line 829 without any user agent to enforce it on.
DEBUG: Rule at line 834 without any user agent to enforce it on.
DEBUG: Rule at line 836 without any user agent to enforce it on.
DEBUG: Rule at line 839 without any user agent to enforce it on.
DEBUG: Rule at line 840 without any user agent to enforce it on.
DEBUG: Rule at line 841 without any user agent to enforce it on.
DEBUG: Rule at line 894 without any user agent to enforce it on.
DEBUG: Rule at line 895 without any user agent to enforce it on.
DEBUG: Rule at line 896 without any user agent to enforce it on.
DEBUG: Rule at line 897 without any user agent to enforce it on.
DEBUG: Rule at line 898 without any user agent to enforce it on.
DEBUG: Rule at line 900 without any user agent to enforce it on.
DEBUG: Rule at line 905 without any user agent to enforce it on.
DEBUG: Rule at line 906 without any user agent to enforce it on.
DEBUG: Rule at line 907 without any user agent to enforce it on.
DEBUG: Rule at line 908 without any user agent to enforce it on.
DEBUG: Rule at line 913 without any user agent to enforce it on.
DEBUG: Rule at line 932 without any user agent to enforce it on.
DEBUG: Rule at line 933 without any user agent to enforce it on.
DEBUG: Rule at line 944 without any user agent to enforce it on.
DEBUG: Rule at line 953 without any user agent to enforce it on.
DEBUG: Rule at line 956 without any user agent to enforce it on.
DEBUG: Rule at line 961 without any user agent to enforce it on.
DEBUG: Rule at line 970 without any user agent to enforce it on.
DEBUG: Rule at line 986 without any user agent to enforce it on.
DEBUG: Rule at line 987 without any user agent to enforce it on.
DEBUG: Rule at line 988 without any user agent to enforce it on.
DEBUG: Rule at line 989 without any user agent to enforce it on.
DEBUG: Rule at line 995 without any user agent to enforce it on.
DEBUG: Redirecting (301) to <GET https://www.thebluedoorhanford.com/> from <GET http://www.thebluedoorhanford.com/>
DEBUG: Crawled (200) <GET https://wecarespa.com/> (referer: None)
INFO: Ignoring response <404 https://azurepalmhotsprings.com/dev/contact-us/>: HTTP status code is not handled or not allowed
DEBUG: Redirecting (301) to <GET https://www.missioninn.com/robots.txt> from <GET http://www.missioninn.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.thebluedoorhanford.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.missioninn.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.trilogyspa.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.missioninn.com/> from <GET http://www.missioninn.com/>
DEBUG: Crawled (200) <GET https://www.missioninn.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.paseahotel.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.miraclesprings.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.teahousespa.com/robots.txt> from <GET http://www.teahousespa.com/robots.txt>
DEBUG: Redirecting (301) to <GET http://harbin.org/robots.txt> from <GET http://www.harbin.org/robots.txt>
DEBUG: Crawled (200) <GET https://www.sweetwaterspa.com/contact-us/> (referer: https://www.sweetwaterspa.com/)
DEBUG: Scraped from <200 https://www.sweetwaterspa.com/contact-us/>
{'emails': ['lodging@sweetwaterspa.com'],
 'facebook': 'https://www.facebook.com/SweetwaterInnandSpa/',
 'instagram': 'https://www.instagram.com/sweetwaterinnspa/',
 'linkedin': '',
 'twitter': 'https://twitter.com/SweetwaterInnCA'}
DEBUG: Crawled (200) <GET https://carnerosresort.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.teahousespa.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.paseahotel.com/?utm_medium=organic&utm_source=google&utm_campaign=business-listing> (referer: None)
DEBUG: Redirecting (301) to <GET https://harbin.org/robots.txt> from <GET http://harbin.org/robots.txt>
DEBUG: Redirecting (301) to <GET https://www.bardessono.com/robots.txt> from <GET https://bardessono.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.thelandingtahoe.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://wecarespa.com/contact-us/> from <GET https://wecarespa.com/contact/>
DEBUG: Redirecting (301) to <GET https://www.teahousespa.com/> from <GET http://www.teahousespa.com/>
DEBUG: Crawled (200) <GET https://www.montagehotels.com/robots.txt> (referer: None)
DEBUG: Crawled (403) <GET https://www.montagehotels.com/lagunabeach/> (referer: None)
DEBUG: Crawled (200) <GET https://www.teahousespa.com/> (referer: None)
INFO: Ignoring response <403 https://www.montagehotels.com/lagunabeach/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://www.miraclesprings.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.doubleeagle.com/robots.txt> from <GET https://doubleeagle.com/robots.txt>
DEBUG: Crawled (200) <GET https://wecarespa.com/contact-us/> (referer: https://wecarespa.com/)
DEBUG: Crawled (200) <GET https://www.ranchobernardoinn.com/spa/overview?utm_source=gmb&utm_medium=yext> (referer: None)
DEBUG: Scraped from <200 https://wecarespa.com/contact-us/>
{'emails': ['info@wecarespa.com'],
 'facebook': 'https://www.facebook.com/WeCareSpa',
 'instagram': 'https://www.instagram.com/wecarespaca/',
 'linkedin': '',
 'twitter': 'https://twitter.com/wecarespa'}
DEBUG: Crawled (200) <GET https://www.paseahotel.com/contact/press-room> (referer: https://www.paseahotel.com/?utm_medium=organic&utm_source=google&utm_campaign=business-listing)
DEBUG: Crawled (200) <GET https://www.estancialajolla.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://harbin.org/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.bardessono.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.bardessono.com/> from <GET https://bardessono.com/>
DEBUG: Redirecting (301) to <GET https://mysheerbliss.com/robots.txt> from <GET https://www.mysheerbliss.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.paseahotel.com/contact/blogs> (referer: https://www.paseahotel.com/?utm_medium=organic&utm_source=google&utm_campaign=business-listing)
DEBUG: Redirecting (301) to <GET https://harbin.org/> from <GET http://www.harbin.org/>
DEBUG: Crawled (200) <GET https://mysheerbliss.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.silveradoresort.com/?utm_source=google&utm_medium=organic&utm_campaign=business-listing> (referer: None)
DEBUG: Crawled (200) <GET https://www.estancialajolla.com/spa/?utm_source=gmb-spa&utm_medium=organic&utm_campaign=gmb> (referer: None)
DEBUG: Crawled (200) <GET https://www.bardessono.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://mysheerbliss.com/> from <GET https://www.mysheerbliss.com/>
DEBUG: Crawled (200) <GET https://harbin.org/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://mysheerbliss.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.paseahotel.com/contact> (referer: https://www.paseahotel.com/?utm_medium=organic&utm_source=google&utm_campaign=business-listing)
DEBUG: Crawled (200) <GET https://www.romanspahotsprings.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.miraclesprings.com/contact/> (referer: https://www.miraclesprings.com/)
DEBUG: Retrying <GET https://www.shuttersonthebeach.com/robots.txt> (failed 1 times): 502 Bad Gateway
DEBUG: Crawled (200) <GET https://mysheerbliss.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.romanspahotsprings.com/?utm_source=google%20my%20business&utm_medium=listing&utm_campaign=visit%20website> (referer: None)
DEBUG: Crawled (200) <GET https://www.bardessono.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.estancialajolla.com/contact/> (referer: https://www.estancialajolla.com/spa/?utm_source=gmb-spa&utm_medium=organic&utm_campaign=gmb)
DEBUG: Crawled (200) <GET https://thehealingcornerca.com/> (referer: None)
DEBUG: Crawled (200) <GET https://milkandhoneyspa.com/robots.txt> (referer: None)
DEBUG: Retrying <GET https://www.shuttersonthebeach.com/robots.txt> (failed 2 times): 502 Bad Gateway
DEBUG: Crawled (200) <GET https://www.thelandingtahoe.com/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.senspa.com/robots.txt> from <GET http://www.senspa.com/robots.txt>
DEBUG: Crawled (200) <GET http://metropolissalonspa.com/robots.txt> (referer: None)
DEBUG: Forbidden by robots.txt: <GET http://metropolissalonspa.com/>
ERROR: Gave up retrying <GET https://www.shuttersonthebeach.com/robots.txt> (failed 3 times): 502 Bad Gateway
DEBUG: Crawled (502) <GET https://www.shuttersonthebeach.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://petitespa.net/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.doubleeagle.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://harbin.org/> (referer: None)
DEBUG: Crawled (200) <GET https://www.romanspahotsprings.com/contact> (referer: https://www.romanspahotsprings.com/?utm_source=google%20my%20business&utm_medium=listing&utm_campaign=visit%20website)
DEBUG: Crawled (200) <GET https://petitespa.net/> (referer: None)
DEBUG: Retrying <GET https://www.shuttersonthebeach.com/spa/one-spa> (failed 1 times): 502 Bad Gateway
DEBUG: Redirecting (301) to <GET https://oceanohalfmoonbay.com/robots.txt> from <GET http://www.oceanohalfmoonbay.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://mysheerbliss.com/robots.txt> from <GET https://sheer-bliss-organic-spa.myshopify.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.bardessono.com/contact.htm> (referer: https://www.bardessono.com/)
DEBUG: Scraped from <200 https://www.romanspahotsprings.com/contact>
{'emails': ['Reservations75@Romanspahotsprings.com'],
 'facebook': 'https://www.facebook.com/romanspahotsprings',
 'instagram': 'https://www.instagram.com/romanspahotspringsresort/?ref=badge',
 'linkedin': '',
 'twitter': 'https://twitter.com/romansparesort'}
DEBUG: Crawled (404) <GET https://www.ventanabigsur.com/robots.txt> (referer: None)
DEBUG: Rule at line 20 without any user agent to enforce it on.
DEBUG: Rule at line 43 without any user agent to enforce it on.
DEBUG: Rule at line 54 without any user agent to enforce it on.
DEBUG: Rule at line 55 without any user agent to enforce it on.
DEBUG: Crawled (200) <GET https://mysheerbliss.com/robots.txt> (referer: None)
DEBUG: Retrying <GET https://www.shuttersonthebeach.com/spa/one-spa> (failed 2 times): 502 Bad Gateway
DEBUG: Crawled (200) <GET https://oceanohalfmoonbay.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://mysheerbliss.com/pages/contact-us-spa-policy> from <GET https://sheer-bliss-organic-spa.myshopify.com/pages/contact-us-spa-policy>
DEBUG: Redirecting (301) to <GET https://oceanohalfmoonbay.com/> from <GET http://www.oceanohalfmoonbay.com/>
ERROR: Gave up retrying <GET https://www.shuttersonthebeach.com/spa/one-spa> (failed 3 times): 502 Bad Gateway
DEBUG: Crawled (502) <GET https://www.shuttersonthebeach.com/spa/one-spa> (referer: None)
DEBUG: Crawled (200) <GET https://oceanohalfmoonbay.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.rushcreeklodge.com/robots.txt> from <GET http://www.rushcreeklodge.com/robots.txt>
DEBUG: Crawled (200) <GET https://oceanohalfmoonbay.com/> (referer: None)
INFO: Ignoring response <502 https://www.shuttersonthebeach.com/spa/one-spa>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://www.senspa.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://milkandhoneyspa.com/?utm_source=google&utm_medium=organic&utm_campaign=gmb-listing&utm_content=brentwood> (referer: None)
DEBUG: Crawled (200) <GET https://mysheerbliss.com/pages/contact-us-spa-policy> (referer: https://mysheerbliss.com/)
DEBUG: Crawled (451) <GET https://www.morongocasinoresort.com/robots.txt> (referer: None)
DEBUG: Rule at line 3 without any user agent to enforce it on.
DEBUG: Rule at line 4 without any user agent to enforce it on.
DEBUG: Rule at line 6 without any user agent to enforce it on.
DEBUG: Rule at line 27 without any user agent to enforce it on.
DEBUG: Rule at line 28 without any user agent to enforce it on.
DEBUG: Rule at line 29 without any user agent to enforce it on.
DEBUG: Rule at line 30 without any user agent to enforce it on.
DEBUG: Rule at line 31 without any user agent to enforce it on.
DEBUG: Rule at line 32 without any user agent to enforce it on.
DEBUG: Rule at line 35 without any user agent to enforce it on.
DEBUG: Rule at line 38 without any user agent to enforce it on.
DEBUG: Rule at line 41 without any user agent to enforce it on.
DEBUG: Rule at line 43 without any user agent to enforce it on.
DEBUG: Rule at line 44 without any user agent to enforce it on.
DEBUG: Rule at line 45 without any user agent to enforce it on.
DEBUG: Rule at line 46 without any user agent to enforce it on.
DEBUG: Rule at line 49 without any user agent to enforce it on.
DEBUG: Rule at line 50 without any user agent to enforce it on.
DEBUG: Rule at line 56 without any user agent to enforce it on.
DEBUG: Rule at line 57 without any user agent to enforce it on.
DEBUG: Rule at line 58 without any user agent to enforce it on.
DEBUG: Rule at line 64 without any user agent to enforce it on.
DEBUG: Rule at line 69 without any user agent to enforce it on.
DEBUG: Rule at line 70 without any user agent to enforce it on.
DEBUG: Rule at line 71 without any user agent to enforce it on.
DEBUG: Rule at line 72 without any user agent to enforce it on.
DEBUG: Rule at line 76 without any user agent to enforce it on.
DEBUG: Rule at line 77 without any user agent to enforce it on.
DEBUG: Rule at line 78 without any user agent to enforce it on.
DEBUG: Rule at line 79 without any user agent to enforce it on.
DEBUG: Rule at line 82 without any user agent to enforce it on.
DEBUG: Rule at line 86 without any user agent to enforce it on.
DEBUG: Rule at line 89 without any user agent to enforce it on.
DEBUG: Rule at line 90 without any user agent to enforce it on.
DEBUG: Rule at line 91 without any user agent to enforce it on.
DEBUG: Rule at line 92 without any user agent to enforce it on.
DEBUG: Rule at line 93 without any user agent to enforce it on.
DEBUG: Rule at line 94 without any user agent to enforce it on.
DEBUG: Rule at line 98 without any user agent to enforce it on.
DEBUG: Rule at line 102 without any user agent to enforce it on.
DEBUG: Rule at line 103 without any user agent to enforce it on.
DEBUG: Rule at line 104 without any user agent to enforce it on.
DEBUG: Rule at line 108 without any user agent to enforce it on.
DEBUG: Rule at line 109 without any user agent to enforce it on.
DEBUG: Rule at line 110 without any user agent to enforce it on.
DEBUG: Rule at line 114 without any user agent to enforce it on.
DEBUG: Rule at line 115 without any user agent to enforce it on.
DEBUG: Rule at line 116 without any user agent to enforce it on.
DEBUG: Rule at line 118 without any user agent to enforce it on.
DEBUG: Rule at line 119 without any user agent to enforce it on.
DEBUG: Rule at line 120 without any user agent to enforce it on.
DEBUG: Rule at line 121 without any user agent to enforce it on.
DEBUG: Rule at line 122 without any user agent to enforce it on.
DEBUG: Rule at line 123 without any user agent to enforce it on.
DEBUG: Rule at line 124 without any user agent to enforce it on.
DEBUG: Rule at line 125 without any user agent to enforce it on.
DEBUG: Rule at line 126 without any user agent to enforce it on.
DEBUG: Rule at line 127 without any user agent to enforce it on.
DEBUG: Rule at line 128 without any user agent to enforce it on.
DEBUG: Rule at line 131 without any user agent to enforce it on.
DEBUG: Rule at line 132 without any user agent to enforce it on.
DEBUG: Rule at line 133 without any user agent to enforce it on.
DEBUG: Rule at line 137 without any user agent to enforce it on.
DEBUG: Rule at line 140 without any user agent to enforce it on.
DEBUG: Rule at line 143 without any user agent to enforce it on.
DEBUG: Rule at line 144 without any user agent to enforce it on.
DEBUG: Rule at line 145 without any user agent to enforce it on.
DEBUG: Rule at line 146 without any user agent to enforce it on.
DEBUG: Rule at line 147 without any user agent to enforce it on.
DEBUG: Rule at line 148 without any user agent to enforce it on.
DEBUG: Rule at line 150 without any user agent to enforce it on.
DEBUG: Rule at line 151 without any user agent to enforce it on.
DEBUG: Rule at line 154 without any user agent to enforce it on.
DEBUG: Rule at line 155 without any user agent to enforce it on.
DEBUG: Rule at line 156 without any user agent to enforce it on.
DEBUG: Rule at line 157 without any user agent to enforce it on.
DEBUG: Rule at line 160 without any user agent to enforce it on.
DEBUG: Rule at line 161 without any user agent to enforce it on.
DEBUG: Rule at line 164 without any user agent to enforce it on.
DEBUG: Rule at line 167 without any user agent to enforce it on.
DEBUG: Rule at line 170 without any user agent to enforce it on.
DEBUG: Rule at line 173 without any user agent to enforce it on.
DEBUG: Rule at line 174 without any user agent to enforce it on.
DEBUG: Rule at line 189 without any user agent to enforce it on.
DEBUG: Redirecting (301) to <GET https://aquasoleilhotel.com/robots.txt> from <GET http://www.aquasoleilhotel.com/robots.txt>
DEBUG: Crawled (200) <GET https://paradisepoint.com/?utm_source=google-local&utm_medium=organic&utm_campaign=gmb> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.senspa.com/> from <GET http://www.senspa.com/>
DEBUG: Crawled (451) <GET https://www.morongocasinoresort.com/> (referer: None)
DEBUG: Crawled (200) <GET https://oceanohalfmoonbay.com/contact/> (referer: https://oceanohalfmoonbay.com/)
DEBUG: Crawled (200) <GET https://aquasoleilhotel.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://aquasoleilhotel.com/> from <GET http://www.aquasoleilhotel.com/>
INFO: Ignoring response <451 https://www.morongocasinoresort.com/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://aquasoleilhotel.com/robots.txt> (referer: None)
DEBUG: Scraped from <200 https://oceanohalfmoonbay.com/contact/>
{'emails': ['admin@oceanohalfmoonbay.com'],
 'facebook': 'https://www.facebook.com/oceanohotelspa/',
 'instagram': 'https://www.instagram.com/oceanohotelspa/?ref=badge',
 'linkedin': '',
 'twitter': ''}
DEBUG: Crawled (200) <GET http://www.thankyouandbewell.com/robots.txt> (referer: None)
DEBUG: Forbidden by robots.txt: <GET http://www.thankyouandbewell.com/>
DEBUG: Crawled (200) <GET https://aquasoleilhotel.com/> (referer: None)
ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "C:\Users\moffi\PycharmProjects\scraping_web\venv\lib\site-packages\scrapy\core\engine.py", line 152, in _next_request
    request = next(self.slot.start_requests)
  File "C:\Users\moffi\PycharmProjects\scraping_web\scraper\scraper\spiders\website_spider.py", line 34, in start_requests
    for row in reader:
  File "C:\Users\moffi\AppData\Local\Programs\Python\Python310\lib\encodings\cp1252.py", line 23, in decode
    return codecs.charmap_decode(input,self.errors,decoding_table)[0]
UnicodeDecodeError: 'charmap' codec can't decode byte 0x81 in position 4042: character maps to <undefined>
DEBUG: Redirecting (301) to <GET https://www.thespaatfgv.com/robots.txt> from <GET http://www.thespaatfgv.com/robots.txt>
DEBUG: Redirecting (301) to <GET https://doubleeagle.com/> from <GET http://doubleeagle.com/>
DEBUG: Crawled (200) <GET https://www.macarthurplace.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.senspa.com/> (referer: None)
DEBUG: Crawled (200) <GET https://aquasoleilhotel.com/contact/> (referer: https://aquasoleilhotel.com/)
DEBUG: Redirecting (301) to <GET https://www.miramonteresort.com/robots.txt> from <GET http://www.miramonteresort.com/robots.txt>
DEBUG: Crawled (200) <GET https://www.rushcreeklodge.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET http://www.athenaspa-mv.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.rushcreeklodge.com/> from <GET http://www.rushcreeklodge.com/>
DEBUG: Redirecting (301) to <GET https://thespaatfgv.com/robots.txt> from <GET https://www.thespaatfgv.com/robots.txt>
DEBUG: Crawled (200) <GET https://truerest.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.ventanabigsur.com/spa/overview> (referer: None)
DEBUG: Crawled (200) <GET https://www.macarthurplace.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.rushcreeklodge.com/> (referer: None)
DEBUG: Crawled (200) <GET https://www.miramonteresort.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.doubleeagle.com/> from <GET https://doubleeagle.com/>
DEBUG: Crawled (403) <GET https://truerest.com/locations/fresno/> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.miramonteresort.com/spa> from <GET http://www.miramonteresort.com/spa>
INFO: Ignoring response <403 https://truerest.com/locations/fresno/>: HTTP status code is not handled or not allowed
DEBUG: Crawled (200) <GET https://www.rushcreeklodge.com/us/contact-us/> (referer: https://www.rushcreeklodge.com/)
DEBUG: Crawled (200) <GET https://www.macarthurplace.com/contact/> (referer: https://www.macarthurplace.com/)
DEBUG: Crawled (200) <GET http://www.athenaspa-mv.com/> (referer: None)
DEBUG: Scraped from <200 https://www.rushcreeklodge.com/us/contact-us/>
{'emails': ['info@rushcreeklodge.com.'],
 'facebook': 'https://www.facebook.com/RushCreekLodge/',
 'instagram': 'https://www.instagram.com/yosemite_rushcreek/',
 'linkedin': '',
 'twitter': ''}
DEBUG: Redirecting (301) to <GET https://www.miramonteresort.com/the-well-spa/> from <GET https://www.miramonteresort.com/spa>
DEBUG: Crawled (200) <GET http://www.athenaspa-mv.com/contact-us> (referer: http://www.athenaspa-mv.com/)
DEBUG: Crawled (200) <GET https://www.miramonteresort.com/the-well-spa/> (referer: None)
DEBUG: Crawled (200) <GET https://www.doubleeagle.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://thespaatfgv.com/robots.txt> (referer: None)
DEBUG: Redirecting (301) to <GET https://www.thespaatfgv.com/> from <GET http://www.thespaatfgv.com/>
DEBUG: Redirecting (301) to <GET https://thespaatfgv.com/> from <GET https://www.thespaatfgv.com/>
DEBUG: Crawled (200) <GET https://thespaatfgv.com/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://www.doubleeagle.com/> (referer: None)
DEBUG: Crawled (200) <GET https://thespaatfgv.com/> (referer: None)
DEBUG: Crawled (200) <GET https://thespaatfgv.com/contact/> (referer: https://thespaatfgv.com/)
DEBUG: Scraped from <200 https://thespaatfgv.com/contact/>
{'emails': ['tammie@hungryhairsalon.com'],
 'facebook': 'https://www.facebook.com/SpaHungryHair/',
 'instagram': 'https://www.instagram.com/themedicalspa_hungryhair/',
 'linkedin': '',
 'twitter': ''}
INFO: Closing spider (finished)
INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/scrapy.exceptions.IgnoreRequest': 3,
 'downloader/request_bytes': 89280,
 'downloader/request_count': 319,
 'downloader/request_method_count/GET': 319,
 'downloader/response_bytes': 5954200,
 'downloader/response_count': 319,
 'downloader/response_status_count/200': 204,
 'downloader/response_status_count/301': 93,
 'downloader/response_status_count/403': 2,
 'downloader/response_status_count/404': 6,
 'downloader/response_status_count/429': 6,
 'downloader/response_status_count/451': 2,
 'downloader/response_status_count/502': 6,
 'dupefilter/filtered': 38,
 'elapsed_time_seconds': 20.067712,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 12, 9, 13, 31, 47, 67248),
 'httpcompression/response_bytes': 20733238,
 'httpcompression/response_count': 188,
 'httperror/response_ignored_count': 6,
 'httperror/response_ignored_status_count/403': 2,
 'httperror/response_ignored_status_count/404': 1,
 'httperror/response_ignored_status_count/429': 1,
 'httperror/response_ignored_status_count/451': 1,
 'httperror/response_ignored_status_count/502': 1,
 'item_scraped_count': 11,
 'log_count/DEBUG': 704,
 'log_count/ERROR': 5,
 'log_count/INFO': 16,
 'request_depth_max': 1,
 'response_received_count': 218,
 'retry/count': 8,
 'retry/max_reached': 4,
 'retry/reason_count/429 Unknown Status': 4,
 'retry/reason_count/502 Bad Gateway': 4,
 'robotstxt/forbidden': 3,
 'robotstxt/request_count': 101,
 'robotstxt/response_count': 101,
 'robotstxt/response_status_count/200': 93,
 'robotstxt/response_status_count/404': 5,
 'robotstxt/response_status_count/429': 1,
 'robotstxt/response_status_count/451': 1,
 'robotstxt/response_status_count/502': 1,
 'scheduler/dequeued': 179,
 'scheduler/dequeued/memory': 179,
 'scheduler/enqueued': 179,
 'scheduler/enqueued/memory': 179,
 'start_time': datetime.datetime(2022, 12, 9, 13, 31, 26, 999536)}
INFO: Spider closed (finished)
